{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d3fa097-c74b-457c-9b9a-6dcb7dc691e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ed7eb37-e9a8-4497-9534-74142f9f656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.reasoning import make_segment, split_cot\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59b9ccc3-6d10-44e9-878b-91ab725b521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import StoppingCriteria\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "from modeling_rmt.language_modeling import MemoryCell\n",
    "# from modeling_rmt.experimental import RecurrentWrapperNoSegmentation\n",
    "\n",
    "from modeling_rmt.experimental import RecurrentWrapperNoSegmentationGenerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d706ca2-fbd6-4c63-a4f4-370aacbe6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "bos = [tokenizer.bos_token_id]\n",
    "eos = [tokenizer.eos_token_id]\n",
    "think = tokenizer.encode(\"<issue_start>\")\n",
    "ans = tokenizer.encode(\"<issue_closed>\")\n",
    "delim = \">> <<\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967c5f0-3b46-448c-8b7b-4462b0e79fd8",
   "metadata": {},
   "source": [
    "class StopOnSpecialTokenCriteria(StoppingCriteria):\n",
    "    def __init__(self, special_token_ids):\n",
    "        self.special_token_ids = set(special_token_ids)\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        last_token = input_ids[0, -1].item()\n",
    "        return last_token in self.special_token_ids\n",
    "\n",
    "\n",
    "class RecurrentWrapperNoSegmentationGenerate(RecurrentWrapperNoSegmentation):\n",
    "    def forward(self, segments, labels, output_attentions=None, output_hidden_states=None):\n",
    "        memory_state = None\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segments):\n",
    "            cell_out, memory_state = self.memory_cell(input_ids=segment['input_ids'],\n",
    "                                                      attention_mask=segment['attention_mask'],\n",
    "                                                      memory_state=memory_state, output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, segments,\n",
    "                                   output_attentions=output_attentions,\n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "    \n",
    "    def generate(self, segments, **kwargs):\n",
    "        memory_state = None\n",
    "        \n",
    "        # cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segments):\n",
    "            cell_out, memory_state = self.memory_cell(input_ids=segment['input_ids'],\n",
    "                                                        attention_mask=segment['attention_mask'],\n",
    "                                                        memory_state=memory_state, output_hidden_states=True)\n",
    "            # cell_outputs.append(cell_out)\n",
    "\n",
    "        generated_segments = []\n",
    "        for seg_num in range(len(segments), self.rmt_config.get(\"max_n_segments\", 32)):\n",
    "            output_ids, memory_state = self.generate_segment(memory_state=memory_state, **kwargs)\n",
    "            generated_segments.append(output_ids)\n",
    "            \n",
    "            if self.all_done(generated_segments):\n",
    "                break\n",
    "\n",
    "        # return self.post_process_generated_segments(generated_segments)\n",
    "        return generated_segments\n",
    "\n",
    "\n",
    "    def generate_segment(self, memory_state, **kwargs):\n",
    "        input_ids = self.get_bos_tensor(memory_state)\n",
    "        attention_mask = torch.ones_like(input_ids).bool()\n",
    "\n",
    "        generated = self.memory_cell.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            memory_state=memory_state,\n",
    "            # max_new_tokens=kwargs.get('max_new_tokens', 50),\n",
    "            stopping_criteria=self.make_custom_stopping_criteria(),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Update memory from generation\n",
    "        fwd_inputs = torch.cat((input_ids, generated), dim=1)[:, :-1]\n",
    "        _, memory_state = self.memory_cell(input_ids=fwd_inputs, memory_state=memory_state)\n",
    "\n",
    "        return generated, memory_state\n",
    "    \n",
    "    def get_bos_tensor(self, memory_state):\n",
    "        bos = self.rmt_config[\"bos_token_id\"]\n",
    "        bos_tensor = torch.tensor([bos] * memory_state.shape[0]).reshape(-1, 1)\n",
    "        return bos_tensor.to(memory_state.device)\n",
    "    \n",
    "    def all_done(self, generated_segments):\n",
    "        eos = self.rmt_config['eos_token_id']\n",
    "        bs = generated_segments[0].shape[0]\n",
    "        have_eos = [any([eos in seg[i] for seg in generated_segments]) for i in range(bs)]\n",
    "        all_done = all(have_eos)\n",
    "        return all_done\n",
    "    \n",
    "    def make_custom_stopping_criteria(self):\n",
    "        return [StopOnSpecialTokenCriteria([self.rmt_config['think_token_id'], self.rmt_config['answer_token_id']])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2fae0222-d14a-4a72-9f50-50efffdf0b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # first, we segment each sample into task, cot steps and labels\n",
    "    segments_batch = []\n",
    "    for sample in batch:\n",
    "        task, lab, cot = sample['task'], sample['labels'], sample['cot']\n",
    "        task_tokens = tokenizer.encode(task, add_special_tokens=False)\n",
    "        labels_tokens = tokenizer.encode(lab, add_special_tokens=False)\n",
    "        if getattr(args, 'use_cot', False):\n",
    "            cot_segments = split_cot(cot, by=delim)\n",
    "        else:\n",
    "            cot_segments = [cot]\n",
    "        cot_segment_tokens = tokenizer.batch_encode_plus(cot_segments, add_special_tokens=False)['input_ids']\n",
    "\n",
    "        segments = []\n",
    "        segments.append(make_segment(bos + task_tokens + think, loss=False))\n",
    "        for segment in cot_segment_tokens[:-1]:\n",
    "            segments.append(make_segment(bos + segment + think, loss=True))\n",
    "        segments.append(make_segment(bos + cot_segment_tokens[-1] + ans, loss=True))\n",
    "\n",
    "        segments.append(make_segment(bos + labels_tokens + eos, loss=True))\n",
    "        segments_batch.append(segments)\n",
    "\n",
    "    # if some samples have less segments than others, we pad them with empty segments\n",
    "    num_segments = max(len(segments) for segments in segments_batch)\n",
    "    for segments in segments_batch:\n",
    "        if len(segments) < num_segments:\n",
    "            segments.extend([make_segment(eos, loss=False)] * (num_segments - len(segments)))\n",
    "\n",
    "    # prepare segments for the whole batch\n",
    "    batch_segments = []\n",
    "    for i in range(num_segments):\n",
    "        input_ids = [s[i]['input_ids'] for s in segments_batch]\n",
    "        attention_mask = [s[i]['attention_mask'] for s in segments_batch]\n",
    "        labels = [s[i]['labels'] for s in segments_batch]\n",
    "        labels_mask = [s[i]['labels_mask'] for s in segments_batch]\n",
    "        # FIXME, pad by right side!!!\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=id_pad_value)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        labels_mask = pad_sequence(labels_mask, batch_first=True, padding_value=False)\n",
    "\n",
    "        batch_segment = {'input_ids': input_ids,\n",
    "                         'attention_mask': attention_mask,\n",
    "                         'labels_mask': labels_mask,\n",
    "                         'labels': labels\n",
    "                         }\n",
    "        batch_segments.append(batch_segment)\n",
    "    full_labels = torch.cat([s['labels'] for s in batch_segments], dim=1)\n",
    "    return {\"segments\": batch_segments, 'labels': full_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dbeb556-09b7-4ac6-98b1-545d4e2bf8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Holder()\n",
    "args.use_cot = True\n",
    "args.num_mem_tokens = 16\n",
    "args.segment_size = 64\n",
    "args.max_n_segments = 10\n",
    "args.max_cot_steps = 8\n",
    "args.task_name = 'gsm8k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "528158b9-977c-4be1-821b-2656e2fb411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":)\n"
     ]
    }
   ],
   "source": [
    "memory_cell = MemoryCell(\n",
    "    model,\n",
    "    num_mem_tokens=16\n",
    ")\n",
    "\n",
    "rmt = RecurrentWrapperNoSegmentationGenerate(\n",
    "    memory_cell, \n",
    "    max_n_segments=8, \n",
    "    think_token_id=think[0],\n",
    "    answer_token_id=ans[0],\n",
    "    bos_token_id=bos[0],\n",
    "    eos_token_id=eos[0]\n",
    ")\n",
    "\n",
    "# checkpoint_path = \"/home/user33/kashurin/RMT_SmolLM2-135M/pt/checkpoint-29500/pytorch_model.bin\"\n",
    "checkpoint_path = \"/home/user33/kashurin/RMT_SmolLM2-135M/cot/checkpoint-2200/pytorch_model.bin\"\n",
    "device = 'cpu'\n",
    "\n",
    "rmt.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "rmt.to(device)\n",
    "print(':)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf29ce02-661f-40d1-9be0-f33507c47029",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'booydar/gsm8k'\n",
    "train_dataset = datasets.load_dataset(dataset, split='train')\n",
    "valid_dataset = datasets.load_dataset(dataset, split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b4a9243-395d-4399-943e-9345cfe6cc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year?',\n",
       " 'labels': '300',\n",
       " 'cot': '<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>>',\n",
       " 'cot_len': 4}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "05d4f3e9-802e-4714-bb43-2a5d16fadf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text_batch: list[str], max_new_tokens=64):\n",
    "    collated = collate_fn(batch)\n",
    "    task = collated['segments'][0]\n",
    "    task = {k:v.to(device) for k,v in task.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_out = rmt.generate([task], max_new_tokens=max_new_tokens, pad_token_id=eos[0])\n",
    "    preds_full = torch.cat(gen_out, dim=1)\n",
    "    \n",
    "    gen_text = tokenizer.batch_decode(preds_full, skip_special_tokens=True)\n",
    "\n",
    "    # generations = []\n",
    "    # for gen in gen_segments:\n",
    "    #     gen_text = tokenizer.batch_decode(gen, skip_special_tokens=False)\n",
    "    #     generations.append(gen_text)\n",
    "\n",
    "    # full_gen = []\n",
    "    # for i in range(len(text_batch)):\n",
    "    #     full_gen.append([])\n",
    "    #     for gen in generations:\n",
    "    #         full_gen[-1].append(gen[i])\n",
    "        \n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fde8503e-29b1-4ae5-93bf-d0183274d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = valid_dataset.select(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4b3fc8d7-d7a8-4f5c-9cf6-4f285d6f35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = generate_text(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "da3e318b-ec3f-4bf9-8a6f-cf7aa8799c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2*4=88*2=1616*1=161200-16=11641164/12=949419*1=9',\n",
       " '1.5*2=1.5*2.51.5+2.5+2.5*1.5=3.752.5*3.75=112*1.75=3.53+1+1+1+1+1+3.75+3=5.55555555555555555',\n",
       " '2000*32000-202000-20020000*0.05=102000000000020000000600000',\n",
       " '21/7=321*5=1014*1=1414*2=28214+28=4242422*15=3']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee163a4f-d63c-477d-83ed-c1578e5838f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, device='cpu', bs=16, max_new_tokens=25):\n",
    "    all_preds, all_labels = [], []\n",
    "    all_preds_cot, all_labels_cot = [], []\n",
    "    all_preds_ans, all_labels_ans = [], []\n",
    "\n",
    "    for start_ind in range(0, len(dataset), bs):\n",
    "        batch = dataset.select(range(start_ind, min(len(dataset), start_ind + bs)))\n",
    "        collated = collate_fn(batch)\n",
    "        task = collated['segments'][0]\n",
    "        task = {k:v.to(device) for k,v in task.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen_out = model.generate([task], max_new_tokens=max_new_tokens, pad_token_id=eos[0])\n",
    "        \n",
    "\n",
    "        preds_full = torch.cat(gen_out, dim=1)\n",
    "        labels = collated['labels']\n",
    "\n",
    "        labels_masks = labels > 0\n",
    "        labels_full = [lab[m] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "        for lab_tokens, pred_tokens in zip(labels_full, preds_full):\n",
    "            lab_tokens = [t.item() for t in lab_tokens if t != bos[0]]\n",
    "            \n",
    "            ans_start_index_l = max(i for i, x in enumerate(lab_tokens) if x == ans[0])\n",
    "            if ans[0] in pred_tokens:\n",
    "                ans_start_index_p = max(i for i, x in enumerate(pred_tokens) if x == ans[0])\n",
    "            else:\n",
    "                ans_start_index_p = ans_start_index_l\n",
    "            ans_start_index_p = ans_start_index_l\n",
    "\n",
    "            pred_cot_tokens = pred_tokens[:ans_start_index_p].tolist()\n",
    "            lab_cot_tokens = lab_tokens[:ans_start_index_l]\n",
    "\n",
    "            all_preds_cot.append(pred_cot_tokens)\n",
    "            all_labels_cot.append(lab_cot_tokens)\n",
    "\n",
    "            all_preds_ans.append(pred_tokens[ans_start_index_p:].tolist())\n",
    "            all_labels_ans.append(lab_tokens[ans_start_index_l:])\n",
    "\n",
    "            all_preds.append(pred_tokens.tolist())\n",
    "            all_labels.append(lab_tokens)\n",
    "    \n",
    "    cot_correct = [p == l for p, l in zip(all_preds_cot, all_labels_cot)]\n",
    "    ans_correct = [p == l for p, l in zip(all_preds_ans, all_labels_ans)]\n",
    "    res = {'accuracy_cot': np.mean(cot_correct), 'accuracy_ans': np.mean(ans_correct)}\n",
    "    data = {\"all_preds_cot\": all_preds_cot,\n",
    "            \"all_labels_cot\": all_labels_cot,\n",
    "            \"all_preds_ans\": all_preds_ans,\n",
    "            \"all_labels_ans\": all_labels_ans,\n",
    "            \"all_preds\": all_preds,\n",
    "            \"all_labels\": all_labels}\n",
    "    return res, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c19beef-dbb7-48d3-9bfb-23dfbdb11513",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, data = evaluate(rmt, valid_dataset.select(range(1)), bs=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee5852-195d-4913-a86f-3f5a5067af50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f9fd446-44e0-4e19-8964-4871041aaa23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2*4=8<issue_start>8*2=16<issue_start>16*12=192<issue_start>192<|endoftext|>'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"all_preds\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2b94f-49d1-40be-afb0-0ced08b2f6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6648a66-1dbb-474c-8813-b678980b41a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc41733-7167-44e7-8e2b-8b598bc128f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b2c6e10-7588-4f65-9a9d-80884a315d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generation = generate_text([\"London is the capital of \", \"Hello \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "01e4a68f-7f5d-4f88-bbd4-4651dbbba9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[', the world', ', and/', '\\n- The'], ['\\nThe name', '\\n- The', '.\\nThe']]\n"
     ]
    }
   ],
   "source": [
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f22a3160-d419-48df-a03b-9a04f99e94ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "task_batch = [\n",
    "    valid_dataset[0][\"task\"],\n",
    "    valid_dataset[1][\"task\"]\n",
    "]\n",
    "\n",
    "generation = generate_text(task_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63a20a5d-5348-497e-b4f9-52015d1a80cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2*4', '2*4', '2*2', '12*', '50*', '12*', '500'],\n",
       " ['1.5', '1.5', '2.5', '2.5', '2.5', '2.5', '2.5']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca6fef6d-c886-4e23-b06b-359d082c9824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year?<issue_start><|endoftext|>'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0,  9682, 10672,   650,  5372,   288,   216,    34,  6439,    30,\n",
    "             216,   657,  8759,  1673,    37,  6439,   567,  3531,    30,   216,\n",
    "            1550,   357,  4364,   288,   216,    36,  6439,   384, 10672,   357,\n",
    "            1056,  1187,   288,   216,    34,  6439,    30,   216,   657,  1708,\n",
    "            1885,    33,    32,    32,   288,   820,   650,  5372,  2304,    30,\n",
    "             216,  1073,  1083,  1072,   384,  2290,   567,   713,    47,     8, eos[0]], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a66bf3-7125-4f67-afaa-eb609d3dc5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Today is a beautiful\", return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    num_beams=3,\n",
    "    do_sample=False,\n",
    "    top_p=0.95,\n",
    "    # return_dict_in_generate=True,\n",
    "    # output_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0276b9-8972-47ca-91d2-62c4721e0f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rmt/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3841\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3838\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3839\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3841\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3844\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3845\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3846\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rmt/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:682\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    681\u001b[39m     token_ids = [token_ids]\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m clean_up_tokenization_spaces = (\n\u001b[32m    685\u001b[39m     clean_up_tokenization_spaces\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clean_up_tokenization_spaces\n\u001b[32m    688\u001b[39m )\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[31mTypeError\u001b[39m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(outputs, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e41184-80a1-47a9-b60f-a1017aded195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated (-1.43): Today is a beautiful day.\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "for seq, score in zip(outputs.sequences, outputs.sequences_scores):\n",
    "    text = tokenizer.decode(seq, skip_special_tokens=False)\n",
    "    print(f\"Generated ({score:.2f}): {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7455c1-6a35-4648-926c-badc694eff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, N, H = 2, 4, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463dd2f-bc2a-44c5-9fec-f769eab091fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(bs, N, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b976463c-5012-4716-8a3a-ade7d9a6d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([[True, False, False, True],\n",
    "                    [False, True, False, False]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa23dc-252a-4d14-95d1-49490dc1a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values = torch.randn(mask.sum(), H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7221d3-33a5-4ee9-95d1-faa263fb204f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73faf2dd-affd-4f09-a385-b5463363d801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
