{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.reasoning import make_segment, split_cot\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from modeling_rmt.language_modeling import MemoryCell\n",
    "# from modeling_rmt.experimental import RecurrentWrapperNoSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_holder = Holder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention, GPT2MLP\n",
    "\n",
    "from modeling_rmt.experimental import RecurrentWrapperNoSegmentationGenerate\n",
    "\n",
    "\n",
    "class RMCrossAttention(torch.nn.Module):\n",
    "    def __init__(self, config, dropout=False, ff=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        hidden_size = config.hidden_size\n",
    "        self.ln = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.cross_attn = GPT2Attention(config=config, is_cross_attention=True)\n",
    "        self.dropout = nn.Dropout(config.attention_dropout) if dropout else None\n",
    "\n",
    "        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n",
    "        self.ff = GPT2MLP(inner_dim, config) if ff else None\n",
    "\n",
    "    def forward(self, hidden_states, encoder_hidden_states, attention_mask=None,\n",
    "                output_attentions=False\n",
    "                ):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        cross_attn_outputs = self.cross_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = cross_attn_outputs[0]\n",
    "        if self.dropout is not None:\n",
    "            attn_output = self.dropout(attn_output)\n",
    "        if self.ff is not None:\n",
    "            attn_output = self.ff(attn_output)\n",
    "        hidden_states = residual + attn_output\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if output_attentions:\n",
    "            outputs += (cross_attn_outputs[1:],)\n",
    "\n",
    "        self.debug_state = dict(encoder_hidden_states=encoder_hidden_states,\n",
    "                                inp_hidden_states=residual,\n",
    "                                output=outputs,\n",
    "                                attn_output=attn_output,\n",
    "                                cross_attn_outputs=cross_attn_outputs,\n",
    "        )\n",
    "\n",
    "\n",
    "        return outputs  # hidden_states, attention_weights\n",
    "\n",
    "\n",
    "class RMCALayerWrapper(torch.nn.Module):\n",
    "    def __init__(self, layer, num_mem_tokens, config):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.create_memory(num_mem_tokens, config.hidden_size)\n",
    "        self.memory_state = None\n",
    "        self.mem_read_layer = self.create_mem_read_layer(config)\n",
    "        self.mem_write_layer = self.create_mem_write_layer(config)\n",
    "        self.generate_mode = False\n",
    "\n",
    "    def forward(self, hidden_states, output_attentions=False, *args, **kwargs):\n",
    "        if self.memory_state is None:\n",
    "            self.memory_state = self.set_memory(hidden_states)\n",
    "\n",
    "        mem_read_out = self.mem_read_layer(hidden_states, encoder_hidden_states=self.memory_state,\n",
    "                                           output_attentions=output_attentions)\n",
    "        hidden_states, cross_attentions_read = mem_read_out[0], mem_read_out[1:]\n",
    "\n",
    "        mem_write_out = self.mem_write_layer(self.memory_state, encoder_hidden_states=hidden_states,\n",
    "                                             output_attentions=output_attentions)\n",
    "        self.memory_state, cross_attentions_write = mem_write_out[0], mem_write_out[1:]\n",
    "\n",
    "        out = self.layer(hidden_states, **kwargs, output_attentions=output_attentions)\n",
    "\n",
    "        self.debug_state = dict(inp_hidden_states=hidden_states, \n",
    "                                mem_read_out=mem_read_out,\n",
    "                                mem_write_out=mem_write_out,\n",
    "                                memory_state=self.memory_state, output=out,\n",
    "                                cross_attentions_read=cross_attentions_read,\n",
    "                                cross_attentions_write=cross_attentions_write)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def create_mem_read_layer(self, config):\n",
    "        mem_read_layer = RMCrossAttention(config, dropout=getattr(self, 'use_dropout', False))\n",
    "        mem_read_layer.cross_attn.c_proj.weight.data.zero_()\n",
    "        return mem_read_layer\n",
    "\n",
    "    def create_mem_write_layer(self, config):\n",
    "        mem_write_layer = RMCrossAttention(config, dropout=getattr(self, 'use_dropout', False))\n",
    "        mem_write_layer.cross_attn.c_proj.weight.data.zero_()\n",
    "        return mem_write_layer\n",
    "\n",
    "    def create_memory(self, num_mem_tokens, memory_dim):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        memory_weights = torch.randn((num_mem_tokens, memory_dim))\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "    def set_memory(self, hidden_states):\n",
    "        memory = self.memory.repeat(hidden_states.shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.memory_state = None\n",
    "\n",
    "    def detach_memory_state(self):\n",
    "        if self.memory_state is not None:\n",
    "            self.memory_state = self.memory_state.detach()\n",
    "\n",
    "\n",
    "class RMCAMemoryCell(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_mem_tokens, layers_attr: str = 'transformer.h'):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "\n",
    "        self.layers = self.model\n",
    "        self.layers_attrs = layers_attr.split('.')\n",
    "        for i, attr in enumerate(self.layers_attrs):\n",
    "            self.layers = getattr(self.layers, attr)\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i] = RMCALayerWrapper(\n",
    "                self.layers[i],\n",
    "                self.num_mem_tokens,\n",
    "                self.model.config\n",
    "            )\n",
    "\n",
    "    def reset_memory(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].reset_memory()\n",
    "\n",
    "    def detach_memory_state(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].detach_memory_state()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def generate(self, input_ids, memory_state, attention_mask=None, **generate_kwargs):\n",
    "        raise NotImplementedError(\"Generation is not supported yet.\")\n",
    "\n",
    "\n",
    "class RMCAWrapperNoSegmentationGenerate(RecurrentWrapperNoSegmentationGenerate):\n",
    "    def forward(self, segments, labels, output_attentions=None, output_hidden_states=None, *args, **kwargs):\n",
    "        self.memory_cell.reset_memory()\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segments):\n",
    "            cell_out = self.memory_cell(input_ids=segment['input_ids'],\n",
    "                                        attention_mask=segment['attention_mask'],\n",
    "                                        output_attentions=output_attentions,\n",
    "                                        output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, segments,\n",
    "                                   output_attentions=output_attentions,\n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def generate(self, segments, **kwargs):\n",
    "        raise NotImplementedError(\"Generation is not supported yet.\")\n",
    "        memory_state = None\n",
    "\n",
    "        for seg_num, segment in enumerate(segments):\n",
    "            cell_out, memory_state = self.memory_cell(input_ids=segment['input_ids'],\n",
    "                                                      attention_mask=segment['attention_mask'],\n",
    "                                                      memory_state=memory_state, output_hidden_states=True)\n",
    "\n",
    "        generated_segments = []\n",
    "        for seg_num in range(len(segments), self.rmt_config.get(\"max_n_segments\", 32)):\n",
    "            output_ids, memory_state = self.generate_segment(memory_state=memory_state, **kwargs)\n",
    "            generated_segments.append(output_ids)\n",
    "\n",
    "            if self.all_done(generated_segments):\n",
    "                break\n",
    "\n",
    "        return generated_segments\n",
    "\n",
    "    def manage_gradients(self, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2', -1), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "                or seg_num + k2 > max_n_segments:\n",
    "            return\n",
    "\n",
    "        self.memory_cell.detach_memory_state()\n",
    "\n",
    "    def process_outputs(self, cell_outputs, segments, **kwargs):\n",
    "        out = CausalLMOutputWithCrossAttentions()\n",
    "        proxy_out = {}\n",
    "        for seg_num, segment in enumerate(segments):\n",
    "            cell_out = cell_outputs[seg_num]\n",
    "\n",
    "            full_logits = cell_out.logits\n",
    "\n",
    "            labels = segment.get('labels')\n",
    "            if labels is not None:\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "                shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "                flat_labels = shift_labels.view(-1)\n",
    "                flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                labels_mask = segment.get('labels_mask')\n",
    "                if labels_mask is not None:\n",
    "                    shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "                    flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                    flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "\n",
    "                    if labels_mask.sum() == 0:\n",
    "                        loss_value = 0\n",
    "                    else:\n",
    "                        loss_value = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "                proxy_out[f'loss_{seg_num}'] = loss_value\n",
    "            else:\n",
    "                proxy_out[f'loss_{seg_num}'] = 0\n",
    "\n",
    "            segment_keys = ['loss']\n",
    "            if kwargs.get('output_attentions'):\n",
    "                segment_keys.append('attentions')\n",
    "            if kwargs.get('output_hidden_states'):\n",
    "                segment_keys.append('hidden_states')\n",
    "\n",
    "            for key, value in cell_out.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    proxy_out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        num_segments = len(segments)\n",
    "        out['loss'] = sum([proxy_out[f'loss_{seg_num}'] for seg_num in range(num_segments)]) / num_segments\n",
    "        out['logits'] = torch.cat([cell_out.logits for cell_out in cell_outputs], dim=1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_cell = RMCAMemoryCell(base_model, num_mem_tokens=4, layers_attr=\"transformer.h\")\n",
    "rmt = RMCAWrapperNoSegmentationGenerate(memory_cell, max_n_segments=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model_1 = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# test_in = tokenizer.encode(\"Hello world\", return_tensors=\"pt\")\n",
    "# test_out = base_model_1(input_ids=test_in, output_hidden_states=True)\n",
    "# for hs in test_out.hidden_states:\n",
    "#     print(hs.mean().item(), hs.std().item(), hs.min().item(), hs.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RMCAWrapperNoSegmentationGenerate:\n\tMissing key(s) in state_dict: \"memory_cell.model.transformer.h.0.layer.memory\", \"memory_cell.model.transformer.h.0.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.0.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.0.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.0.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.0.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.0.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.0.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.0.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.0.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.0.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.memory\", \"memory_cell.model.transformer.h.1.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.1.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.1.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.1.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.1.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.1.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.1.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.1.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.1.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.memory\", \"memory_cell.model.transformer.h.2.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.2.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.2.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.2.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.2.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.2.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.2.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.2.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.2.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.memory\", \"memory_cell.model.transformer.h.3.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.3.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.3.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.3.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.3.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.3.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.3.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.3.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.3.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.memory\", \"memory_cell.model.transformer.h.4.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.4.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.4.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.4.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.4.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.4.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.4.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.4.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.4.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.memory\", \"memory_cell.model.transformer.h.5.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.5.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.5.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.5.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.5.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.5.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.5.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.5.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.5.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.memory\", \"memory_cell.model.transformer.h.6.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.6.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.6.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.6.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.6.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.6.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.6.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.6.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.6.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.memory\", \"memory_cell.model.transformer.h.7.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.7.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.7.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.7.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.7.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.7.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.7.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.7.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.7.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.memory\", \"memory_cell.model.transformer.h.8.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.8.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.8.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.8.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.8.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.8.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.8.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.8.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.8.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.memory\", \"memory_cell.model.transformer.h.9.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.9.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.9.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.9.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.9.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.9.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.9.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.9.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.9.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.memory\", \"memory_cell.model.transformer.h.10.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.10.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.10.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.10.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.10.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.10.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.10.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.10.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.10.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.memory\", \"memory_cell.model.transformer.h.11.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.11.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.11.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.11.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.11.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.11.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.11.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.11.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.11.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.0.layer.memory\", \"memory_cell.layers.0.layer.layer.ln_1.weight\", \"memory_cell.layers.0.layer.layer.ln_1.bias\", \"memory_cell.layers.0.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.0.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.0.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.0.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.0.layer.layer.ln_2.weight\", \"memory_cell.layers.0.layer.layer.ln_2.bias\", \"memory_cell.layers.0.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.0.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.0.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.0.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.0.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.0.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.0.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.0.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.1.layer.memory\", \"memory_cell.layers.1.layer.layer.ln_1.weight\", \"memory_cell.layers.1.layer.layer.ln_1.bias\", \"memory_cell.layers.1.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.1.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.1.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.1.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.1.layer.layer.ln_2.weight\", \"memory_cell.layers.1.layer.layer.ln_2.bias\", \"memory_cell.layers.1.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.1.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.1.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.1.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.1.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.1.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.1.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.1.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.2.layer.memory\", \"memory_cell.layers.2.layer.layer.ln_1.weight\", \"memory_cell.layers.2.layer.layer.ln_1.bias\", \"memory_cell.layers.2.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.2.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.2.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.2.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.2.layer.layer.ln_2.weight\", \"memory_cell.layers.2.layer.layer.ln_2.bias\", \"memory_cell.layers.2.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.2.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.2.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.2.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.2.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.2.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.2.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.2.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.3.layer.memory\", \"memory_cell.layers.3.layer.layer.ln_1.weight\", \"memory_cell.layers.3.layer.layer.ln_1.bias\", \"memory_cell.layers.3.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.3.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.3.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.3.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.3.layer.layer.ln_2.weight\", \"memory_cell.layers.3.layer.layer.ln_2.bias\", \"memory_cell.layers.3.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.3.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.3.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.3.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.3.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.3.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.3.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.3.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.4.layer.memory\", \"memory_cell.layers.4.layer.layer.ln_1.weight\", \"memory_cell.layers.4.layer.layer.ln_1.bias\", \"memory_cell.layers.4.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.4.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.4.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.4.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.4.layer.layer.ln_2.weight\", \"memory_cell.layers.4.layer.layer.ln_2.bias\", \"memory_cell.layers.4.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.4.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.4.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.4.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.4.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.4.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.4.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.4.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.5.layer.memory\", \"memory_cell.layers.5.layer.layer.ln_1.weight\", \"memory_cell.layers.5.layer.layer.ln_1.bias\", \"memory_cell.layers.5.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.5.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.5.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.5.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.5.layer.layer.ln_2.weight\", \"memory_cell.layers.5.layer.layer.ln_2.bias\", \"memory_cell.layers.5.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.5.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.5.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.5.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.5.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.5.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.5.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.5.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.6.layer.memory\", \"memory_cell.layers.6.layer.layer.ln_1.weight\", \"memory_cell.layers.6.layer.layer.ln_1.bias\", \"memory_cell.layers.6.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.6.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.6.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.6.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.6.layer.layer.ln_2.weight\", \"memory_cell.layers.6.layer.layer.ln_2.bias\", \"memory_cell.layers.6.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.6.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.6.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.6.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.6.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.6.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.6.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.6.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.7.layer.memory\", \"memory_cell.layers.7.layer.layer.ln_1.weight\", \"memory_cell.layers.7.layer.layer.ln_1.bias\", \"memory_cell.layers.7.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.7.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.7.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.7.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.7.layer.layer.ln_2.weight\", \"memory_cell.layers.7.layer.layer.ln_2.bias\", \"memory_cell.layers.7.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.7.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.7.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.7.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.7.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.7.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.7.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.7.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.8.layer.memory\", \"memory_cell.layers.8.layer.layer.ln_1.weight\", \"memory_cell.layers.8.layer.layer.ln_1.bias\", \"memory_cell.layers.8.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.8.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.8.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.8.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.8.layer.layer.ln_2.weight\", \"memory_cell.layers.8.layer.layer.ln_2.bias\", \"memory_cell.layers.8.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.8.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.8.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.8.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.8.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.8.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.8.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.8.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.9.layer.memory\", \"memory_cell.layers.9.layer.layer.ln_1.weight\", \"memory_cell.layers.9.layer.layer.ln_1.bias\", \"memory_cell.layers.9.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.9.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.9.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.9.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.9.layer.layer.ln_2.weight\", \"memory_cell.layers.9.layer.layer.ln_2.bias\", \"memory_cell.layers.9.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.9.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.9.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.9.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.9.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.9.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.9.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.9.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.10.layer.memory\", \"memory_cell.layers.10.layer.layer.ln_1.weight\", \"memory_cell.layers.10.layer.layer.ln_1.bias\", \"memory_cell.layers.10.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.10.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.10.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.10.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.10.layer.layer.ln_2.weight\", \"memory_cell.layers.10.layer.layer.ln_2.bias\", \"memory_cell.layers.10.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.10.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.10.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.10.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.10.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.10.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.10.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.10.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.11.layer.memory\", \"memory_cell.layers.11.layer.layer.ln_1.weight\", \"memory_cell.layers.11.layer.layer.ln_1.bias\", \"memory_cell.layers.11.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.11.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.11.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.11.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.11.layer.layer.ln_2.weight\", \"memory_cell.layers.11.layer.layer.ln_2.bias\", \"memory_cell.layers.11.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.11.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.11.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.11.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.11.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.11.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.11.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.11.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.c_proj.bias\". \n\tUnexpected key(s) in state_dict: \"memory_cell.model.transformer.h.0.layer.ln_1.weight\", \"memory_cell.model.transformer.h.0.layer.ln_1.bias\", \"memory_cell.model.transformer.h.0.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.0.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.0.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.0.layer.ln_2.weight\", \"memory_cell.model.transformer.h.0.layer.ln_2.bias\", \"memory_cell.model.transformer.h.0.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.0.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.0.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.ln_1.weight\", \"memory_cell.model.transformer.h.1.layer.ln_1.bias\", \"memory_cell.model.transformer.h.1.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.1.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.1.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.ln_2.weight\", \"memory_cell.model.transformer.h.1.layer.ln_2.bias\", \"memory_cell.model.transformer.h.1.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.1.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.1.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.ln_1.weight\", \"memory_cell.model.transformer.h.2.layer.ln_1.bias\", \"memory_cell.model.transformer.h.2.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.2.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.2.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.ln_2.weight\", \"memory_cell.model.transformer.h.2.layer.ln_2.bias\", \"memory_cell.model.transformer.h.2.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.2.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.2.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.ln_1.weight\", \"memory_cell.model.transformer.h.3.layer.ln_1.bias\", \"memory_cell.model.transformer.h.3.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.3.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.3.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.ln_2.weight\", \"memory_cell.model.transformer.h.3.layer.ln_2.bias\", \"memory_cell.model.transformer.h.3.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.3.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.3.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.ln_1.weight\", \"memory_cell.model.transformer.h.4.layer.ln_1.bias\", \"memory_cell.model.transformer.h.4.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.4.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.4.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.ln_2.weight\", \"memory_cell.model.transformer.h.4.layer.ln_2.bias\", \"memory_cell.model.transformer.h.4.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.4.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.4.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.ln_1.weight\", \"memory_cell.model.transformer.h.5.layer.ln_1.bias\", \"memory_cell.model.transformer.h.5.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.5.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.5.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.ln_2.weight\", \"memory_cell.model.transformer.h.5.layer.ln_2.bias\", \"memory_cell.model.transformer.h.5.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.5.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.5.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.ln_1.weight\", \"memory_cell.model.transformer.h.6.layer.ln_1.bias\", \"memory_cell.model.transformer.h.6.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.6.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.6.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.ln_2.weight\", \"memory_cell.model.transformer.h.6.layer.ln_2.bias\", \"memory_cell.model.transformer.h.6.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.6.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.6.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.ln_1.weight\", \"memory_cell.model.transformer.h.7.layer.ln_1.bias\", \"memory_cell.model.transformer.h.7.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.7.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.7.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.ln_2.weight\", \"memory_cell.model.transformer.h.7.layer.ln_2.bias\", \"memory_cell.model.transformer.h.7.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.7.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.7.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.ln_1.weight\", \"memory_cell.model.transformer.h.8.layer.ln_1.bias\", \"memory_cell.model.transformer.h.8.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.8.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.8.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.ln_2.weight\", \"memory_cell.model.transformer.h.8.layer.ln_2.bias\", \"memory_cell.model.transformer.h.8.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.8.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.8.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.ln_1.weight\", \"memory_cell.model.transformer.h.9.layer.ln_1.bias\", \"memory_cell.model.transformer.h.9.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.9.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.9.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.ln_2.weight\", \"memory_cell.model.transformer.h.9.layer.ln_2.bias\", \"memory_cell.model.transformer.h.9.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.9.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.9.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.ln_1.weight\", \"memory_cell.model.transformer.h.10.layer.ln_1.bias\", \"memory_cell.model.transformer.h.10.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.10.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.10.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.ln_2.weight\", \"memory_cell.model.transformer.h.10.layer.ln_2.bias\", \"memory_cell.model.transformer.h.10.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.10.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.10.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.ln_1.weight\", \"memory_cell.model.transformer.h.11.layer.ln_1.bias\", \"memory_cell.model.transformer.h.11.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.11.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.11.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.ln_2.weight\", \"memory_cell.model.transformer.h.11.layer.ln_2.bias\", \"memory_cell.model.transformer.h.11.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.11.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.11.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.mlp.c_proj.bias\", \"memory_cell.layers.0.layer.ln_1.weight\", \"memory_cell.layers.0.layer.ln_1.bias\", \"memory_cell.layers.0.layer.attn.c_attn.weight\", \"memory_cell.layers.0.layer.attn.c_attn.bias\", \"memory_cell.layers.0.layer.attn.c_proj.weight\", \"memory_cell.layers.0.layer.attn.c_proj.bias\", \"memory_cell.layers.0.layer.ln_2.weight\", \"memory_cell.layers.0.layer.ln_2.bias\", \"memory_cell.layers.0.layer.mlp.c_fc.weight\", \"memory_cell.layers.0.layer.mlp.c_fc.bias\", \"memory_cell.layers.0.layer.mlp.c_proj.weight\", \"memory_cell.layers.0.layer.mlp.c_proj.bias\", \"memory_cell.layers.1.layer.ln_1.weight\", \"memory_cell.layers.1.layer.ln_1.bias\", \"memory_cell.layers.1.layer.attn.c_attn.weight\", \"memory_cell.layers.1.layer.attn.c_attn.bias\", \"memory_cell.layers.1.layer.attn.c_proj.weight\", \"memory_cell.layers.1.layer.attn.c_proj.bias\", \"memory_cell.layers.1.layer.ln_2.weight\", \"memory_cell.layers.1.layer.ln_2.bias\", \"memory_cell.layers.1.layer.mlp.c_fc.weight\", \"memory_cell.layers.1.layer.mlp.c_fc.bias\", \"memory_cell.layers.1.layer.mlp.c_proj.weight\", \"memory_cell.layers.1.layer.mlp.c_proj.bias\", \"memory_cell.layers.2.layer.ln_1.weight\", \"memory_cell.layers.2.layer.ln_1.bias\", \"memory_cell.layers.2.layer.attn.c_attn.weight\", \"memory_cell.layers.2.layer.attn.c_attn.bias\", \"memory_cell.layers.2.layer.attn.c_proj.weight\", \"memory_cell.layers.2.layer.attn.c_proj.bias\", \"memory_cell.layers.2.layer.ln_2.weight\", \"memory_cell.layers.2.layer.ln_2.bias\", \"memory_cell.layers.2.layer.mlp.c_fc.weight\", \"memory_cell.layers.2.layer.mlp.c_fc.bias\", \"memory_cell.layers.2.layer.mlp.c_proj.weight\", \"memory_cell.layers.2.layer.mlp.c_proj.bias\", \"memory_cell.layers.3.layer.ln_1.weight\", \"memory_cell.layers.3.layer.ln_1.bias\", \"memory_cell.layers.3.layer.attn.c_attn.weight\", \"memory_cell.layers.3.layer.attn.c_attn.bias\", \"memory_cell.layers.3.layer.attn.c_proj.weight\", \"memory_cell.layers.3.layer.attn.c_proj.bias\", \"memory_cell.layers.3.layer.ln_2.weight\", \"memory_cell.layers.3.layer.ln_2.bias\", \"memory_cell.layers.3.layer.mlp.c_fc.weight\", \"memory_cell.layers.3.layer.mlp.c_fc.bias\", \"memory_cell.layers.3.layer.mlp.c_proj.weight\", \"memory_cell.layers.3.layer.mlp.c_proj.bias\", \"memory_cell.layers.4.layer.ln_1.weight\", \"memory_cell.layers.4.layer.ln_1.bias\", \"memory_cell.layers.4.layer.attn.c_attn.weight\", \"memory_cell.layers.4.layer.attn.c_attn.bias\", \"memory_cell.layers.4.layer.attn.c_proj.weight\", \"memory_cell.layers.4.layer.attn.c_proj.bias\", \"memory_cell.layers.4.layer.ln_2.weight\", \"memory_cell.layers.4.layer.ln_2.bias\", \"memory_cell.layers.4.layer.mlp.c_fc.weight\", \"memory_cell.layers.4.layer.mlp.c_fc.bias\", \"memory_cell.layers.4.layer.mlp.c_proj.weight\", \"memory_cell.layers.4.layer.mlp.c_proj.bias\", \"memory_cell.layers.5.layer.ln_1.weight\", \"memory_cell.layers.5.layer.ln_1.bias\", \"memory_cell.layers.5.layer.attn.c_attn.weight\", \"memory_cell.layers.5.layer.attn.c_attn.bias\", \"memory_cell.layers.5.layer.attn.c_proj.weight\", \"memory_cell.layers.5.layer.attn.c_proj.bias\", \"memory_cell.layers.5.layer.ln_2.weight\", \"memory_cell.layers.5.layer.ln_2.bias\", \"memory_cell.layers.5.layer.mlp.c_fc.weight\", \"memory_cell.layers.5.layer.mlp.c_fc.bias\", \"memory_cell.layers.5.layer.mlp.c_proj.weight\", \"memory_cell.layers.5.layer.mlp.c_proj.bias\", \"memory_cell.layers.6.layer.ln_1.weight\", \"memory_cell.layers.6.layer.ln_1.bias\", \"memory_cell.layers.6.layer.attn.c_attn.weight\", \"memory_cell.layers.6.layer.attn.c_attn.bias\", \"memory_cell.layers.6.layer.attn.c_proj.weight\", \"memory_cell.layers.6.layer.attn.c_proj.bias\", \"memory_cell.layers.6.layer.ln_2.weight\", \"memory_cell.layers.6.layer.ln_2.bias\", \"memory_cell.layers.6.layer.mlp.c_fc.weight\", \"memory_cell.layers.6.layer.mlp.c_fc.bias\", \"memory_cell.layers.6.layer.mlp.c_proj.weight\", \"memory_cell.layers.6.layer.mlp.c_proj.bias\", \"memory_cell.layers.7.layer.ln_1.weight\", \"memory_cell.layers.7.layer.ln_1.bias\", \"memory_cell.layers.7.layer.attn.c_attn.weight\", \"memory_cell.layers.7.layer.attn.c_attn.bias\", \"memory_cell.layers.7.layer.attn.c_proj.weight\", \"memory_cell.layers.7.layer.attn.c_proj.bias\", \"memory_cell.layers.7.layer.ln_2.weight\", \"memory_cell.layers.7.layer.ln_2.bias\", \"memory_cell.layers.7.layer.mlp.c_fc.weight\", \"memory_cell.layers.7.layer.mlp.c_fc.bias\", \"memory_cell.layers.7.layer.mlp.c_proj.weight\", \"memory_cell.layers.7.layer.mlp.c_proj.bias\", \"memory_cell.layers.8.layer.ln_1.weight\", \"memory_cell.layers.8.layer.ln_1.bias\", \"memory_cell.layers.8.layer.attn.c_attn.weight\", \"memory_cell.layers.8.layer.attn.c_attn.bias\", \"memory_cell.layers.8.layer.attn.c_proj.weight\", \"memory_cell.layers.8.layer.attn.c_proj.bias\", \"memory_cell.layers.8.layer.ln_2.weight\", \"memory_cell.layers.8.layer.ln_2.bias\", \"memory_cell.layers.8.layer.mlp.c_fc.weight\", \"memory_cell.layers.8.layer.mlp.c_fc.bias\", \"memory_cell.layers.8.layer.mlp.c_proj.weight\", \"memory_cell.layers.8.layer.mlp.c_proj.bias\", \"memory_cell.layers.9.layer.ln_1.weight\", \"memory_cell.layers.9.layer.ln_1.bias\", \"memory_cell.layers.9.layer.attn.c_attn.weight\", \"memory_cell.layers.9.layer.attn.c_attn.bias\", \"memory_cell.layers.9.layer.attn.c_proj.weight\", \"memory_cell.layers.9.layer.attn.c_proj.bias\", \"memory_cell.layers.9.layer.ln_2.weight\", \"memory_cell.layers.9.layer.ln_2.bias\", \"memory_cell.layers.9.layer.mlp.c_fc.weight\", \"memory_cell.layers.9.layer.mlp.c_fc.bias\", \"memory_cell.layers.9.layer.mlp.c_proj.weight\", \"memory_cell.layers.9.layer.mlp.c_proj.bias\", \"memory_cell.layers.10.layer.ln_1.weight\", \"memory_cell.layers.10.layer.ln_1.bias\", \"memory_cell.layers.10.layer.attn.c_attn.weight\", \"memory_cell.layers.10.layer.attn.c_attn.bias\", \"memory_cell.layers.10.layer.attn.c_proj.weight\", \"memory_cell.layers.10.layer.attn.c_proj.bias\", \"memory_cell.layers.10.layer.ln_2.weight\", \"memory_cell.layers.10.layer.ln_2.bias\", \"memory_cell.layers.10.layer.mlp.c_fc.weight\", \"memory_cell.layers.10.layer.mlp.c_fc.bias\", \"memory_cell.layers.10.layer.mlp.c_proj.weight\", \"memory_cell.layers.10.layer.mlp.c_proj.bias\", \"memory_cell.layers.11.layer.ln_1.weight\", \"memory_cell.layers.11.layer.ln_1.bias\", \"memory_cell.layers.11.layer.attn.c_attn.weight\", \"memory_cell.layers.11.layer.attn.c_attn.bias\", \"memory_cell.layers.11.layer.attn.c_proj.weight\", \"memory_cell.layers.11.layer.attn.c_proj.bias\", \"memory_cell.layers.11.layer.ln_2.weight\", \"memory_cell.layers.11.layer.ln_2.bias\", \"memory_cell.layers.11.layer.mlp.c_fc.weight\", \"memory_cell.layers.11.layer.mlp.c_fc.bias\", \"memory_cell.layers.11.layer.mlp.c_proj.weight\", \"memory_cell.layers.11.layer.mlp.c_proj.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m cpt_path = \u001b[33m\"\u001b[39m\u001b[33m/workspace-SR006.nfs2/bulatov/rmt/runs/gsm8k-MS1/gpt2-rmca/1x1024_mem4_1024-cot-v2-curr-zero_mem-wd5e-2/run1/checkpoint-1200/pytorch_model.bin\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m cpt = torch.load(cpt_path, map_location=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mrmt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace-SR006.nfs2/Bulatov_A/env_main/lib/python3.11/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for RMCAWrapperNoSegmentationGenerate:\n\tMissing key(s) in state_dict: \"memory_cell.model.transformer.h.0.layer.memory\", \"memory_cell.model.transformer.h.0.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.0.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.0.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.0.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.0.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.0.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.0.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.0.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.0.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.0.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.memory\", \"memory_cell.model.transformer.h.1.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.1.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.1.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.1.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.1.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.1.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.1.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.1.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.1.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.memory\", \"memory_cell.model.transformer.h.2.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.2.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.2.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.2.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.2.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.2.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.2.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.2.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.2.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.memory\", \"memory_cell.model.transformer.h.3.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.3.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.3.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.3.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.3.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.3.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.3.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.3.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.3.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.memory\", \"memory_cell.model.transformer.h.4.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.4.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.4.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.4.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.4.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.4.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.4.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.4.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.4.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.memory\", \"memory_cell.model.transformer.h.5.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.5.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.5.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.5.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.5.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.5.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.5.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.5.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.5.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.memory\", \"memory_cell.model.transformer.h.6.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.6.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.6.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.6.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.6.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.6.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.6.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.6.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.6.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.memory\", \"memory_cell.model.transformer.h.7.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.7.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.7.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.7.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.7.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.7.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.7.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.7.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.7.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.memory\", \"memory_cell.model.transformer.h.8.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.8.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.8.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.8.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.8.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.8.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.8.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.8.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.8.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.memory\", \"memory_cell.model.transformer.h.9.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.9.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.9.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.9.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.9.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.9.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.9.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.9.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.9.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.memory\", \"memory_cell.model.transformer.h.10.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.10.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.10.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.10.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.10.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.10.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.10.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.10.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.10.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.memory\", \"memory_cell.model.transformer.h.11.layer.layer.ln_1.weight\", \"memory_cell.model.transformer.h.11.layer.layer.ln_1.bias\", \"memory_cell.model.transformer.h.11.layer.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.11.layer.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.11.layer.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.layer.ln_2.weight\", \"memory_cell.model.transformer.h.11.layer.layer.ln_2.bias\", \"memory_cell.model.transformer.h.11.layer.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.11.layer.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.11.layer.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.ln.weight\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.ln.bias\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.ln.weight\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.ln.bias\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.0.layer.memory\", \"memory_cell.layers.0.layer.layer.ln_1.weight\", \"memory_cell.layers.0.layer.layer.ln_1.bias\", \"memory_cell.layers.0.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.0.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.0.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.0.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.0.layer.layer.ln_2.weight\", \"memory_cell.layers.0.layer.layer.ln_2.bias\", \"memory_cell.layers.0.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.0.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.0.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.0.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.0.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.0.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.0.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.0.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.0.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.0.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.1.layer.memory\", \"memory_cell.layers.1.layer.layer.ln_1.weight\", \"memory_cell.layers.1.layer.layer.ln_1.bias\", \"memory_cell.layers.1.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.1.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.1.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.1.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.1.layer.layer.ln_2.weight\", \"memory_cell.layers.1.layer.layer.ln_2.bias\", \"memory_cell.layers.1.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.1.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.1.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.1.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.1.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.1.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.1.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.1.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.1.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.1.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.2.layer.memory\", \"memory_cell.layers.2.layer.layer.ln_1.weight\", \"memory_cell.layers.2.layer.layer.ln_1.bias\", \"memory_cell.layers.2.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.2.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.2.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.2.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.2.layer.layer.ln_2.weight\", \"memory_cell.layers.2.layer.layer.ln_2.bias\", \"memory_cell.layers.2.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.2.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.2.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.2.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.2.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.2.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.2.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.2.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.2.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.2.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.3.layer.memory\", \"memory_cell.layers.3.layer.layer.ln_1.weight\", \"memory_cell.layers.3.layer.layer.ln_1.bias\", \"memory_cell.layers.3.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.3.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.3.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.3.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.3.layer.layer.ln_2.weight\", \"memory_cell.layers.3.layer.layer.ln_2.bias\", \"memory_cell.layers.3.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.3.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.3.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.3.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.3.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.3.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.3.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.3.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.3.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.3.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.4.layer.memory\", \"memory_cell.layers.4.layer.layer.ln_1.weight\", \"memory_cell.layers.4.layer.layer.ln_1.bias\", \"memory_cell.layers.4.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.4.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.4.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.4.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.4.layer.layer.ln_2.weight\", \"memory_cell.layers.4.layer.layer.ln_2.bias\", \"memory_cell.layers.4.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.4.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.4.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.4.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.4.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.4.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.4.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.4.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.4.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.4.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.5.layer.memory\", \"memory_cell.layers.5.layer.layer.ln_1.weight\", \"memory_cell.layers.5.layer.layer.ln_1.bias\", \"memory_cell.layers.5.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.5.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.5.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.5.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.5.layer.layer.ln_2.weight\", \"memory_cell.layers.5.layer.layer.ln_2.bias\", \"memory_cell.layers.5.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.5.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.5.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.5.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.5.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.5.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.5.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.5.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.5.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.5.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.6.layer.memory\", \"memory_cell.layers.6.layer.layer.ln_1.weight\", \"memory_cell.layers.6.layer.layer.ln_1.bias\", \"memory_cell.layers.6.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.6.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.6.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.6.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.6.layer.layer.ln_2.weight\", \"memory_cell.layers.6.layer.layer.ln_2.bias\", \"memory_cell.layers.6.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.6.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.6.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.6.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.6.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.6.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.6.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.6.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.6.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.6.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.7.layer.memory\", \"memory_cell.layers.7.layer.layer.ln_1.weight\", \"memory_cell.layers.7.layer.layer.ln_1.bias\", \"memory_cell.layers.7.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.7.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.7.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.7.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.7.layer.layer.ln_2.weight\", \"memory_cell.layers.7.layer.layer.ln_2.bias\", \"memory_cell.layers.7.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.7.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.7.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.7.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.7.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.7.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.7.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.7.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.7.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.7.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.8.layer.memory\", \"memory_cell.layers.8.layer.layer.ln_1.weight\", \"memory_cell.layers.8.layer.layer.ln_1.bias\", \"memory_cell.layers.8.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.8.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.8.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.8.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.8.layer.layer.ln_2.weight\", \"memory_cell.layers.8.layer.layer.ln_2.bias\", \"memory_cell.layers.8.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.8.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.8.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.8.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.8.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.8.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.8.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.8.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.8.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.8.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.9.layer.memory\", \"memory_cell.layers.9.layer.layer.ln_1.weight\", \"memory_cell.layers.9.layer.layer.ln_1.bias\", \"memory_cell.layers.9.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.9.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.9.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.9.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.9.layer.layer.ln_2.weight\", \"memory_cell.layers.9.layer.layer.ln_2.bias\", \"memory_cell.layers.9.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.9.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.9.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.9.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.9.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.9.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.9.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.9.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.9.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.9.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.10.layer.memory\", \"memory_cell.layers.10.layer.layer.ln_1.weight\", \"memory_cell.layers.10.layer.layer.ln_1.bias\", \"memory_cell.layers.10.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.10.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.10.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.10.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.10.layer.layer.ln_2.weight\", \"memory_cell.layers.10.layer.layer.ln_2.bias\", \"memory_cell.layers.10.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.10.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.10.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.10.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.10.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.10.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.10.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.10.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.10.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.10.layer.mem_write_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.11.layer.memory\", \"memory_cell.layers.11.layer.layer.ln_1.weight\", \"memory_cell.layers.11.layer.layer.ln_1.bias\", \"memory_cell.layers.11.layer.layer.attn.c_attn.weight\", \"memory_cell.layers.11.layer.layer.attn.c_attn.bias\", \"memory_cell.layers.11.layer.layer.attn.c_proj.weight\", \"memory_cell.layers.11.layer.layer.attn.c_proj.bias\", \"memory_cell.layers.11.layer.layer.ln_2.weight\", \"memory_cell.layers.11.layer.layer.ln_2.bias\", \"memory_cell.layers.11.layer.layer.mlp.c_fc.weight\", \"memory_cell.layers.11.layer.layer.mlp.c_fc.bias\", \"memory_cell.layers.11.layer.layer.mlp.c_proj.weight\", \"memory_cell.layers.11.layer.layer.mlp.c_proj.bias\", \"memory_cell.layers.11.layer.mem_read_layer.ln.weight\", \"memory_cell.layers.11.layer.mem_read_layer.ln.bias\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.11.layer.mem_read_layer.cross_attn.c_proj.bias\", \"memory_cell.layers.11.layer.mem_write_layer.ln.weight\", \"memory_cell.layers.11.layer.mem_write_layer.ln.bias\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.c_attn.weight\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.c_attn.bias\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.q_attn.weight\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.q_attn.bias\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.c_proj.weight\", \"memory_cell.layers.11.layer.mem_write_layer.cross_attn.c_proj.bias\". \n\tUnexpected key(s) in state_dict: \"memory_cell.model.transformer.h.0.layer.ln_1.weight\", \"memory_cell.model.transformer.h.0.layer.ln_1.bias\", \"memory_cell.model.transformer.h.0.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.0.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.0.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.0.layer.ln_2.weight\", \"memory_cell.model.transformer.h.0.layer.ln_2.bias\", \"memory_cell.model.transformer.h.0.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.0.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.0.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.0.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.ln_1.weight\", \"memory_cell.model.transformer.h.1.layer.ln_1.bias\", \"memory_cell.model.transformer.h.1.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.1.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.1.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.1.layer.ln_2.weight\", \"memory_cell.model.transformer.h.1.layer.ln_2.bias\", \"memory_cell.model.transformer.h.1.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.1.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.1.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.1.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.ln_1.weight\", \"memory_cell.model.transformer.h.2.layer.ln_1.bias\", \"memory_cell.model.transformer.h.2.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.2.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.2.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.2.layer.ln_2.weight\", \"memory_cell.model.transformer.h.2.layer.ln_2.bias\", \"memory_cell.model.transformer.h.2.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.2.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.2.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.2.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.ln_1.weight\", \"memory_cell.model.transformer.h.3.layer.ln_1.bias\", \"memory_cell.model.transformer.h.3.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.3.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.3.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.3.layer.ln_2.weight\", \"memory_cell.model.transformer.h.3.layer.ln_2.bias\", \"memory_cell.model.transformer.h.3.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.3.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.3.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.3.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.ln_1.weight\", \"memory_cell.model.transformer.h.4.layer.ln_1.bias\", \"memory_cell.model.transformer.h.4.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.4.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.4.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.4.layer.ln_2.weight\", \"memory_cell.model.transformer.h.4.layer.ln_2.bias\", \"memory_cell.model.transformer.h.4.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.4.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.4.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.4.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.ln_1.weight\", \"memory_cell.model.transformer.h.5.layer.ln_1.bias\", \"memory_cell.model.transformer.h.5.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.5.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.5.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.5.layer.ln_2.weight\", \"memory_cell.model.transformer.h.5.layer.ln_2.bias\", \"memory_cell.model.transformer.h.5.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.5.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.5.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.5.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.ln_1.weight\", \"memory_cell.model.transformer.h.6.layer.ln_1.bias\", \"memory_cell.model.transformer.h.6.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.6.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.6.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.6.layer.ln_2.weight\", \"memory_cell.model.transformer.h.6.layer.ln_2.bias\", \"memory_cell.model.transformer.h.6.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.6.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.6.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.6.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.ln_1.weight\", \"memory_cell.model.transformer.h.7.layer.ln_1.bias\", \"memory_cell.model.transformer.h.7.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.7.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.7.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.7.layer.ln_2.weight\", \"memory_cell.model.transformer.h.7.layer.ln_2.bias\", \"memory_cell.model.transformer.h.7.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.7.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.7.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.7.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.ln_1.weight\", \"memory_cell.model.transformer.h.8.layer.ln_1.bias\", \"memory_cell.model.transformer.h.8.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.8.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.8.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.8.layer.ln_2.weight\", \"memory_cell.model.transformer.h.8.layer.ln_2.bias\", \"memory_cell.model.transformer.h.8.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.8.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.8.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.8.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.ln_1.weight\", \"memory_cell.model.transformer.h.9.layer.ln_1.bias\", \"memory_cell.model.transformer.h.9.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.9.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.9.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.9.layer.ln_2.weight\", \"memory_cell.model.transformer.h.9.layer.ln_2.bias\", \"memory_cell.model.transformer.h.9.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.9.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.9.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.9.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.ln_1.weight\", \"memory_cell.model.transformer.h.10.layer.ln_1.bias\", \"memory_cell.model.transformer.h.10.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.10.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.10.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.10.layer.ln_2.weight\", \"memory_cell.model.transformer.h.10.layer.ln_2.bias\", \"memory_cell.model.transformer.h.10.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.10.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.10.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.10.layer.mlp.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.ln_1.weight\", \"memory_cell.model.transformer.h.11.layer.ln_1.bias\", \"memory_cell.model.transformer.h.11.layer.attn.c_attn.weight\", \"memory_cell.model.transformer.h.11.layer.attn.c_attn.bias\", \"memory_cell.model.transformer.h.11.layer.attn.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.attn.c_proj.bias\", \"memory_cell.model.transformer.h.11.layer.ln_2.weight\", \"memory_cell.model.transformer.h.11.layer.ln_2.bias\", \"memory_cell.model.transformer.h.11.layer.mlp.c_fc.weight\", \"memory_cell.model.transformer.h.11.layer.mlp.c_fc.bias\", \"memory_cell.model.transformer.h.11.layer.mlp.c_proj.weight\", \"memory_cell.model.transformer.h.11.layer.mlp.c_proj.bias\", \"memory_cell.layers.0.layer.ln_1.weight\", \"memory_cell.layers.0.layer.ln_1.bias\", \"memory_cell.layers.0.layer.attn.c_attn.weight\", \"memory_cell.layers.0.layer.attn.c_attn.bias\", \"memory_cell.layers.0.layer.attn.c_proj.weight\", \"memory_cell.layers.0.layer.attn.c_proj.bias\", \"memory_cell.layers.0.layer.ln_2.weight\", \"memory_cell.layers.0.layer.ln_2.bias\", \"memory_cell.layers.0.layer.mlp.c_fc.weight\", \"memory_cell.layers.0.layer.mlp.c_fc.bias\", \"memory_cell.layers.0.layer.mlp.c_proj.weight\", \"memory_cell.layers.0.layer.mlp.c_proj.bias\", \"memory_cell.layers.1.layer.ln_1.weight\", \"memory_cell.layers.1.layer.ln_1.bias\", \"memory_cell.layers.1.layer.attn.c_attn.weight\", \"memory_cell.layers.1.layer.attn.c_attn.bias\", \"memory_cell.layers.1.layer.attn.c_proj.weight\", \"memory_cell.layers.1.layer.attn.c_proj.bias\", \"memory_cell.layers.1.layer.ln_2.weight\", \"memory_cell.layers.1.layer.ln_2.bias\", \"memory_cell.layers.1.layer.mlp.c_fc.weight\", \"memory_cell.layers.1.layer.mlp.c_fc.bias\", \"memory_cell.layers.1.layer.mlp.c_proj.weight\", \"memory_cell.layers.1.layer.mlp.c_proj.bias\", \"memory_cell.layers.2.layer.ln_1.weight\", \"memory_cell.layers.2.layer.ln_1.bias\", \"memory_cell.layers.2.layer.attn.c_attn.weight\", \"memory_cell.layers.2.layer.attn.c_attn.bias\", \"memory_cell.layers.2.layer.attn.c_proj.weight\", \"memory_cell.layers.2.layer.attn.c_proj.bias\", \"memory_cell.layers.2.layer.ln_2.weight\", \"memory_cell.layers.2.layer.ln_2.bias\", \"memory_cell.layers.2.layer.mlp.c_fc.weight\", \"memory_cell.layers.2.layer.mlp.c_fc.bias\", \"memory_cell.layers.2.layer.mlp.c_proj.weight\", \"memory_cell.layers.2.layer.mlp.c_proj.bias\", \"memory_cell.layers.3.layer.ln_1.weight\", \"memory_cell.layers.3.layer.ln_1.bias\", \"memory_cell.layers.3.layer.attn.c_attn.weight\", \"memory_cell.layers.3.layer.attn.c_attn.bias\", \"memory_cell.layers.3.layer.attn.c_proj.weight\", \"memory_cell.layers.3.layer.attn.c_proj.bias\", \"memory_cell.layers.3.layer.ln_2.weight\", \"memory_cell.layers.3.layer.ln_2.bias\", \"memory_cell.layers.3.layer.mlp.c_fc.weight\", \"memory_cell.layers.3.layer.mlp.c_fc.bias\", \"memory_cell.layers.3.layer.mlp.c_proj.weight\", \"memory_cell.layers.3.layer.mlp.c_proj.bias\", \"memory_cell.layers.4.layer.ln_1.weight\", \"memory_cell.layers.4.layer.ln_1.bias\", \"memory_cell.layers.4.layer.attn.c_attn.weight\", \"memory_cell.layers.4.layer.attn.c_attn.bias\", \"memory_cell.layers.4.layer.attn.c_proj.weight\", \"memory_cell.layers.4.layer.attn.c_proj.bias\", \"memory_cell.layers.4.layer.ln_2.weight\", \"memory_cell.layers.4.layer.ln_2.bias\", \"memory_cell.layers.4.layer.mlp.c_fc.weight\", \"memory_cell.layers.4.layer.mlp.c_fc.bias\", \"memory_cell.layers.4.layer.mlp.c_proj.weight\", \"memory_cell.layers.4.layer.mlp.c_proj.bias\", \"memory_cell.layers.5.layer.ln_1.weight\", \"memory_cell.layers.5.layer.ln_1.bias\", \"memory_cell.layers.5.layer.attn.c_attn.weight\", \"memory_cell.layers.5.layer.attn.c_attn.bias\", \"memory_cell.layers.5.layer.attn.c_proj.weight\", \"memory_cell.layers.5.layer.attn.c_proj.bias\", \"memory_cell.layers.5.layer.ln_2.weight\", \"memory_cell.layers.5.layer.ln_2.bias\", \"memory_cell.layers.5.layer.mlp.c_fc.weight\", \"memory_cell.layers.5.layer.mlp.c_fc.bias\", \"memory_cell.layers.5.layer.mlp.c_proj.weight\", \"memory_cell.layers.5.layer.mlp.c_proj.bias\", \"memory_cell.layers.6.layer.ln_1.weight\", \"memory_cell.layers.6.layer.ln_1.bias\", \"memory_cell.layers.6.layer.attn.c_attn.weight\", \"memory_cell.layers.6.layer.attn.c_attn.bias\", \"memory_cell.layers.6.layer.attn.c_proj.weight\", \"memory_cell.layers.6.layer.attn.c_proj.bias\", \"memory_cell.layers.6.layer.ln_2.weight\", \"memory_cell.layers.6.layer.ln_2.bias\", \"memory_cell.layers.6.layer.mlp.c_fc.weight\", \"memory_cell.layers.6.layer.mlp.c_fc.bias\", \"memory_cell.layers.6.layer.mlp.c_proj.weight\", \"memory_cell.layers.6.layer.mlp.c_proj.bias\", \"memory_cell.layers.7.layer.ln_1.weight\", \"memory_cell.layers.7.layer.ln_1.bias\", \"memory_cell.layers.7.layer.attn.c_attn.weight\", \"memory_cell.layers.7.layer.attn.c_attn.bias\", \"memory_cell.layers.7.layer.attn.c_proj.weight\", \"memory_cell.layers.7.layer.attn.c_proj.bias\", \"memory_cell.layers.7.layer.ln_2.weight\", \"memory_cell.layers.7.layer.ln_2.bias\", \"memory_cell.layers.7.layer.mlp.c_fc.weight\", \"memory_cell.layers.7.layer.mlp.c_fc.bias\", \"memory_cell.layers.7.layer.mlp.c_proj.weight\", \"memory_cell.layers.7.layer.mlp.c_proj.bias\", \"memory_cell.layers.8.layer.ln_1.weight\", \"memory_cell.layers.8.layer.ln_1.bias\", \"memory_cell.layers.8.layer.attn.c_attn.weight\", \"memory_cell.layers.8.layer.attn.c_attn.bias\", \"memory_cell.layers.8.layer.attn.c_proj.weight\", \"memory_cell.layers.8.layer.attn.c_proj.bias\", \"memory_cell.layers.8.layer.ln_2.weight\", \"memory_cell.layers.8.layer.ln_2.bias\", \"memory_cell.layers.8.layer.mlp.c_fc.weight\", \"memory_cell.layers.8.layer.mlp.c_fc.bias\", \"memory_cell.layers.8.layer.mlp.c_proj.weight\", \"memory_cell.layers.8.layer.mlp.c_proj.bias\", \"memory_cell.layers.9.layer.ln_1.weight\", \"memory_cell.layers.9.layer.ln_1.bias\", \"memory_cell.layers.9.layer.attn.c_attn.weight\", \"memory_cell.layers.9.layer.attn.c_attn.bias\", \"memory_cell.layers.9.layer.attn.c_proj.weight\", \"memory_cell.layers.9.layer.attn.c_proj.bias\", \"memory_cell.layers.9.layer.ln_2.weight\", \"memory_cell.layers.9.layer.ln_2.bias\", \"memory_cell.layers.9.layer.mlp.c_fc.weight\", \"memory_cell.layers.9.layer.mlp.c_fc.bias\", \"memory_cell.layers.9.layer.mlp.c_proj.weight\", \"memory_cell.layers.9.layer.mlp.c_proj.bias\", \"memory_cell.layers.10.layer.ln_1.weight\", \"memory_cell.layers.10.layer.ln_1.bias\", \"memory_cell.layers.10.layer.attn.c_attn.weight\", \"memory_cell.layers.10.layer.attn.c_attn.bias\", \"memory_cell.layers.10.layer.attn.c_proj.weight\", \"memory_cell.layers.10.layer.attn.c_proj.bias\", \"memory_cell.layers.10.layer.ln_2.weight\", \"memory_cell.layers.10.layer.ln_2.bias\", \"memory_cell.layers.10.layer.mlp.c_fc.weight\", \"memory_cell.layers.10.layer.mlp.c_fc.bias\", \"memory_cell.layers.10.layer.mlp.c_proj.weight\", \"memory_cell.layers.10.layer.mlp.c_proj.bias\", \"memory_cell.layers.11.layer.ln_1.weight\", \"memory_cell.layers.11.layer.ln_1.bias\", \"memory_cell.layers.11.layer.attn.c_attn.weight\", \"memory_cell.layers.11.layer.attn.c_attn.bias\", \"memory_cell.layers.11.layer.attn.c_proj.weight\", \"memory_cell.layers.11.layer.attn.c_proj.bias\", \"memory_cell.layers.11.layer.ln_2.weight\", \"memory_cell.layers.11.layer.ln_2.bias\", \"memory_cell.layers.11.layer.mlp.c_fc.weight\", \"memory_cell.layers.11.layer.mlp.c_fc.bias\", \"memory_cell.layers.11.layer.mlp.c_proj.weight\", \"memory_cell.layers.11.layer.mlp.c_proj.bias\". "
     ]
    }
   ],
   "source": [
    "# cpt_path = \"/workspace-SR006.nfs2/bulatov/rmt/runs/multiplication_4x4/gpt2-rmca/1x1024_mem16_1024_LR1e-04-cot-v2/run_1/checkpoint-3500/pytorch_model.bin\"\n",
    "# cpt_path = \"/workspace-SR006.nfs2/bulatov/rmt/runs/gsm8k-MS1/gpt2-rmca/1x1024_mem64_1024-cot-v2-curr-zero_mem/run1/checkpoint-800/pytorch_model.bin\"\n",
    "cpt_path = \"/workspace-SR006.nfs2/bulatov/rmt/runs/gsm8k-MS1/gpt2-rmca/1x1024_mem4_1024-cot-v2-curr-zero_mem-wd5e-2/run1/checkpoint-1200/pytorch_model.bin\"\n",
    "cpt = torch.load(cpt_path, map_location='cpu')\n",
    "\n",
    "rmt.load_state_dict(cpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181763328, 181763328)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Get the total number of parameters\n",
    "count_parameters(base_model), count_parameters(rmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in rmt.named_parameters():\n",
    "#     print(n, p.shape, p.requires_grad, p.grad.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = rmt(**collated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer.mem_read_layer.cross_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMCrossAttention(\n",
       "  (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (cross_attn): GPT2Attention(\n",
       "    (c_attn): Conv1D(nf=1536, nx=768)\n",
       "    (q_attn): Conv1D(nf=768, nx=768)\n",
       "    (c_proj): Conv1D(nf=768, nx=768)\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.memory_cell.model.transformer.h[0].mem_read_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in rmt.memory_cell.model.transformer.h:\n",
    "    layer.mem_read_layer.cross_attn.c_proj.weight.data.zero_()\n",
    "    layer.mem_write_layer.cross_attn.c_proj.weight.data.zero_()\n",
    "    # print(layer.mem_read_layer.cross_attn.c_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "self = rmt\n",
    "segments = collated['segments']\n",
    "output_attentions = True\n",
    "output_hidden_states = True\n",
    "\n",
    "\n",
    "debug_states = {}\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segments):\n",
    "    cell_out = self.memory_cell(input_ids=segment['input_ids'],\n",
    "                                attention_mask=segment['attention_mask'],\n",
    "                                output_attentions=output_attentions,\n",
    "                                output_hidden_states=True)\n",
    "    cell_outputs.append(cell_out)\n",
    "    self.manage_gradients(seg_num)\n",
    "    # break\n",
    "    debug_states[seg_num] = [layer.debug_state for layer in rmt.memory_cell.model.transformer.h]\n",
    "    debug_states[f\"{seg_num}-read\"] = [layer.mem_read_layer.debug_state for layer in rmt.memory_cell.model.transformer.h]\n",
    "    debug_states[f\"{seg_num}-write\"] = [layer.mem_write_layer.debug_state for layer in rmt.memory_cell.model.transformer.h]\n",
    "    \n",
    "\n",
    "# out = self.process_outputs(cell_outputs, segments,\n",
    "#                             output_attentions=output_attentions,\n",
    "#                             output_hidden_states=output_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collated['segments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys([0, '0-read', '0-write', 1, '1-read', '1-write', 2, '2-read', '2-write']),\n",
       " dict_keys(['inp_hidden_states', 'mem_read_out', 'mem_write_out', 'memory_state', 'output', 'cross_attentions_read', 'cross_attentions_write']),\n",
       " dict_keys(['encoder_hidden_states', 'inp_hidden_states', 'output', 'attn_output', 'cross_attn_outputs']))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_states.keys(), debug_states[0][0].keys(), debug_states['0-read'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(layer_state[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inp_hidden_states\n",
      "-0.0005578971467912197 0.17760612070560455 -4.4091796875 3.728515625\n",
      "-0.006232638843357563 2.0028131008148193 -71.403564453125 104.01599884033203\n",
      "-0.006312801968306303 3.349181652069092 -73.46725463867188 593.6646728515625\n",
      "0.03301436826586723 11.159987449645996 -73.06352233886719 2476.620849609375\n",
      "0.044311583042144775 11.908797264099121 -72.47608947753906 2634.2236328125\n",
      "0.058057330548763275 12.55832290649414 -71.00603485107422 2774.500732421875\n",
      "0.07055488973855972 12.982148170471191 -66.20497131347656 2867.1318359375\n",
      "0.0876457616686821 13.284734725952148 -60.688133239746094 2922.619140625\n",
      "0.11958087235689163 13.608667373657227 -55.6173095703125 2956.922607421875\n",
      "0.13541331887245178 13.890575408935547 -54.81237030029297 2977.7724609375\n",
      "0.16596980392932892 14.439210891723633 -79.15882110595703 2988.052978515625\n",
      "0.2772412598133087 15.742889404296875 -151.67889404296875 2989.009033203125\n",
      "\n",
      "encoder_hidden_states\n",
      "0.009510686621069908 0.9964831471443176 -4.25 4.125\n",
      "0.004937547259032726 1.0003727674484253 -4.53125 4.28125\n",
      "-0.005025223828852177 1.0005842447280884 -4.0625 4.46875\n",
      "0.007071167230606079 0.9953972101211548 -4.28125 4.28125\n",
      "0.001245675957761705 0.995232343673706 -4.125 4.4375\n",
      "0.0032816901803016663 0.9993728399276733 -3.84375 4.625\n",
      "0.00117669312749058 1.0020911693572998 -4.4375 4.34375\n",
      "0.0009203761001117527 0.9988435506820679 -4.0 4.9375\n",
      "-0.0010230112820863724 1.002326250076294 -4.1875 4.71875\n",
      "-0.0005577646661549807 0.9986088871955872 -4.25 4.25\n",
      "0.009414607658982277 1.0033951997756958 -4.40625 4.71875\n",
      "-0.008952823467552662 0.9987989068031311 -4.75 4.90625\n",
      "\n",
      "attn_output\n",
      "8.76641643117182e-05 0.018165364861488342 -0.05099595710635185 0.04779248312115669\n",
      "-0.00023310247343033552 0.015782130882143974 -0.03932223096489906 0.0405038483440876\n",
      "0.00010287768964190036 0.01016244851052761 -0.03183049336075783 0.02891780249774456\n",
      "4.579662345349789e-05 0.005718375090509653 -0.01725413277745247 0.0154552748426795\n",
      "0.00011618864664342254 0.005030614789575338 -0.014492812566459179 0.01645543985068798\n",
      "0.00013677288370672613 0.00524858059361577 -0.014249192550778389 0.01494088489562273\n",
      "0.00012789785978384316 0.005356643348932266 -0.015617088414728642 0.013327045366168022\n",
      "2.90858733933419e-05 0.0047801798209548 -0.013399536721408367 0.013099517673254013\n",
      "-4.670779162552208e-05 0.005203439854085445 -0.012233392335474491 0.013603593222796917\n",
      "-2.197135700043873e-06 0.005344099830836058 -0.012423127889633179 0.01531631126999855\n",
      "5.103611692902632e-05 0.005266713444143534 -0.014951958321034908 0.015157275833189487\n",
      "0.00012185548985144123 0.005203568376600742 -0.014266024343669415 0.012593068182468414\n"
     ]
    }
   ],
   "source": [
    "seg_num = \"0-read\"\n",
    "# seg_num = \"0-write\"\n",
    "\n",
    "keys = ['inp_hidden_states', 'encoder_hidden_states', 'attn_output']#, 'cross_attn_outputs'][-1:]\n",
    "segment_states = debug_states[seg_num]\n",
    "for key in keys:\n",
    "    print()\n",
    "    print(key)\n",
    "    for layer_state in segment_states:\n",
    "        mean_ = layer_state[key].mean()\n",
    "        std_ = layer_state[key].std()\n",
    "        min_ = layer_state[key].min()\n",
    "        max_ = layer_state[key].max()\n",
    "        print(mean_.item(), std_.item(), min_.item(), max_.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inp_hidden_states\n",
      "-0.0004702328587882221 0.17899121344089508 -4.4458794593811035 3.7651281356811523\n",
      "-0.006465740501880646 2.001833915710449 -71.40843200683594 104.05224609375\n",
      "-0.00620992248877883 3.348708391189575 -73.4843978881836 593.6775512695312\n",
      "0.033060163259506226 11.159908294677734 -73.0713882446289 2476.62158203125\n",
      "0.04442776367068291 11.908841133117676 -72.48859405517578 2634.23876953125\n",
      "0.058194100856781006 12.558292388916016 -71.01464080810547 2774.5048828125\n",
      "0.07068280130624771 12.982096672058105 -66.1973876953125 2867.130126953125\n",
      "0.08767484873533249 13.284655570983887 -60.688568115234375 2922.61328125\n",
      "0.11953415721654892 13.608614921569824 -55.6114501953125 2956.91943359375\n",
      "0.1354111284017563 13.89045524597168 -54.79837417602539 2977.76708984375\n",
      "0.16602084040641785 14.438973426818848 -79.14502716064453 2988.04638671875\n",
      "0.277363121509552 15.742731094360352 -151.67041015625 2989.01025390625\n",
      "\n",
      "memory_state\n",
      "0.011711889877915382 1.0000689029693604 -4.303404331207275 4.176929950714111\n",
      "-0.015840161591768265 1.0709645748138428 -4.155057430267334 4.567733287811279\n",
      "0.004346752539277077 1.2105522155761719 -4.660529136657715 4.735461235046387\n",
      "0.06336787343025208 3.104865789413452 -11.642803192138672 11.837564468383789\n",
      "0.08167531341314316 2.70879864692688 -9.278943061828613 9.417762756347656\n",
      "-0.07511446624994278 3.294074296951294 -15.03154182434082 14.245865821838379\n",
      "0.09688576310873032 3.111447811126709 -13.893301963806152 13.60698127746582\n",
      "-0.026659835129976273 3.0087759494781494 -11.766084671020508 10.42192268371582\n",
      "-0.10253690183162689 3.337477445602417 -14.270828247070312 15.214855194091797\n",
      "-0.018191367387771606 3.2470316886901855 -11.640693664550781 15.037586212158203\n",
      "-0.13722307980060577 3.371931314468384 -12.615863800048828 13.55610179901123\n",
      "-0.19482733309268951 3.4398415088653564 -12.110039710998535 12.217391967773438\n"
     ]
    }
   ],
   "source": [
    "seg_num = 0\n",
    "\n",
    "keys = ['inp_hidden_states', 'memory_state']#, 'output']\n",
    "segment_states = debug_states[seg_num]\n",
    "for key in keys:\n",
    "    print()\n",
    "    print(key)\n",
    "    for layer_state in segment_states:\n",
    "        mean_ = layer_state[key].mean()\n",
    "        std_ = layer_state[key].std()\n",
    "        min_ = layer_state[key].min()\n",
    "        max_ = layer_state[key].max()\n",
    "        print(mean_.item(), std_.item(), min_.item(), max_.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inp_hidden_states\n",
      "-0.0020966017618775368 0.2840779423713684 -4.481912612915039 3.8016772270202637\n",
      "0.05370934680104256 3.8139808177948 -80.22399139404297 118.39396667480469\n",
      "0.5015147924423218 16.520660400390625 -89.22883605957031 665.1481323242188\n",
      "3.3518521785736084 83.21925354003906 -90.10358428955078 2538.81689453125\n",
      "3.580336093902588 87.45240783691406 -90.94190216064453 2770.834228515625\n",
      "3.732922315597534 90.58382415771484 -89.65648651123047 2937.01416015625\n",
      "3.813560724258423 92.4358901977539 -83.88822937011719 3046.693115234375\n",
      "3.85661244392395 93.54927825927734 -77.65959167480469 3111.789306640625\n",
      "3.8736605644226074 94.2428970336914 -71.942626953125 3151.786376953125\n",
      "3.8853845596313477 94.51702117919922 -64.370849609375 3176.25537109375\n",
      "3.901632308959961 94.51990509033203 -54.785762786865234 3187.723876953125\n",
      "3.9437255859375 94.15647888183594 -41.6655158996582 3191.5703125\n",
      "\n",
      "memory_state\n",
      "0.015733545646071434 1.0116857290267944 -4.403924465179443 4.273698329925537\n",
      "-0.05159401148557663 1.3903632164001465 -4.809098720550537 5.382144451141357\n",
      "0.026456495746970177 2.1985690593719482 -7.738059997558594 7.673688888549805\n",
      "0.2605680227279663 11.464685440063477 -35.51163864135742 34.04791259765625\n",
      "0.37183088064193726 11.059209823608398 -26.332674026489258 29.13799285888672\n",
      "-0.37355008721351624 14.098559379577637 -42.306549072265625 44.438106536865234\n",
      "0.4688200354576111 13.274421691894531 -39.01564025878906 42.522499084472656\n",
      "-0.11683007329702377 10.497055053710938 -29.540367126464844 26.679485321044922\n",
      "-0.528981626033783 15.433438301086426 -48.575775146484375 54.03559112548828\n",
      "-0.07195029407739639 14.654911994934082 -39.144432067871094 53.256195068359375\n",
      "-0.6724036335945129 13.347427368164062 -36.61356735229492 39.03394317626953\n",
      "-0.849352240562439 14.165754318237305 -41.977516174316406 40.56138229370117\n"
     ]
    }
   ],
   "source": [
    "seg_num = 2\n",
    "\n",
    "keys = ['inp_hidden_states', 'memory_state']#, 'output']\n",
    "segment_states = debug_states[seg_num]\n",
    "for key in keys:\n",
    "    print()\n",
    "    print(key)\n",
    "    for layer_state in segment_states:\n",
    "        mean_ = layer_state[key].mean()\n",
    "        std_ = layer_state[key].std()\n",
    "        min_ = layer_state[key].min()\n",
    "        max_ = layer_state[key].max()\n",
    "        print(mean_.item(), std_.item(), min_.item(), max_.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'debug_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m seg_num = \u001b[32m7\u001b[39m\n\u001b[32m      3\u001b[39m keys = [\u001b[33m'\u001b[39m\u001b[33minp_hidden_states\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmemory_state\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;66;03m#, 'output']\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m segment_states = \u001b[43mdebug_states\u001b[49m[seg_num]\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'debug_states' is not defined"
     ]
    }
   ],
   "source": [
    "seg_num = 7\n",
    "\n",
    "keys = ['inp_hidden_states', 'memory_state']#, 'output']\n",
    "segment_states = debug_states[seg_num]\n",
    "for key in keys:\n",
    "    print()\n",
    "    print(key)\n",
    "    for layer_state in segment_states:\n",
    "        mean_ = layer_state[key].mean()\n",
    "        std_ = layer_state[key].std()\n",
    "        min_ = layer_state[key].min()\n",
    "        max_ = layer_state[key].max()\n",
    "        print(mean_.item(), std_.item(), min_.item(), max_.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(-0.0063, grad_fn=<MeanBackward0>),\n",
       "  tensor(0.6333, grad_fn=<StdBackward0>)),\n",
       " (tensor(-0.5642, grad_fn=<MeanBackward0>),\n",
       "  tensor(29.4408, grad_fn=<StdBackward0>)),\n",
       " (tensor(-1.4124, grad_fn=<MeanBackward0>),\n",
       "  tensor(50.7379, grad_fn=<StdBackward0>)),\n",
       " (tensor(-1.1679, grad_fn=<MeanBackward0>),\n",
       "  tensor(72.6272, grad_fn=<StdBackward0>)),\n",
       " (tensor(-0.2154, grad_fn=<MeanBackward0>),\n",
       "  tensor(98.1661, grad_fn=<StdBackward0>)),\n",
       " (tensor(-1.2143, grad_fn=<MeanBackward0>),\n",
       "  tensor(120.1388, grad_fn=<StdBackward0>)),\n",
       " (tensor(-1.6876, grad_fn=<MeanBackward0>),\n",
       "  tensor(134.8322, grad_fn=<StdBackward0>)),\n",
       " (tensor(-2.3790, grad_fn=<MeanBackward0>),\n",
       "  tensor(147.0923, grad_fn=<StdBackward0>)),\n",
       " (tensor(-2.9443, grad_fn=<MeanBackward0>),\n",
       "  tensor(164.2453, grad_fn=<StdBackward0>)),\n",
       " (tensor(-4.4038, grad_fn=<MeanBackward0>),\n",
       "  tensor(179.6345, grad_fn=<StdBackward0>)),\n",
       " (tensor(-3.0699, grad_fn=<MeanBackward0>),\n",
       "  tensor(196.0588, grad_fn=<StdBackward0>)),\n",
       " (tensor(-4.7097, grad_fn=<MeanBackward0>),\n",
       "  tensor(215.2324, grad_fn=<StdBackward0>))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(layer_state['hidden_states'].mean(), layer_state['hidden_states'].std()) for layer_state in debug_states[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768]),\n",
       " torch.Size([10, 115, 768])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cell_out.hidden_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'memory_state', 'output', 'cross_attentions_read', 'cross_attentions_write'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_ind = 0\n",
    "layer = rmt.memory_cell.model.transformer.h[0]\n",
    "layer.debug_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer.debug_state['memory_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(layer.debug_state['cross_attentions_read'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Conv1D(nf=1536, nx=768)\n",
       "  (q_attn): Conv1D(nf=768, nx=768)\n",
       "  (c_proj): Conv1D(nf=768, nx=768)\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.memory_cell.model.transformer.h[0].mem_read_layer.cross_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = Holder()\n",
    "# args.use_cot = False\n",
    "args.num_mem_tokens = None\n",
    "args.task_name = 'gsm8k'\n",
    "# args.task_name = 'multiplication'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "think = tokenizer.encode('????')\n",
    "ans = tokenizer.encode('!!!!')\n",
    "eos = [tokenizer.eos_token_id]\n",
    "if 'gsm8k' in args.task_name:\n",
    "    delim = \">> <<\"\n",
    "elif 'multiplication' in args.task_name:\n",
    "    delim = ' + '\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unknown task name {args.task_name}\")\n",
    "\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "bos = tokenizer.encode('////')\n",
    "think = tokenizer.encode('????')\n",
    "ans = tokenizer.encode('!!!!')\n",
    "eos = [tokenizer.eos_token_id]\n",
    "if 'gsm8k' in args.task_name:\n",
    "    delim = \">> <<\"\n",
    "elif 'multiplication' in args.task_name:\n",
    "    delim = ' + '\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unknown task name {args.task_name}\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # first, we segment each sample into task, cot steps and labels\n",
    "    segments_batch = []\n",
    "    for sample in batch:\n",
    "        task, lab, cot = sample['task'], sample['labels'], sample['cot']\n",
    "        task_tokens = tokenizer.encode(task, add_special_tokens=False)\n",
    "        labels_tokens = tokenizer.encode(lab, add_special_tokens=False)\n",
    "        cot_segments = split_cot(cot, by=delim)\n",
    "        cot_segment_tokens = tokenizer.batch_encode_plus(cot_segments, add_special_tokens=False)['input_ids']\n",
    "\n",
    "        segments = []\n",
    "        segments.append(make_segment(bos + task_tokens + think, loss=False))\n",
    "        for segment in cot_segment_tokens[:-1]:\n",
    "            segments.append(make_segment(bos + segment + think, loss=True))\n",
    "        segments.append(make_segment(bos + cot_segment_tokens[-1] + ans, loss=True))\n",
    "\n",
    "        segments.append(make_segment(bos + labels_tokens + eos, loss=True))\n",
    "        segments_batch.append(segments)\n",
    "\n",
    "    # if some samples have less segments than others, we pad them with empty segments\n",
    "    num_segments = max(len(segments) for segments in segments_batch)\n",
    "    for segments in segments_batch:\n",
    "        if len(segments) < num_segments:\n",
    "            segments.extend([make_segment(eos, loss=False)] * (num_segments - len(segments)))\n",
    "\n",
    "    # prepare segments for the whole batch\n",
    "    batch_segments = []\n",
    "    for i in range(num_segments):\n",
    "        input_ids = [s[i]['input_ids'] for s in segments_batch]\n",
    "        attention_mask = [s[i]['attention_mask'] for s in segments_batch]\n",
    "        labels = [s[i]['labels'] for s in segments_batch]\n",
    "        labels_mask = [s[i]['labels_mask'] for s in segments_batch]\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=id_pad_value)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        labels_mask = pad_sequence(labels_mask, batch_first=True, padding_value=False)\n",
    "\n",
    "        batch_segment = {'input_ids': input_ids,\n",
    "                            'attention_mask': attention_mask,\n",
    "                            'labels_mask': labels_mask,\n",
    "                            'labels': labels\n",
    "                            }\n",
    "        batch_segments.append(batch_segment)\n",
    "    full_labels = torch.cat([s['labels'] for s in batch_segments], dim=1)\n",
    "    return {\"segments\": batch_segments, 'labels': full_labels}\n",
    "\n",
    "\n",
    "# dataset = 'booydar/gsm8k'\n",
    "dataset = '/workspace-SR006.nfs2/bulatov/rmt/data/gsm8k'\n",
    "# dataset = 'booydar/multiplication_4x4'\n",
    "# dataset = f\"booydar/{args.task_name}\"\n",
    "train_dataset = datasets.load_dataset(dataset, split='train')\n",
    "valid_dataset = datasets.load_dataset(dataset, split='valid')\n",
    "\n",
    "args.max_cot_steps = 1\n",
    "if args.max_cot_steps is not None:\n",
    "    train_dataset = train_dataset.filter(lambda x: x['cot_len'] <= args.max_cot_steps)\n",
    "    valid_dataset = valid_dataset.filter(lambda x: x['cot_len'] <= args.max_cot_steps)\n",
    "    # test_dataset = test_dataset.filter(lambda x: x['cot_len'] <= args.max_cot_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [valid_dataset[i] for i in range(10)]\n",
    "collated = collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "out = rmt(**collated, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('gpt2')\n",
    "base_model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_config = config\n",
    "block_config.add_cross_attention=True\n",
    "block = GPT2Block(block_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my block\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.rand(4, 32, config.n_embd)\n",
    "ca_hidden_states = torch.rand(4, 10, config.n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = block(hidden_states, encoder_hidden_states=ca_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0185, -0.0107, -0.0151,  ..., -0.0018, -0.0174, -0.0430],\n",
       "        [-0.0091, -0.0032,  0.0377,  ..., -0.0201,  0.0110,  0.0046],\n",
       "        [ 0.0122, -0.0052, -0.0007,  ..., -0.0021,  0.0069, -0.0088],\n",
       "        ...,\n",
       "        [ 0.0200,  0.0150, -0.0071,  ...,  0.0020,  0.0151, -0.0020],\n",
       "        [-0.0079, -0.0102, -0.0498,  ...,  0.0047, -0.0243,  0.0262],\n",
       "        [-0.0123,  0.0150,  0.0093,  ..., -0.0046,  0.0151, -0.0227]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.mlp.c_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0082,  0.0061,  0.0125,  ..., -0.0002, -0.0164, -0.0277],\n",
       "        [-0.0287, -0.0139, -0.0143,  ...,  0.0064, -0.0097, -0.0254],\n",
       "        [ 0.0210,  0.0078, -0.0080,  ..., -0.0346,  0.0157,  0.0078],\n",
       "        ...,\n",
       "        [ 0.0356, -0.0018,  0.0124,  ...,  0.0365, -0.0038, -0.0041],\n",
       "        [-0.0207,  0.0009, -0.0061,  ..., -0.0011,  0.0134,  0.0031],\n",
       "        [ 0.0092,  0.0036, -0.0126,  ...,  0.0296,  0.0002,  0.0204]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0182, -0.0330,  0.0011,  ..., -0.0458,  0.0089,  0.0296],\n",
       "        [-0.0164,  0.0429,  0.0111,  ..., -0.0027,  0.0223, -0.0236],\n",
       "        [-0.0230,  0.0143,  0.0186,  ..., -0.0436,  0.0020,  0.0011],\n",
       "        ...,\n",
       "        [ 0.0060, -0.0180,  0.0040,  ...,  0.0082,  0.0026, -0.0469],\n",
       "        [-0.0089, -0.0230,  0.0076,  ...,  0.0394,  0.0006,  0.0032],\n",
       "        [-0.0021,  0.0327, -0.0022,  ..., -0.0012, -0.0276,  0.0112]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.transformer.h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Conv1D(nf=2304, nx=768)\n",
       "  (c_proj): Conv1D(nf=768, nx=768)\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.nn as nn\n",
    "\n",
    "def clone_layer_fresh(layer: nn.Module, reset_weights=False) -> nn.Module:\n",
    "    new_layer = copy.deepcopy(layer)\n",
    "\n",
    "    def recursive_reset(module: nn.Module):\n",
    "        for child in module.children():\n",
    "            print(child)\n",
    "            recursive_reset(child)\n",
    "        # if hasattr(module, 'reset_parameters'):\n",
    "            module.reset_parameters()\n",
    "        \n",
    "        return module\n",
    "\n",
    "    if reset_weights:\n",
    "        new_layer = recursive_reset(new_layer)\n",
    "    return new_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_paraa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2Block' object has no attribute 'reset_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset_parameters\u001b[49m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace-SR006.nfs2/Bulatov_A/env_main/lib/python3.11/site-packages/torch/nn/modules/module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'GPT2Block' object has no attribute 'reset_parameters'"
     ]
    }
   ],
   "source": [
    "layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPT2Block' object has no attribute 'reset_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cloned = \u001b[43mclone_layer_fresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mclone_layer_fresh\u001b[39m\u001b[34m(layer, reset_weights)\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reset_weights:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     new_layer = \u001b[43mrecursive_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_layer\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mclone_layer_fresh.<locals>.recursive_reset\u001b[39m\u001b[34m(module)\u001b[39m\n\u001b[32m     10\u001b[39m     recursive_reset(child)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# if hasattr(module, 'reset_parameters'):\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset_parameters\u001b[49m()\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace-SR006.nfs2/Bulatov_A/env_main/lib/python3.11/site-packages/torch/nn/modules/module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'GPT2Block' object has no attribute 'reset_parameters'"
     ]
    }
   ],
   "source": [
    "cloned = clone_layer_fresh(layer, reset_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
       "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
       "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
       "        ...,\n",
       "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
       "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
       "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.attn.c_attn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
       "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
       "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
       "        ...,\n",
       "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
       "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
       "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloned.attn.c_attn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config == layer.attn.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModel.from_config(layer.attn.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = Holder()\n",
    "# args.use_cot = False\n",
    "args.num_mem_tokens = None\n",
    "# args.task_name = 'gsm8k'\n",
    "args.task_name = 'multiplication'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "think = tokenizer.encode('????')\n",
    "ans = tokenizer.encode('!!!!')\n",
    "eos = [tokenizer.eos_token_id]\n",
    "if 'gsm8k' in args.task_name:\n",
    "    delim = \">> <<\"\n",
    "elif 'multiplication' in args.task_name:\n",
    "    delim = ' + '\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unknown task name {args.task_name}\")\n",
    "\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "think = tokenizer.encode('????')\n",
    "bos = tokenizer.encode('////')\n",
    "ans = tokenizer.encode('!!!!')\n",
    "eos = [tokenizer.eos_token_id]\n",
    "if 'gsm8k' in args.task_name:\n",
    "    delim = \">> <<\"\n",
    "elif 'multiplication' in args.task_name:\n",
    "    delim = ' + '\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unknown task name {args.task_name}\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # first, we segment each sample into task, cot steps and labels\n",
    "    segments_batch = []\n",
    "    for sample in batch:\n",
    "        task, lab, cot = sample['task'], sample['labels'], sample['cot']\n",
    "        task_tokens = tokenizer.encode(task, add_special_tokens=False)\n",
    "        labels_tokens = tokenizer.encode(lab, add_special_tokens=False)\n",
    "        cot_segments = split_cot(cot, by=delim)\n",
    "        cot_segment_tokens = tokenizer.batch_encode_plus(cot_segments, add_special_tokens=False)['input_ids']\n",
    "\n",
    "        segments = []\n",
    "        segments.append(make_segment(bos + task_tokens + think, loss=False))\n",
    "        for segment in cot_segment_tokens[:-1]:\n",
    "            segments.append(make_segment(bos + segment + think, loss=True))\n",
    "        segments.append(make_segment(bos + cot_segment_tokens[-1] + ans, loss=True))\n",
    "\n",
    "        segments.append(make_segment(bos + labels_tokens + eos, loss=True))\n",
    "        segments_batch.append(segments)\n",
    "\n",
    "    # if some samples have less segments than others, we pad them with empty segments\n",
    "    num_segments = max(len(segments) for segments in segments_batch)\n",
    "    for segments in segments_batch:\n",
    "        if len(segments) < num_segments:\n",
    "            segments.extend([make_segment(eos, loss=False)] * (num_segments - len(segments)))\n",
    "\n",
    "    # prepare segments for the whole batch\n",
    "    batch_segments = []\n",
    "    for i in range(num_segments):\n",
    "        input_ids = [s[i]['input_ids'] for s in segments_batch]\n",
    "        attention_mask = [s[i]['attention_mask'] for s in segments_batch]\n",
    "        labels = [s[i]['labels'] for s in segments_batch]\n",
    "        labels_mask = [s[i]['labels_mask'] for s in segments_batch]\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=id_pad_value)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        labels_mask = pad_sequence(labels_mask, batch_first=True, padding_value=False)\n",
    "\n",
    "        batch_segment = {'input_ids': input_ids,\n",
    "                            'attention_mask': attention_mask,\n",
    "                            'labels_mask': labels_mask,\n",
    "                            'labels': labels\n",
    "                            }\n",
    "        batch_segments.append(batch_segment)\n",
    "    full_labels = torch.cat([s['labels'] for s in batch_segments], dim=1)\n",
    "    return {\"segments\": batch_segments, 'labels': full_labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since booydar/multiplication_4x4 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/jovyan/.cache/huggingface/datasets/booydar___multiplication_4x4/default/0.0.0/e224243bb9970a0937976173287d24816d683ff9 (last modified on Fri Mar 14 16:54:33 2025).\n",
      "Using the latest cached version of the dataset since booydar/multiplication_4x4 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/jovyan/.cache/huggingface/datasets/booydar___multiplication_4x4/default/0.0.0/e224243bb9970a0937976173287d24816d683ff9 (last modified on Fri Mar 14 16:54:33 2025).\n"
     ]
    }
   ],
   "source": [
    "# dataset = 'booydar/gsm8k'\n",
    "dataset = 'booydar/multiplication_4x4'\n",
    "# dataset = f\"booydar/{args.task_name}\"\n",
    "train_dataset = datasets.load_dataset(dataset, split='train')\n",
    "valid_dataset = datasets.load_dataset(dataset, split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': '5 6 3 2 * 7 4 3 4',\n",
       " 'labels': '5 5 6 0 8 2 0 1',\n",
       " 'cot': '5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2LMHeadModel' object has no attribute 'ge'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mge\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace-SR006.nfs2/Bulatov_A/env_main/lib/python3.11/site-packages/torch/nn/modules/module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'GPT2LMHeadModel' object has no attribute 'ge'"
     ]
    }
   ],
   "source": [
    "model.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory_cell = MemoryCell(\n",
    "    model,\n",
    "    num_mem_tokens=16\n",
    ")\n",
    "rmt = RecurrentWrapperNoSegmentation(memory_cell, max_n_segments=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":)\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_path = \"/workspace-SR006.nfs2/bulatov/rmt/runs/gsm8k/gpt2/SEGM_1x1024_1024_LR3e-04-cot/checkpoint-17500/pytorch_model.bin\"\n",
    "# checkpoint_path = \"/workspace-SR006.nfs2/bulatov/rmt/runs/multiplication_4x4/gpt2/1x1024_mem4_1024_LR6e-04/checkpoint-124500/pytorch_model.bin\"\n",
    "# checkpoint_path = \"/workspace-SR006.nfs2/bulatov/rmt/runs/multiplication_4x4/gpt2/1x1024_mem16_1024_LR1e-03-v2/checkpoint-78000/pytorch_model.bin\"\n",
    "# checkpoint_path = \"/workspace-SR006.nfs2/bulatov/rmt/runs/multiplication_4x4/gpt2/1x1024_mem16_1024_LR1e-03-cot-v2/checkpoint-60000/pytorch_model.bin\"\n",
    "# checkpoint_path = \"/workspace-SR006.nfs2/bulatov/rmt/runs/multiplication_4x4/gpt2/1x1024_mem16_1024_LR1e-03-cot-v2/checkpoint-90000/pytorch_model.bin\"\n",
    "\n",
    "# checkpoint_path = \"/workspace-SR006.nfs2/bulatov/rmt/runs/gsm8k/gpt2/1x1024_mem4_1024_LR3e-03-cot-v2/checkpoint-47500/pytorch_model.bin\"\n",
    "device = 'cpu'\n",
    "\n",
    "rmt.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "# rmt.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "rmt.to(device)\n",
    "print(':)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_cot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collated = collate_fn([sample for sample in valid_dataset.select(range(128))])\n",
    "# collated = collate_fn([sample for sample in train_dataset.select(range(16))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = collated['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.batch_decode(segments[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 11]) torch.Size([128, 11])\n",
      "torch.Size([128, 7]) torch.Size([128, 7])\n",
      "torch.Size([128, 16]) torch.Size([128, 16])\n",
      "torch.Size([128, 18]) torch.Size([128, 18])\n",
      "torch.Size([128, 10]) torch.Size([128, 10])\n",
      "torch.Size([128, 10]) torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(segments)):\n",
    "    print(segments[i]['input_ids'].shape, segments[i]['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = rmt(**collated)\n",
    "self = rmt\n",
    "segments = collated['segments']\n",
    "\n",
    "memory_state = None\n",
    "\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segments):\n",
    "    cell_out, memory_state = self.memory_cell(input_ids=segment['input_ids'],\n",
    "                                                attention_mask=segment['attention_mask'],\n",
    "                                                # labels=segment['input_ids'],\n",
    "\n",
    "                                                memory_state=memory_state, \n",
    "                                                output_hidden_states=True)\n",
    "    cell_outputs.append(cell_out)\n",
    "    self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "# out = self.process_outputs(cell_outputs, segments,\n",
    "#                             output_attentions=output_attentions,\n",
    "#                             output_hidden_states=output_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dict()\n",
    "from torch.nn import CrossEntropyLoss\n",
    "self = rmt\n",
    "kwargs = {}\n",
    "\n",
    "proxy_out = {}\n",
    "for seg_num, segment in enumerate(segments):\n",
    "    cell_out = cell_outputs[seg_num]\n",
    "\n",
    "    full_logits = cell_out.logits\n",
    "\n",
    "    labels = segment.get('labels')\n",
    "    if labels is not None:\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "        flat_labels = shift_labels.view(-1)\n",
    "        flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        labels_mask = segment.get('labels_mask')\n",
    "        if labels_mask is not None:\n",
    "            shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "            flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "            flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "\n",
    "            if labels_mask.sum() == 0:\n",
    "                loss_value = 0\n",
    "            else:\n",
    "                loss_value = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "        proxy_out[f'loss_{seg_num}'] = loss_value\n",
    "    else:\n",
    "        proxy_out[f'loss_{seg_num}'] = 0\n",
    "\n",
    "    segment_keys = ['loss']\n",
    "    if kwargs.get('output_attentions'):\n",
    "        segment_keys.append('attentions')\n",
    "    if kwargs.get('output_hidden_states'):\n",
    "        segment_keys.append('hidden_states')\n",
    "\n",
    "    for key, value in cell_out.items():\n",
    "        if any([sk in key for sk in segment_keys]):\n",
    "            proxy_out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "num_segments = len(segments)\n",
    "out['loss'] = sum([proxy_out[f'loss_{seg_num}'] for seg_num in range(num_segments)]) / num_segments\n",
    "out['logits'] = torch.cat([cell_out.logits for cell_out in cell_outputs], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_0 0\n",
      "loss_1 1.2404494782458642e-06\n",
      "loss_2 1.437594346498372e-05\n",
      "loss_3 2.0184384993626736e-05\n",
      "loss_4 2.69055385615502e-06\n",
      "loss_5 0.2612733542919159\n"
     ]
    }
   ],
   "source": [
    "for k, v in proxy_out.items():\n",
    "    print(k, v.item() if isinstance(v, torch.Tensor) else v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': '5 6 3 2 * 7 4 3 4',\n",
       " 'labels': '5 5 6 0 8 2 0 1',\n",
       " 'cot': '5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "    preds = eval_pred.predictions.argmax(axis=-1)[:, :-1]\n",
    "    labels = eval_pred.label_ids[:, 1:]\n",
    "\n",
    "    labels_masks = labels > 0\n",
    "    preds_full = [p[m] for p, m in zip(preds, labels_masks)]\n",
    "    labels_full = [lab[m] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "    special_tokens = {ans[0], bos[0]}\n",
    "    acc_cot, acc_ans = [], []\n",
    "    for lab_tokens, pred_tokens in zip(labels_full, preds_full):\n",
    "        ans_start_index = max(i for i, x in enumerate(lab_tokens) if x == ans[0])\n",
    "\n",
    "        pred_cot_tokens = pred_tokens[:ans_start_index].tolist()\n",
    "        lab_cot_tokens = lab_tokens[:ans_start_index].tolist()\n",
    "\n",
    "        cot_correct = [p == l for p, l in zip(pred_cot_tokens, lab_cot_tokens) if l not in special_tokens]\n",
    "        acc_cot.append(all(cot_correct))\n",
    "\n",
    "        pred_ans_tokens = pred_tokens[ans_start_index:].tolist()\n",
    "        lab_ans_tokens = lab_tokens[ans_start_index:].tolist()\n",
    "\n",
    "        ans_correct = [p == l for p, l in zip(pred_ans_tokens, lab_ans_tokens) if l not in special_tokens]\n",
    "        acc_ans.append(all(ans_correct))\n",
    "\n",
    "    return {'accuracy_cot': np.mean(acc_cot), 'accuracy_ans': np.mean(acc_ans)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {ans[0], bos[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9 1 0 3 2 8 6 0<|endoftext|>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([p for p, l in zip(pred_ans_tokens, lab_ans_tokens) if l not in special_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9 1 0 3 0 8 6 0<|endoftext|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([l for p, l in zip(pred_ans_tokens, lab_ans_tokens) if l not in special_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!!!!////9 1 0 3 0 8 6 0<|endoftext|>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([l for p, l in zip(pred_ans_tokens, lab_ans_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!!!! 69 1 0 3 2 8 6 0<|endoftext|>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([p for p, l in zip(pred_ans_tokens, lab_ans_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pred = Holder()\n",
    "\n",
    "eval_pred.predictions = out['logits']\n",
    "eval_pred.label_ids = collated['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_cot': 1.0, 'accuracy_ans': 0.0625}\n"
     ]
    }
   ],
   "source": [
    "acc = compute_accuracy(eval_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = eval_pred.predictions.argmax(axis=-1)[:, :-1]\n",
    "labels = eval_pred.label_ids[:, 1:]\n",
    "\n",
    "labels_masks = labels > 0\n",
    "preds_full = [p[m] for p, m in zip(preds, labels_masks)]\n",
    "labels_full = [lab[m] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "acc_cot, acc_ans = [], []\n",
    "correct_samples = []\n",
    "for lab_tokens, pred_tokens in zip(labels_full, preds_full):\n",
    "    ans_start_index = max(i for i, x in enumerate(lab_tokens) if x == ans[0])\n",
    "\n",
    "    pred_cot_tokens = pred_tokens[:ans_start_index].tolist()\n",
    "    lab_cot_tokens = lab_tokens[:ans_start_index].tolist()\n",
    "\n",
    "    cot_correct = [p == l for p, l in zip(pred_cot_tokens, lab_cot_tokens) if l != bos[0]]\n",
    "    acc_cot.append(all(cot_correct))\n",
    "\n",
    "    pred_ans_tokens = pred_tokens[ans_start_index:].tolist()\n",
    "    lab_ans_tokens = lab_tokens[ans_start_index:].tolist()\n",
    "\n",
    "    # ans_correct = [p == l for p, l in zip(pred_ans_tokens, lab_ans_tokens)]\n",
    "    ans_correct = [p == l for p, l in zip(pred_ans_tokens, lab_ans_tokens) if l not in special_tokens]\n",
    "    acc_ans.append(all(ans_correct))\n",
    "    if all(ans_correct):\n",
    "        correct_samples.append(lab_tokens)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['////1 9 7 9 1????////0 0 0 0 0 0 ( 1 9 7 9 1 0 )????////0 0 9 9 1 2 0 ( 1 9 6 9 3 2 0 )????////0 0 0 3 9 3 5 1!!!!////1 9 6 2 3 6 5 1<|endoftext|>',\n",
       " '////4 7 3 4 0????////0 2 2 1 3 1 ( 4 9 5 5 3 1 )????////0 0 8 4 7 8 0 ( 4 9 3 0 1 0 1 )????////0 0 0 8 4 7 8 0!!!!////4 9 3 8 5 7 9 0<|endoftext|>',\n",
       " '////0 0 2 6 2????////0 0 5 5 6 0 ( 0 0 7 1 9 0 )????////0 0 0 0 2 6 2 ( 0 0 7 1 1 7 2 )????////0 0 0 0 0 2 6 2!!!!////0 0 7 1 1 9 8 2<|endoftext|>',\n",
       " '////9 3 1 1 4????////0 2 4 1 9 0 ( 9 5 5 2 3 1 )????////0 0 3 1 7 3 1 ( 9 5 8 3 0 5 1 )????////0 0 0 5 5 8 2 2!!!!////9 5 8 8 5 3 4 2<|endoftext|>',\n",
       " '////5 4 9 7 4????////0 2 1 7 6 7 ( 5 6 0 5 1 8 )????////0 0 6 5 3 8 3 ( 5 6 6 0 5 6 4 )????////0 0 0 2 1 7 6 7!!!!////5 6 6 2 6 3 1 8<|endoftext|>',\n",
       " '////8 4 6 3 5????////0 2 1 4 3 1 ( 8 6 7 7 8 1 )????////0 0 8 1 1 0 2 ( 8 6 5 9 9 1 2 )????////0 0 0 4 5 3 0 6!!!!////8 6 5 3 5 5 2 6<|endoftext|>',\n",
       " '////6 1 2 9 0????////0 0 2 5 1 1 ( 6 1 4 4 2 1 )????////0 0 0 0 0 0 0 ( 6 1 4 4 2 1 0 )????////0 0 0 6 1 2 9 0!!!!////6 1 4 0 4 3 9 0<|endoftext|>',\n",
       " '////0 0 0 0 0????////0 8 1 5 6 0 ( 0 8 1 5 6 0 )????////0 0 2 7 0 6 2 ( 0 8 3 2 7 6 2 )????////0 0 0 7 7 7 9 0!!!!////0 8 3 9 4 4 2 1<|endoftext|>']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(correct_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['////5 5 5 6 1????////0 0 6 4 9 0 ( 5 5 1 1 1 1 )????////0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )????////0 0 0 0 6 4 9 0!!!!////5 5 6 0 8 2 0 1<|endoftext|>',\n",
       " '////6 7 1 1 3????////0 4 8 7 0 2 ( 6 1 0 9 3 2 )????////0 0 4 8 7 0 2 ( 6 1 4 7 1 3 2 )????////0 0 0 2 7 3 6 3!!!!////6 1 4 9 8 6 8 3<|endoftext|>',\n",
       " '////8 0 0 5 7????////0 4 8 3 4 8 ( 8 4 8 8 1 9 )????////0 0 6 7 3 9 0 ( 8 4 4 6 5 8 1 )????////0 0 0 2 3 6 5 6!!!!////8 4 4 8 8 4 7 6<|endoftext|>',\n",
       " '////9 0 9 2 1????////0 2 1 2 7 1 ( 9 2 0 5 8 1 )????////0 0 8 1 8 5 2 ( 9 2 8 6 6 7 2 )????////0 0 0 5 1 5 1 2!!!!////9 2 8 1 8 2 4 2<|endoftext|>',\n",
       " '////0 4 6 8 5????////0 0 5 6 6 3 ( 0 4 1 5 2 4 )????////0 0 0 8 9 3 4 ( 0 4 1 3 2 8 4 )????////0 0 0 0 5 6 6 3!!!!////0 4 1 3 7 4 1 4<|endoftext|>']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(labels_full[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ty5 5 5 6 1????????0 0 6 4 9 0 ( 5 5 1 1 1 1 )????????0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )????????0 0 0 0 6 4 9 0!!!! 05 5 6 0 1 2 0 1<|endoftext|>',\n",
       " ' deposit6 7 1 1 3????????0 4 8 7 0 2 ( 6 1 0 9 3 2 )????????0 0 4 8 7 0 2 ( 6 1 4 7 1 3 2 )????????0 0 0 2 7 3 6 3!!!! 26 1 4 9 5 6 8 3<|endoftext|>',\n",
       " ' Bild8 0 0 5 7????????0 4 8 3 4 8 ( 8 4 8 8 1 9 )????????0 0 6 7 3 9 0 ( 8 4 4 6 5 8 1 )????????0 0 0 2 3 6 5 6!!!! 28 4 4 8 5 4 7 6<|endoftext|>',\n",
       " ' Hein9 0 9 2 1????????0 2 1 2 7 1 ( 9 2 0 5 8 1 )????????0 0 8 1 8 5 2 ( 9 2 8 6 6 7 2 )????????0 0 0 5 1 5 1 2!!!! 59 2 8 1 3 2 4 2<|endoftext|>',\n",
       " ' fragrance0 4 6 8 5????????0 0 5 6 6 3 ( 0 4 1 5 2 4 )????????0 0 0 8 9 3 4 ( 0 4 1 3 2 8 4 )????????0 0 0 0 5 6 6 3!!!! 00 4 1 3 9 4 1 4<|endoftext|>']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(preds_full[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'////5 5 5 6 1????////0 0 6 4 9 0 ( 5 5 1 1 1 1 )????////0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )????////0 0 0 0 6 4 9 0!!!!////5 5 6 0 8 2 0 1<|endoftext|>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(labels_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'////9 9 0 9 0????////0 2 3 1 2 1 ( 9 1 4 0 3 1 )????////0 0 6 6 0 6 0 ( 9 1 0 7 3 7 0 )????////0 0 0 6 6 0 6 0!!!!////9 1 0 3 0 8 6 0<|endoftext|>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lab_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!!!!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ty5 5 5 6 1????????0 0 6 4 9 0 ( 5 5 1 1 1 1 )????????0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )????????0 0 0 0 6 4 9 0!!!! 05 5 6 0 1 2 0 1<|endoftext|>',\n",
       " ' deposit6 7 1 1 3????????0 4 8 7 0 2 ( 6 1 0 9 3 2 )????????0 0 4 8 7 0 2 ( 6 1 4 7 1 3 2 )????????0 0 0 2 7 3 6 3!!!! 26 1 4 9 5 6 8 3<|endoftext|>',\n",
       " ' Bild8 0 0 5 7????????0 4 8 3 4 8 ( 8 4 8 8 1 9 )????????0 0 6 7 3 9 0 ( 8 4 4 6 5 8 1 )????????0 0 0 2 3 6 5 6!!!! 28 4 4 8 5 4 7 6<|endoftext|>',\n",
       " ' Hein9 0 9 2 1????????0 2 1 2 7 1 ( 9 2 0 5 8 1 )????????0 0 8 1 8 5 2 ( 9 2 8 6 6 7 2 )????????0 0 0 5 1 5 1 2!!!! 59 2 8 1 3 2 4 2<|endoftext|>',\n",
       " ' fragrance0 4 6 8 5????????0 0 5 6 6 3 ( 0 4 1 5 2 4 )????????0 0 0 8 9 3 4 ( 0 4 1 3 2 8 4 )????????0 0 0 0 5 6 6 3!!!! 00 4 1 3 9 4 1 4<|endoftext|>']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(preds_full[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['////5 5 5 6 1????////0 0 6 4 9 0 ( 5 5 1 1 1 1 )????////0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )????////0 0 0 0 6 4 9 0!!!!////5 5 6 0 8 2 0 1<|endoftext|>',\n",
       " '////6 7 1 1 3????////0 4 8 7 0 2 ( 6 1 0 9 3 2 )????////0 0 4 8 7 0 2 ( 6 1 4 7 1 3 2 )????////0 0 0 2 7 3 6 3!!!!////6 1 4 9 8 6 8 3<|endoftext|>',\n",
       " '////8 0 0 5 7????////0 4 8 3 4 8 ( 8 4 8 8 1 9 )????////0 0 6 7 3 9 0 ( 8 4 4 6 5 8 1 )????////0 0 0 2 3 6 5 6!!!!////8 4 4 8 8 4 7 6<|endoftext|>',\n",
       " '////9 0 9 2 1????////0 2 1 2 7 1 ( 9 2 0 5 8 1 )????////0 0 8 1 8 5 2 ( 9 2 8 6 6 7 2 )????////0 0 0 5 1 5 1 2!!!!////9 2 8 1 8 2 4 2<|endoftext|>',\n",
       " '////0 4 6 8 5????////0 0 5 6 6 3 ( 0 4 1 5 2 4 )????////0 0 0 8 9 3 4 ( 0 4 1 3 2 8 4 )????////0 0 0 0 5 6 6 3!!!!////0 4 1 3 7 4 1 4<|endoftext|>']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(labels_full[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5 5 6 0 1 2 0 1<|endoftext|>',\n",
       " '6 1 4 9 5 6 8 3<|endoftext|>',\n",
       " '8 4 4 8 5 4 7 6<|endoftext|>',\n",
       " '9 2 8 1 3 2 4 2<|endoftext|>',\n",
       " '0 4 1 3 9 4 1 4<|endoftext|>']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([s[-9:] for s in preds_full[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5 5 6 0 8 2 0 1<|endoftext|>',\n",
       " '6 1 4 9 8 6 8 3<|endoftext|>',\n",
       " '8 4 4 8 8 4 7 6<|endoftext|>',\n",
       " '9 2 8 1 8 2 4 2<|endoftext|>',\n",
       " '0 4 1 3 7 4 1 4<|endoftext|>']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([s[-9:] for s in labels_full[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9 9 0 9 0????0 2 3 1 2 1 ( 9 1 4 0 3 1 )????0 0 6 6 0 6 0 ( 9 1 0 7 3 7 0 )????0 0 0 6 6 0 6 0'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([p for p, l in zip(pred_cot_tokens, lab_cot_tokens) if l != bos[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9 9 0 9 0????0 2 3 1 2 1 ( 9 1 4 0 3 1 )????0 0 6 6 0 6 0 ( 9 1 0 7 3 7 0 )????0 0 0 6 6 0 6 0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([l for p, l in zip(pred_cot_tokens, lab_cot_tokens) if l != bos[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean([value.item() if isinstance(value, torch.Tensor) else value  for value in proxy_out.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'hidden_states'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_outputs[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "////6 9 1 5 * 6 4 4 7????\n",
      "////6 7 1 1 3????\n",
      "////0 4 8 7 0 2 ( 6 1 0 9 3 2 )????\n",
      "////0 0 4 8 7 0 2 ( 6 1 4 7 1 3 2 )????\n",
      "////0 0 0 2 7 3 6 3!!!!\n",
      "////6 1 4 9 8 6 8 3<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "for seg in segments[:]:\n",
    "    print(tokenizer.decode(seg['input_ids'][si]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robees buttons 2 4 5- 2 dunk wellnesspard',\n",
       " '6 7 1 1 3????????',\n",
       " '0 4 8 7 0 2 ( 6 1 0 9 3 2 )???? 9',\n",
       " '0 0 4 8 7 0 2 ( 6 1 4 7 1 3 2 )???? 7',\n",
       " '0 0 0 2 7 3 6 3!!!!!!!!',\n",
       " '6 1 4 9 1 6 8 3<|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions\n",
    "[tokenizer.decode(o.logits.argmax(dim=-1)[si]) for o in cell_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(eval_pred):\n",
    "    preds = eval_pred.predictions.argmax(axis=-1)[:, :-1]\n",
    "    labels = eval_pred.label_ids[:, 1:]\n",
    "\n",
    "    labels_masks = labels > 0\n",
    "    preds_full = [p[m] for p, m in zip(preds, labels_masks)]\n",
    "    labels_full = [lab[m] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "    special_tokens = {ans[0], bos[0]}\n",
    "    acc_cot, acc_ans = [], []\n",
    "    for lab_tokens, pred_tokens in zip(labels_full, preds_full):\n",
    "        ans_start_index = max(i for i, x in enumerate(lab_tokens) if x == ans[0])\n",
    "\n",
    "        pred_cot_tokens = pred_tokens[:ans_start_index].tolist()\n",
    "        lab_cot_tokens = lab_tokens[:ans_start_index].tolist()\n",
    "\n",
    "        cot_correct = [p == l for p, l in zip(pred_cot_tokens, lab_cot_tokens) if l not in special_tokens]\n",
    "        acc_cot.append(all(cot_correct))\n",
    "\n",
    "        pred_ans_tokens = pred_tokens[ans_start_index:].tolist()\n",
    "        lab_ans_tokens = lab_tokens[ans_start_index:].tolist()\n",
    "\n",
    "        ans_correct = [p == l for p, l in zip(pred_ans_tokens, lab_ans_tokens)]\n",
    "        acc_ans.append(all(ans_correct))\n",
    "\n",
    "    return {'accuracy_cot': np.mean(acc_cot), 'accuracy_ans': np.mean(acc_ans)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  ...,   657,   352, 50256],\n",
       "        [ -100,  -100,  -100,  ...,   807,   513, 50256],\n",
       "        [ -100,  -100,  -100,  ...,   767,   718, 50256],\n",
       "        ...,\n",
       "        [ -100,  -100,  -100,  ...,   807,   657, 50256],\n",
       "        [ -100,  -100,  -100,  ...,   642,   352, 50256],\n",
       "        [ -100,  -100,  -100,  ...,   604,   807, 50256]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = out['logits'].argmax(axis=-1)[:, :-1]\n",
    "labels = collated['labels'][:, 1:]\n",
    "labels_masks = labels > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16\n"
     ]
    }
   ],
   "source": [
    "preds_full = [p[m] for p, m in zip(preds, labels_masks)]\n",
    "labels_full = [lab[m] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "print(len(preds_full), len(labels_full))\n",
    "\n",
    "preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9805]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpreds_full\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tensor' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "preds_full[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ans(tensor):\n",
    "    tokens = tensor.tolist()\n",
    "    \n",
    "    try:\n",
    "        start_index = max(i for i, x in enumerate(tokens) if x == ans[0])\n",
    "    except ValueError:\n",
    "        return []\n",
    "    \n",
    "    tokens = tokens[start_index + 1:]\n",
    "    \n",
    "    if tokens and tokens[0] == bos[0]:\n",
    "        tokens = tokens[1:]\n",
    "    \n",
    "    return torch.tensor(tokens)\n",
    "\n",
    "def get_cot(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = labels_full[0].tolist()\n",
    "\n",
    "ans_start_index = max(i for i, x in enumerate(tokens) if x == ans[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tokens = labels_full[0]\n",
    "pred_tokens = preds_full[0]\n",
    "\n",
    "special_tokens = {ans[0], bos[0]}\n",
    "pred_cot_tokens = pred_tokens[:ans_start_index].tolist()\n",
    "lab_cot_tokens = lab_tokens[:ans_start_index].tolist()\n",
    "\n",
    "cot_correct = [p == l for p, l in zip(pred_cot_tokens, lab_cot_tokens) if l not in special_tokens]\n",
    "\n",
    "pred_ans_tokens = pred_tokens[ans_start_index:].tolist()\n",
    "lab_ans_tokens = lab_tokens[ans_start_index:].tolist()\n",
    "\n",
    "ans_correct = [p == l for p, l in zip(pred_ans_tokens, lab_ans_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cot_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ans_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 45 5 5 6 1????????0 0 6 4 9 0 ( 5 5 1 1 1 1 )????????0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )???? 00 0 0 0 6 4 9 0!!!!!!!!5 5 6 0 7 2 0 1<|endoftext|>'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 5 5 6 1????0 0 6 4 9 0 ( 5 5 1 1 1 1 )????0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )????0 0 0 0 6 4 9 0'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds\n",
    "tokenizer.decode([p for p, l in zip(pred_cot_tokens, lab_cot_tokens) if l not in special_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 5 5 6 1????0 0 6 4 9 0 ( 5 5 1 1 1 1 )????0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )????0 0 0 0 6 4 9 0'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "tokenizer.decode([l for p, l in zip(pred_cot_tokens, lab_cot_tokens) if l not in special_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9805, 9805, 9805]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l for l in lab_cot_tokens if l not in special_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9805"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "think[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpred_cot_tokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "pred_cot_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cot_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_ans(preds_full[0])), len(get_ans(labels_full[0])), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 5 6 0 7 2 0 1<|endoftext|>'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(get_ans(preds_full[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 5 6 0 8 2 0 1<|endoftext|>'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(get_ans(labels_full[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ans(text):\n",
    "    splits = text.split(ans_text)\n",
    "    splits = list(filter(len, splits))\n",
    "    ans = splits[-1].strip()\n",
    "    if ans.startswith(ans_text):\n",
    "        ans = ans[len(ans_text):]\n",
    "    if ans.startswith(bos_text):\n",
    "        ans = ans[len(bos_text):]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5 5 6 0 7 2 0 1<|endoftext|>', '5 5 6 0 8 2 0 1<|endoftext|>')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ans(preds_full_text[0]), get_ans(labels_full_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 45 5 5 6 1????????0 0 6 4 9 0 ( 5 5 1 1 1 1 )????????0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )???? 00 0 0 0 6 4 9 0',\n",
       " '',\n",
       " '5 5 6 0 7 2 0 1<|endoftext|>']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_full_text[0].split(ans_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['////5 5 5 6 1????////0 0 6 4 9 0 ( 5 5 1 1 1 1 )????////0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )????////0 0 0 0 6 4 9 0',\n",
       " '////5 5 6 0 8 2 0 1<|endoftext|>']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_full_text[0].split(ans_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 45 5 5 6 1????????0 0 6 4 9 0 ( 5 5 1 1 1 1 )????????0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )???? 00 0 0 0 6 4 9 0!!!!!!!!5 5 6 0 7 2 0 1<|endoftext|>',\n",
       " 'pard6 7 1 1 3????????0 4 8 7 0 2 ( 6 1 0 9 3 2 )???? 90 0 4 8 7 0 2 ( 6 1 4 7 1 3 2 )???? 70 0 0 2 7 3 6 3!!!!!!!!6 1 4 9 1 6 8 3<|endoftext|>',\n",
       " ' Doe8 0 0 5 7????????0 4 8 3 4 8 ( 8 4 8 8 1 9 )????????0 0 6 7 3 9 0 ( 8 4 4 6 5 8 1 )???? 60 0 0 2 3 6 5 6!!!!!!!!8 4 4 8 1 4 7 6<|endoftext|>',\n",
       " ' Gentleman9 0 9 2 1????????0 2 1 2 7 1 ( 9 2 0 5 8 1 )???? 50 0 8 1 8 5 2 ( 9 2 8 6 6 7 2 )???? 60 0 0 5 1 5 1 2!!!!!!!!9 2 8 1 0 2 4 2<|endoftext|>',\n",
       " ' drum0 4 6 8 5????????0 0 5 6 6 3 ( 0 4 1 5 2 4 )???? 50 0 0 8 9 3 4 ( 0 4 1 3 2 8 4 )???? 30 0 0 0 5 6 6 3!!!!!!!!0 4 1 3 9 4 1 4<|endoftext|>',\n",
       " ' 42 5 2 4 2????????0 9 8 1 8 1 ( 2 4 1 6 0 2 )???? 60 0 4 0 5 8 4 ( 2 4 5 6 5 0 5 )???? 60 0 0 1 4 4 2 4!!!!!!!!2 4 5 7 1 4 7 4<|endoftext|>',\n",
       " 'hovah8 4 7 9 0????????0 6 6 8 3 4 ( 8 0 4 8 4 4 )???? 80 0 4 7 8 4 0 ( 8 0 8 5 3 9 0 )???? 50 0 0 4 4 2 9 2!!!!!!!!8 0 8 9 9 1 0 3<|endoftext|>',\n",
       " ' ass0 0 0 0 0????????0 0 6 4 8 0 ( 0 0 6 4 8 0 )????????0 0 2 9 6 1 0 ( 0 0 8 3 5 2 0 )???? 30 0 0 8 2 2 5 1!!!!!!!!0 0 8 1 0 4 5 1<|endoftext|>',\n",
       " ' grilled5 5 5 2 2????????0 6 6 0 7 2 ( 5 1 2 3 9 2 )???? 30 0 0 0 0 0 0 ( 5 1 2 3 9 2 0 )???? 30 0 0 5 5 5 2 2!!!! 55 1 2 8 3 8 2 2<|endoftext|>',\n",
       " ' tam0 0 0 0 0????????0 7 3 9 5 3 ( 0 7 3 9 5 3 )????90 0 9 7 9 1 1 ( 0 7 2 7 5 5 1 )???? 70 0 0 9 7 9 1 1!!!!!!!!0 7 2 6 3 5 3 1<|endoftext|>',\n",
       " ' 35 6 8 3 0????????0 0 6 4 5 1 ( 5 6 4 8 5 1 )???? 80 0 5 8 7 4 3 ( 5 6 9 6 3 6 3 )???? 60 0 0 5 5 0 7 2!!!!!!!!5 6 9 1 0 6 0 3<|endoftext|>',\n",
       " ' 84 8 1 4 5????????0 5 6 8 3 3 ( 4 3 8 2 9 3 )???? 20 0 5 6 8 3 3 ( 4 3 3 9 7 7 3 )???? 90 0 0 6 4 5 3 1!!!!!!!!4 3 3 5 0 3 7 1<|endoftext|>',\n",
       " 'eal4 4 8 9 0????????0 9 4 1 2 2 ( 4 3 3 1 3 2 )???? 10 0 8 8 6 9 1 ( 4 3 1 0 0 2 2 )???? 00 0 0 2 2 9 4 0!!!!!!!!4 3 1 2 1 1 7 0<|endoftext|>',\n",
       " ' 66 5 6 7 3????????0 2 3 9 3 4 ( 6 7 9 6 7 4 )????????0 0 2 5 5 2 1 ( 6 7 1 2 3 7 1 )???? 20 0 0 6 7 2 6 0!!!!!!!!6 7 1 8 1 0 8 0<|endoftext|>',\n",
       " ' Grammy0 2 3 1 1????????0 0 0 3 8 2 ( 0 2 3 4 9 2 )???? 40 0 0 6 9 3 3 ( 0 2 3 0 9 6 3 )???? 00 0 0 0 2 3 1 1!!!!!!!!0 2 3 0 0 0 5 1<|endoftext|>',\n",
       " 'kid6 1 2 8 1????????0 8 4 6 4 5 ( 6 9 6 4 6 5 )????40 0 6 1 2 8 1 ( 6 9 2 6 8 3 2 )???? 60 0 0 2 7 9 1 8!!!!!!!!6 9 2 8 2 3 4 8<|endoftext|>']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['////5 5 5 6 1????////0 0 6 4 9 0 ( 5 5 1 1 1 1 )????////0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 )????////0 0 0 0 6 4 9 0!!!!////5 5 6 0 8 2 0 1<|endoftext|>',\n",
       " '////6 7 1 1 3????////0 4 8 7 0 2 ( 6 1 0 9 3 2 )????////0 0 4 8 7 0 2 ( 6 1 4 7 1 3 2 )????////0 0 0 2 7 3 6 3!!!!////6 1 4 9 8 6 8 3<|endoftext|>',\n",
       " '////8 0 0 5 7????////0 4 8 3 4 8 ( 8 4 8 8 1 9 )????////0 0 6 7 3 9 0 ( 8 4 4 6 5 8 1 )????////0 0 0 2 3 6 5 6!!!!////8 4 4 8 8 4 7 6<|endoftext|>',\n",
       " '////9 0 9 2 1????////0 2 1 2 7 1 ( 9 2 0 5 8 1 )????////0 0 8 1 8 5 2 ( 9 2 8 6 6 7 2 )????////0 0 0 5 1 5 1 2!!!!////9 2 8 1 8 2 4 2<|endoftext|>',\n",
       " '////0 4 6 8 5????////0 0 5 6 6 3 ( 0 4 1 5 2 4 )????////0 0 0 8 9 3 4 ( 0 4 1 3 2 8 4 )????////0 0 0 0 5 6 6 3!!!!////0 4 1 3 7 4 1 4<|endoftext|>',\n",
       " '////2 5 2 4 2????////0 9 8 1 8 1 ( 2 4 1 6 0 2 )????////0 0 4 0 5 8 4 ( 2 4 5 6 5 0 5 )????////0 0 0 1 4 4 2 4!!!!////2 4 5 7 9 4 7 4<|endoftext|>',\n",
       " '////8 4 7 9 0????////0 6 6 8 3 4 ( 8 0 4 8 4 4 )????////0 0 4 7 8 4 0 ( 8 0 8 5 3 9 0 )????////0 0 0 4 4 2 9 2!!!!////8 0 8 9 7 1 0 3<|endoftext|>',\n",
       " '////0 0 0 0 0????////0 0 6 4 8 0 ( 0 0 6 4 8 0 )????////0 0 2 9 6 1 0 ( 0 0 8 3 5 2 0 )????////0 0 0 8 2 2 5 1!!!!////0 0 8 1 8 4 5 1<|endoftext|>',\n",
       " '////5 5 5 2 2????////0 6 6 0 7 2 ( 5 1 2 3 9 2 )????////0 0 0 0 0 0 0 ( 5 1 2 3 9 2 0 )????////0 0 0 5 5 5 2 2!!!!////5 1 2 8 4 8 2 2<|endoftext|>',\n",
       " '////0 0 0 0 0????////0 7 3 9 5 3 ( 0 7 3 9 5 3 )????////0 0 9 7 9 1 1 ( 0 7 2 7 5 5 1 )????////0 0 0 9 7 9 1 1!!!!////0 7 2 6 3 5 3 1<|endoftext|>',\n",
       " '////5 6 8 3 0????////0 0 6 4 5 1 ( 5 6 4 8 5 1 )????////0 0 5 8 7 4 3 ( 5 6 9 6 3 6 3 )????////0 0 0 5 5 0 7 2!!!!////5 6 9 1 9 6 0 3<|endoftext|>',\n",
       " '////4 8 1 4 5????////0 5 6 8 3 3 ( 4 3 8 2 9 3 )????////0 0 5 6 8 3 3 ( 4 3 3 9 7 7 3 )????////0 0 0 6 4 5 3 1!!!!////4 3 3 5 2 3 7 1<|endoftext|>',\n",
       " '////4 4 8 9 0????////0 9 4 1 2 2 ( 4 3 3 1 3 2 )????////0 0 8 8 6 9 1 ( 4 3 1 0 0 2 2 )????////0 0 0 2 2 9 4 0!!!!////4 3 1 2 2 1 7 0<|endoftext|>',\n",
       " '////6 5 6 7 3????////0 2 3 9 3 4 ( 6 7 9 6 7 4 )????////0 0 2 5 5 2 1 ( 6 7 1 2 3 7 1 )????////0 0 0 6 7 2 6 0!!!!////6 7 1 8 0 0 8 0<|endoftext|>',\n",
       " '////0 2 3 1 1????////0 0 0 3 8 2 ( 0 2 3 4 9 2 )????////0 0 0 6 9 3 3 ( 0 2 3 0 9 6 3 )????////0 0 0 0 2 3 1 1!!!!////0 2 3 0 1 0 5 1<|endoftext|>',\n",
       " '////6 1 2 8 1????////0 8 4 6 4 5 ( 6 9 6 4 6 5 )????////0 0 6 1 2 8 1 ( 6 9 2 6 8 3 2 )????////0 0 0 2 7 9 1 8!!!!////6 9 2 8 5 3 4 8<|endoftext|>']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "labels_cot = [extract_cot(lab) for lab in labels_full_text]\n",
    "labels_ans = [extract_answer(lab) for lab in labels_full_text]\n",
    "\n",
    "acc_cot = np.mean([c == p for c, p in zip(preds_cot, labels_cot)])\n",
    "acc_ans = np.mean([c == lab for c, lab in zip(preds_ans, labels_ans)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 72]), torch.Size([1, 72]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25481,  1666, 13129,  ...,   352, 50256, 50256],\n",
       "        [25481,   274, 12163,  ...,   513, 50256, 50256],\n",
       "        [25481,   274,   513,  ...,   718, 50256, 50256],\n",
       "        ...,\n",
       "        [25481,   274,   513,  ...,   657, 50256, 50256],\n",
       "        [25481,   657,   513,  ...,   352, 50256, 50256],\n",
       "        [25481,    67,   352,  ...,   807, 50256, 50256]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode([segment['labels'][0][0] for segment in segments[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# res_df = pd.DataFrame(columns=['cpt_path', 'cot', 'acc_cot', 'acc_ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.loss for o in cell_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "gen_outputs = [model.generate(inp.reshape(1, -1).to(device), \n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=torch.ones_like(inp.reshape(1, -1)).to(device),\n",
    "                            max_new_tokens=50)[0] for inp in collated['input_ids_generate']]\n",
    "\n",
    "if args.use_cot:\n",
    "    gen_outputs = [model.generate(inp.reshape(1, -1).to(device), \n",
    "                                    pad_token_id=tokenizer.eos_token_id,\n",
    "                                    attention_mask=torch.ones_like(inp.reshape(1, -1)).to(device))[0].cpu() for inp in gen_outputs]\n",
    "\n",
    "labels = collated['labels']\n",
    "labels_masks = labels > 0\n",
    "\n",
    "preds_full = [out[len(inp):] for inp, out in zip(collated['input_ids_generate'], gen_outputs)]\n",
    "# preds_full = [out[len(inp):] for inp, out in zip(collated['input_ids_generate'], gen_outputs_m2)]\n",
    "labels_full = [lab[m][1:] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "print(len(preds_full), len(labels_full))\n",
    "\n",
    "preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "labels_cot = [extract_cot(lab) for lab in labels_full_text]\n",
    "labels_ans = [extract_answer(lab) for lab in labels_full_text]\n",
    "\n",
    "acc_cot = np.mean([c == p for c, p in zip(preds_cot, labels_cot)])\n",
    "acc_ans = np.mean([c == lab for c, lab in zip(preds_ans, labels_ans)])\n",
    "\n",
    "data = {\"inputs\": collated['input_ids_generate'],\n",
    "        \"preds_full_text\": preds_full_text, \"labels_full_text\": labels_full_text,\n",
    "        \"preds_cot\": preds_cot, \"labels_cot\": labels_cot,\n",
    "        \"preds_ans\": preds_ans, \"labels_ans\": labels_ans}\n",
    "\n",
    "print(f\"Accuracy COT: {acc_cot}\")\n",
    "print(f\"Accuracy Answer: {acc_ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace-SR006.nfs2/Bulatov_A/rmt/runs/gsm8k/gpt2/SEGM_1x1024_1024_LR3e-04/checkpoint-16500/pytorch_model.bin\n",
      "500 500\n",
      "Accuracy COT: 0.172\n",
      "Accuracy Answer: 1.0\n",
      "/workspace-SR006.nfs2/Bulatov_A/rmt/runs/gsm8k/gpt2/SEGM_1x1024_1024_LR1e-03/checkpoint-24500/pytorch_model.bin\n",
      "500 500\n",
      "Accuracy COT: 0.148\n",
      "Accuracy Answer: 1.0\n"
     ]
    }
   ],
   "source": [
    "args.use_cot = False\n",
    "\n",
    "checkpoints = [\n",
    "    \"/workspace-SR006.nfs2/Bulatov_A/rmt/runs/gsm8k/gpt2/SEGM_1x1024_1024_LR3e-04/checkpoint-16500/pytorch_model.bin\",\n",
    "    \"/workspace-SR006.nfs2/Bulatov_A/rmt/runs/gsm8k/gpt2/SEGM_1x1024_1024_LR1e-03/checkpoint-24500/pytorch_model.bin\",\n",
    "]\n",
    "\n",
    "for cpt_path in checkpoints:\n",
    "    print(cpt_path)\n",
    "    acc_cot, acc_ans = evaluate_model_on_dataset(cpt_path, valid_dataset)\n",
    "    res_df.loc[len(res_df)] = [cpt_path.split('/')[-3], args.use_cot, 0, acc_cot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpt_path</th>\n",
       "      <th>cot</th>\n",
       "      <th>acc_cot</th>\n",
       "      <th>acc_ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEGM_1x1024_1024_LR1e-03-cot</td>\n",
       "      <td>True</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEGM_1x1024_1024_LR3e-04-cot</td>\n",
       "      <td>True</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SEGM_1x1024_1024_LR3e-04</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SEGM_1x1024_1024_LR1e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       cpt_path    cot  acc_cot  acc_ans\n",
       "0  SEGM_1x1024_1024_LR1e-03-cot   True    0.172    0.450\n",
       "1  SEGM_1x1024_1024_LR3e-04-cot   True    0.156    0.392\n",
       "2      SEGM_1x1024_1024_LR3e-04  False    0.000    0.172\n",
       "3      SEGM_1x1024_1024_LR1e-03  False    0.000    0.148"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500\n",
      "Accuracy COT: 0.148\n",
      "Accuracy Answer: 1.0\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "model.to(device)\n",
    "    \n",
    "collated = collate_fn([sample for sample in valid_dataset])\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "gen_outputs = [model.generate(inp.reshape(1, -1).to(device), \n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=torch.ones_like(inp.reshape(1, -1)).to(device),\n",
    "                            max_new_tokens=50)[0] for inp in collated['input_ids_generate']]\n",
    "\n",
    "gen_outputs_m2 = [model.generate(inp.reshape(1, -1).to(device), \n",
    "                                    pad_token_id=tokenizer.eos_token_id,\n",
    "                                    attention_mask=torch.ones_like(inp.reshape(1, -1)).to(device))[0].cpu() for inp in gen_outputs]\n",
    "\n",
    "labels = collated['labels']\n",
    "labels_masks = labels > 0\n",
    "\n",
    "preds_full = [out[len(inp):] for inp, out in zip(collated['input_ids_generate'], gen_outputs_m2)]\n",
    "labels_full = [lab[m][1:] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "print(len(preds_full), len(labels_full))\n",
    "\n",
    "preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "labels_cot = [extract_cot(lab) for lab in labels_full_text]\n",
    "labels_ans = [extract_answer(lab) for lab in labels_full_text]\n",
    "\n",
    "acc_cot = np.mean([c == p for c, p in zip(preds_cot, labels_cot)])\n",
    "acc_ans = np.mean([c == lab for c, lab in zip(preds_ans, labels_ans)])\n",
    "\n",
    "print(f\"Accuracy COT: {acc_cot}\")\n",
    "print(f\"Accuracy Answer: {acc_ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cot(text):\n",
    "        try:\n",
    "                return text[:text.index(ans_text)]\n",
    "        except ValueError:\n",
    "                return ''\n",
    "\n",
    "def extract_answer(text):\n",
    "        try:\n",
    "                return text.split(ans_text)[1]\n",
    "        except IndexError:\n",
    "                return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['600<|endoftext|><|endoftext|>',\n",
       " '7.5<|endoftext|><|endoftext|>',\n",
       " '1400<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '240<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '160<|endoftext|><|endoftext|>',\n",
       " '1.5<|endoftext|><|endoftext|>',\n",
       " '21.75<|endoftext|><|endoftext|>',\n",
       " '21.5<|endoftext|><|endoftext|>',\n",
       " '14<|endoftext|><|endoftext|>',\n",
       " '88<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '130<|endoftext|><|endoftext|>',\n",
       " '16.67<|endoftext|><|endoftext|>',\n",
       " '33.33<|endoftext|><|endoftext|>',\n",
       " '25<|endoftext|><|endoftext|>',\n",
       " '-4<|endoftext|><|endoftext|>',\n",
       " '90<|endoftext|><|endoftext|>',\n",
       " '46<|endoftext|><|endoftext|>',\n",
       " '715<|endoftext|><|endoftext|>',\n",
       " '5<|endoftext|><|endoftext|>',\n",
       " '10<|endoftext|><|endoftext|>',\n",
       " '1<|endoftext|><|endoftext|>',\n",
       " '12000<|endoftext|><|endoftext|>',\n",
       " '160<|endoftext|><|endoftext|>',\n",
       " '5<|endoftext|><|endoftext|>',\n",
       " '45<|endoftext|><|endoftext|>',\n",
       " '74<|endoftext|><|endoftext|>',\n",
       " '460<|endoftext|><|endoftext|>',\n",
       " '52<|endoftext|><|endoftext|>',\n",
       " '98<|endoftext|><|endoftext|>',\n",
       " '35<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '550<|endoftext|><|endoftext|>',\n",
       " '64<|endoftext|><|endoftext|>',\n",
       " '240<|endoftext|><|endoftext|>',\n",
       " '12<|endoftext|><|endoftext|>',\n",
       " '25<|endoftext|><|endoftext|>',\n",
       " '65<|endoftext|><|endoftext|>',\n",
       " '16<|endoftext|><|endoftext|>',\n",
       " '1240<|endoftext|><|endoftext|>',\n",
       " '50<|endoftext|><|endoftext|>',\n",
       " '135<|endoftext|><|endoftext|>',\n",
       " '2<|endoftext|><|endoftext|>',\n",
       " '65<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '9<|endoftext|><|endoftext|>',\n",
       " '10<|endoftext|><|endoftext|>',\n",
       " '50<|endoftext|><|endoftext|>',\n",
       " '35<|endoftext|><|endoftext|>',\n",
       " '12000<|endoftext|><|endoftext|>',\n",
       " '2056<|endoftext|><|endoftext|>',\n",
       " '25<|endoftext|><|endoftext|>',\n",
       " '480<|endoftext|><|endoftext|>',\n",
       " '-3000<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '135<|endoftext|><|endoftext|>',\n",
       " '55<|endoftext|><|endoftext|>',\n",
       " '19<|endoftext|><|endoftext|>',\n",
       " '12<|endoftext|><|endoftext|>',\n",
       " '3.6<|endoftext|><|endoftext|>',\n",
       " '19<|endoftext|><|endoftext|>',\n",
       " '130<|endoftext|><|endoftext|>',\n",
       " '50<|endoftext|><|endoftext|>',\n",
       " '2120<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '10000<|endoftext|><|endoftext|>',\n",
       " '18<|endoftext|><|endoftext|>',\n",
       " '1350<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '2<|endoftext|><|endoftext|>',\n",
       " '80<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '66<|endoftext|><|endoftext|>',\n",
       " '57<|endoftext|><|endoftext|>',\n",
       " '60<|endoftext|><|endoftext|>',\n",
       " '60<|endoftext|><|endoftext|>',\n",
       " '10<|endoftext|><|endoftext|>',\n",
       " '54<|endoftext|><|endoftext|>',\n",
       " '312<|endoftext|><|endoftext|>',\n",
       " '0.4<|endoftext|><|endoftext|>',\n",
       " '80<|endoftext|><|endoftext|>',\n",
       " '70<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '16<|endoftext|><|endoftext|>',\n",
       " '25<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '18000<|endoftext|><|endoftext|>',\n",
       " '60<|endoftext|><|endoftext|>',\n",
       " '720<|endoftext|><|endoftext|>',\n",
       " '32<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '16<|endoftext|><|endoftext|>',\n",
       " '9<|endoftext|><|endoftext|>',\n",
       " '100<|endoftext|><|endoftext|>',\n",
       " '70<|endoftext|><|endoftext|>',\n",
       " '1040<|endoftext|><|endoftext|>',\n",
       " '5.5<|endoftext|><|endoftext|>',\n",
       " '145<|endoftext|><|endoftext|>',\n",
       " '2<|endoftext|><|endoftext|>',\n",
       " '16<|endoftext|><|endoftext|>',\n",
       " '55<|endoftext|><|endoftext|>',\n",
       " '216<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '90<|endoftext|><|endoftext|>',\n",
       " '80<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '336<|endoftext|><|endoftext|>',\n",
       " '825<|endoftext|><|endoftext|>',\n",
       " '360<|endoftext|><|endoftext|>',\n",
       " '1500<|endoftext|><|endoftext|>',\n",
       " '120<|endoftext|><|endoftext|>',\n",
       " '22<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '1200<|endoftext|><|endoftext|>',\n",
       " '78<|endoftext|><|endoftext|>',\n",
       " '61.2<|endoftext|><|endoftext|>',\n",
       " '0<|endoftext|><|endoftext|>',\n",
       " '6.25<|endoftext|><|endoftext|>',\n",
       " '24<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '9<|endoftext|><|endoftext|>',\n",
       " '50<|endoftext|><|endoftext|>',\n",
       " '24<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '440<|endoftext|><|endoftext|>',\n",
       " '130<|endoftext|><|endoftext|>',\n",
       " '10<|endoftext|><|endoftext|>',\n",
       " '195<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '-500<|endoftext|><|endoftext|>',\n",
       " '1300<|endoftext|><|endoftext|>',\n",
       " '200<|endoftext|><|endoftext|>',\n",
       " '13<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '150<|endoftext|><|endoftext|>',\n",
       " '1.5<|endoftext|><|endoftext|>',\n",
       " '17<|endoftext|><|endoftext|>',\n",
       " '46.4<|endoftext|><|endoftext|>',\n",
       " '12<|endoftext|><|endoftext|>',\n",
       " '31<|endoftext|><|endoftext|>',\n",
       " '11<|endoftext|><|endoftext|>',\n",
       " '90<|endoftext|><|endoftext|>',\n",
       " '55<|endoftext|><|endoftext|>',\n",
       " '40<|endoftext|><|endoftext|>',\n",
       " '35<|endoftext|><|endoftext|>',\n",
       " '119.5<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '198<|endoftext|><|endoftext|>',\n",
       " '144<|endoftext|><|endoftext|>',\n",
       " '600<|endoftext|><|endoftext|>',\n",
       " '1<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '900<|endoftext|><|endoftext|>',\n",
       " '1800<|endoftext|><|endoftext|>',\n",
       " '63<|endoftext|><|endoftext|>',\n",
       " '25<|endoftext|><|endoftext|>',\n",
       " '14<|endoftext|><|endoftext|>',\n",
       " '38<|endoftext|><|endoftext|>',\n",
       " '14<|endoftext|><|endoftext|>',\n",
       " '100<|endoftext|><|endoftext|>',\n",
       " '23<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '190<|endoftext|><|endoftext|>',\n",
       " '3360<|endoftext|><|endoftext|>',\n",
       " '15300<|endoftext|><|endoftext|>',\n",
       " '2.1<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '1920<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '320<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '10<|endoftext|><|endoftext|>',\n",
       " '585<|endoftext|><|endoftext|>',\n",
       " '80<|endoftext|><|endoftext|>',\n",
       " '48<|endoftext|><|endoftext|>',\n",
       " '41<|endoftext|><|endoftext|>',\n",
       " '7200<|endoftext|><|endoftext|>',\n",
       " '43<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '61.25<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '72<|endoftext|><|endoftext|>',\n",
       " '3.33<|endoftext|><|endoftext|>',\n",
       " '34<|endoftext|><|endoftext|>',\n",
       " '5.5<|endoftext|><|endoftext|>',\n",
       " '60<|endoftext|><|endoftext|>',\n",
       " '2000<|endoftext|><|endoftext|>',\n",
       " '780<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '150<|endoftext|><|endoftext|>',\n",
       " '50<|endoftext|><|endoftext|>',\n",
       " '54<|endoftext|><|endoftext|>',\n",
       " '10<|endoftext|><|endoftext|>',\n",
       " '10<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '1600<|endoftext|><|endoftext|>',\n",
       " '120<|endoftext|><|endoftext|>',\n",
       " '14<|endoftext|><|endoftext|>',\n",
       " '12<|endoftext|><|endoftext|>',\n",
       " '16<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '22000<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '480<|endoftext|><|endoftext|>',\n",
       " '21<|endoftext|><|endoftext|>',\n",
       " '1<|endoftext|><|endoftext|>',\n",
       " '50<|endoftext|><|endoftext|>',\n",
       " '26.67<|endoftext|><|endoftext|>',\n",
       " '4200<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '46.67<|endoftext|><|endoftext|>',\n",
       " '-20<|endoftext|><|endoftext|>',\n",
       " '24<|endoftext|><|endoftext|>',\n",
       " '14<|endoftext|><|endoftext|>',\n",
       " '22<|endoftext|><|endoftext|>',\n",
       " '9<|endoftext|><|endoftext|>',\n",
       " '72<|endoftext|><|endoftext|>',\n",
       " '19<|endoftext|><|endoftext|>',\n",
       " '53<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '21<|endoftext|><|endoftext|>',\n",
       " '9<|endoftext|><|endoftext|>',\n",
       " '70<|endoftext|><|endoftext|>',\n",
       " '180<|endoftext|><|endoftext|>',\n",
       " '86<|endoftext|><|endoftext|>',\n",
       " '18<|endoftext|><|endoftext|>',\n",
       " '27<|endoftext|><|endoftext|>',\n",
       " '32<|endoftext|><|endoftext|>',\n",
       " '82<|endoftext|><|endoftext|>',\n",
       " '12.5<|endoftext|><|endoftext|>',\n",
       " '12<|endoftext|><|endoftext|>',\n",
       " '1260<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '144<|endoftext|><|endoftext|>',\n",
       " '13<|endoftext|><|endoftext|>',\n",
       " '58<|endoftext|><|endoftext|>',\n",
       " '-80<|endoftext|><|endoftext|>',\n",
       " '-1<|endoftext|><|endoftext|>',\n",
       " '50<|endoftext|><|endoftext|>',\n",
       " '21<|endoftext|><|endoftext|>',\n",
       " '46<|endoftext|><|endoftext|>',\n",
       " '1895<|endoftext|><|endoftext|>',\n",
       " '135<|endoftext|><|endoftext|>',\n",
       " '32<|endoftext|><|endoftext|>',\n",
       " '5<|endoftext|><|endoftext|>',\n",
       " '1350<|endoftext|><|endoftext|>',\n",
       " '200<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '22<|endoftext|><|endoftext|>',\n",
       " '180<|endoftext|><|endoftext|>',\n",
       " '29<|endoftext|><|endoftext|>',\n",
       " '4500<|endoftext|><|endoftext|>',\n",
       " '120<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '10.5<|endoftext|><|endoftext|>',\n",
       " '11<|endoftext|><|endoftext|>',\n",
       " '21<|endoftext|><|endoftext|>',\n",
       " '2600<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '385<|endoftext|><|endoftext|>',\n",
       " '18<|endoftext|><|endoftext|>',\n",
       " '80<|endoftext|><|endoftext|>',\n",
       " '1.33<|endoftext|><|endoftext|>',\n",
       " '28<|endoftext|><|endoftext|>',\n",
       " '21<|endoftext|><|endoftext|>',\n",
       " '78<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '46.4<|endoftext|><|endoftext|>',\n",
       " '80<|endoftext|><|endoftext|>',\n",
       " '14000<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '7<|endoftext|><|endoftext|>',\n",
       " '260<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '34<|endoftext|><|endoftext|>',\n",
       " '84<|endoftext|><|endoftext|>',\n",
       " '750<|endoftext|><|endoftext|>',\n",
       " '16<|endoftext|><|endoftext|>',\n",
       " '225<|endoftext|><|endoftext|>',\n",
       " '260<|endoftext|><|endoftext|>',\n",
       " '54<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '49<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '60<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '3240<|endoftext|><|endoftext|>',\n",
       " '54<|endoftext|><|endoftext|>',\n",
       " '5<|endoftext|><|endoftext|>',\n",
       " '62.5<|endoftext|><|endoftext|>',\n",
       " '1000<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '18<|endoftext|><|endoftext|>',\n",
       " '32<|endoftext|><|endoftext|>',\n",
       " '9<|endoftext|><|endoftext|>',\n",
       " '14<|endoftext|><|endoftext|>',\n",
       " '66.67<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '-7<|endoftext|><|endoftext|>',\n",
       " '171<|endoftext|><|endoftext|>',\n",
       " '93<|endoftext|><|endoftext|>',\n",
       " '3000<|endoftext|><|endoftext|>',\n",
       " '306<|endoftext|><|endoftext|>',\n",
       " '4000<|endoftext|><|endoftext|>',\n",
       " '1450<|endoftext|><|endoftext|>',\n",
       " '80<|endoftext|><|endoftext|>',\n",
       " '142.86<|endoftext|><|endoftext|>',\n",
       " '326<|endoftext|><|endoftext|>',\n",
       " '400<|endoftext|><|endoftext|>',\n",
       " '17.5<|endoftext|><|endoftext|>',\n",
       " '7.5<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '120<|endoftext|><|endoftext|>',\n",
       " '-5<|endoftext|><|endoftext|>',\n",
       " '36<|endoftext|><|endoftext|>',\n",
       " '1200<|endoftext|><|endoftext|>',\n",
       " '2200<|endoftext|><|endoftext|>',\n",
       " '19<|endoftext|><|endoftext|>',\n",
       " '90<|endoftext|><|endoftext|>',\n",
       " '21<|endoftext|><|endoftext|>',\n",
       " '288<|endoftext|><|endoftext|>',\n",
       " '3600<|endoftext|><|endoftext|>',\n",
       " '28<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '60<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '71<|endoftext|><|endoftext|>',\n",
       " '700<|endoftext|><|endoftext|>',\n",
       " '200<|endoftext|><|endoftext|>',\n",
       " '31250<|endoftext|><|endoftext|>',\n",
       " '125<|endoftext|><|endoftext|>',\n",
       " '18<|endoftext|><|endoftext|>',\n",
       " '44<|endoftext|><|endoftext|>',\n",
       " '3.5<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '130<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '1456<|endoftext|><|endoftext|>',\n",
       " '2900<|endoftext|><|endoftext|>',\n",
       " '175<|endoftext|><|endoftext|>',\n",
       " '36<|endoftext|><|endoftext|>',\n",
       " '1040<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '600<|endoftext|><|endoftext|>',\n",
       " '-4.5<|endoftext|><|endoftext|>',\n",
       " '-4<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '1.5<|endoftext|><|endoftext|>',\n",
       " '1268<|endoftext|><|endoftext|>',\n",
       " '125<|endoftext|><|endoftext|>',\n",
       " '42<|endoftext|><|endoftext|>',\n",
       " '83<|endoftext|><|endoftext|>',\n",
       " '44<|endoftext|><|endoftext|>',\n",
       " '600<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '49<|endoftext|><|endoftext|>',\n",
       " '2.67<|endoftext|><|endoftext|>',\n",
       " '54<|endoftext|><|endoftext|>',\n",
       " '80<|endoftext|><|endoftext|>',\n",
       " '5.83<|endoftext|><|endoftext|>',\n",
       " '17<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '52<|endoftext|><|endoftext|>',\n",
       " '750<|endoftext|><|endoftext|>',\n",
       " '15<|endoftext|><|endoftext|>',\n",
       " '16<|endoftext|><|endoftext|>',\n",
       " '330<|endoftext|><|endoftext|>',\n",
       " '18000<|endoftext|><|endoftext|>',\n",
       " '2000<|endoftext|><|endoftext|>',\n",
       " '-150<|endoftext|><|endoftext|>',\n",
       " '46.67<|endoftext|><|endoftext|>',\n",
       " '10<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '82<|endoftext|><|endoftext|>',\n",
       " '-38400<|endoftext|><|endoftext|>',\n",
       " '45<|endoftext|><|endoftext|>',\n",
       " '9<|endoftext|><|endoftext|>',\n",
       " '23<|endoftext|><|endoftext|>',\n",
       " '90<|endoftext|><|endoftext|>',\n",
       " '2<|endoftext|><|endoftext|>',\n",
       " '5<|endoftext|><|endoftext|>',\n",
       " '18<|endoftext|><|endoftext|>',\n",
       " '505<|endoftext|><|endoftext|>',\n",
       " '143<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '35<|endoftext|><|endoftext|>',\n",
       " '660<|endoftext|><|endoftext|>',\n",
       " '98<|endoftext|><|endoftext|>',\n",
       " '32<|endoftext|><|endoftext|>',\n",
       " '-200<|endoftext|><|endoftext|>',\n",
       " '50<|endoftext|><|endoftext|>',\n",
       " '13<|endoftext|><|endoftext|>',\n",
       " '2<|endoftext|><|endoftext|>',\n",
       " '22<|endoftext|><|endoftext|>',\n",
       " '25<|endoftext|><|endoftext|>',\n",
       " '27<|endoftext|><|endoftext|>',\n",
       " '4.5<|endoftext|><|endoftext|>',\n",
       " '225<|endoftext|><|endoftext|>',\n",
       " '91<|endoftext|><|endoftext|>',\n",
       " '3.5<|endoftext|><|endoftext|>',\n",
       " '14<|endoftext|><|endoftext|>',\n",
       " '133.34<|endoftext|><|endoftext|>',\n",
       " '94.50<|endoftext|><|endoftext|>',\n",
       " '700<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '64<|endoftext|><|endoftext|>',\n",
       " '10<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '75<|endoftext|><|endoftext|>',\n",
       " '96<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '10<|endoftext|><|endoftext|>',\n",
       " '35<|endoftext|><|endoftext|>',\n",
       " '150<|endoftext|><|endoftext|>',\n",
       " '40<|endoftext|><|endoftext|>',\n",
       " '7<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '27<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '19<|endoftext|><|endoftext|>',\n",
       " '450<|endoftext|><|endoftext|>',\n",
       " '180<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>',\n",
       " '25<|endoftext|><|endoftext|>',\n",
       " '320<|endoftext|><|endoftext|>',\n",
       " '58<|endoftext|><|endoftext|>',\n",
       " '12<|endoftext|><|endoftext|>',\n",
       " '27<|endoftext|><|endoftext|>',\n",
       " '180<|endoftext|><|endoftext|>',\n",
       " '1440<|endoftext|><|endoftext|>',\n",
       " '300<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '90<|endoftext|><|endoftext|>',\n",
       " '2<|endoftext|><|endoftext|>',\n",
       " '35<|endoftext|><|endoftext|>',\n",
       " '5<|endoftext|><|endoftext|>',\n",
       " '19200<|endoftext|><|endoftext|>',\n",
       " '-5<|endoftext|><|endoftext|>',\n",
       " '-18<|endoftext|><|endoftext|>',\n",
       " '203<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '440<|endoftext|><|endoftext|>',\n",
       " '18<|endoftext|><|endoftext|>',\n",
       " '96<|endoftext|><|endoftext|>',\n",
       " '500<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '21<|endoftext|><|endoftext|>',\n",
       " '56<|endoftext|><|endoftext|>',\n",
       " '2000<|endoftext|><|endoftext|>',\n",
       " '25<|endoftext|><|endoftext|>',\n",
       " '4.33<|endoftext|><|endoftext|>',\n",
       " '-1489<|endoftext|><|endoftext|>',\n",
       " '100<|endoftext|><|endoftext|>',\n",
       " '9<|endoftext|><|endoftext|>',\n",
       " '2<|endoftext|><|endoftext|>',\n",
       " '8<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '12<|endoftext|><|endoftext|>',\n",
       " '90<|endoftext|><|endoftext|>',\n",
       " '30<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '1260<|endoftext|><|endoftext|>',\n",
       " '5<|endoftext|><|endoftext|>',\n",
       " '48<|endoftext|><|endoftext|>',\n",
       " '36<|endoftext|><|endoftext|>',\n",
       " '2.92<|endoftext|><|endoftext|>',\n",
       " '5.5<|endoftext|><|endoftext|>',\n",
       " '56<|endoftext|><|endoftext|>',\n",
       " '6<|endoftext|><|endoftext|>',\n",
       " '18<|endoftext|><|endoftext|>',\n",
       " '203<|endoftext|><|endoftext|>',\n",
       " '45<|endoftext|><|endoftext|>',\n",
       " '12<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '113<|endoftext|><|endoftext|>',\n",
       " '4<|endoftext|><|endoftext|>',\n",
       " '22<|endoftext|><|endoftext|>',\n",
       " '20<|endoftext|><|endoftext|>',\n",
       " '24<|endoftext|><|endoftext|>',\n",
       " '3<|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['600', '7.5', '1400', '15', '240', '20', '160', '1.5', '21.75', '21.5']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_cot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300', '10', '1400', '15', '240', '20', '10', '2', '25', '25']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ans[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_ans[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "Accuracy COT: 0.6\n",
      "Accuracy Answer: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = collated['labels'][:, 1:]\n",
    "labels_masks = labels > 0\n",
    "\n",
    "preds_full = [out[len(inp):] for inp, out in zip(collated['input_ids_generate'], gen_outputs_m2)]\n",
    "labels_full = [lab[m][1:] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "print(len(preds_full), len(labels_full))\n",
    "\n",
    "preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "labels_cot = [extract_cot(lab) for lab in labels_full_text]\n",
    "labels_ans = [extract_answer(lab) for lab in labels_full_text]\n",
    "\n",
    "acc_cot = np.mean([c == p for c, p in zip(preds_cot, labels_cot)])\n",
    "acc_ans = np.mean([c == lab for c, lab in zip(preds_ans, labels_ans)])\n",
    "\n",
    "print(f\"Accuracy COT: {acc_cot}\")\n",
    "print(f\"Accuracy Answer: {acc_ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'booydar/gsm8k'\n",
    "train_dataset = datasets.load_dataset(dataset, split='train')\n",
    "valid_dataset = datasets.load_dataset(dataset, split='valid')\n",
    "\n",
    "# cot\n",
    "args.use_cot = True\n",
    "\n",
    "checkpoints = [\n",
    "    \"/workspace-SR006.nfs2/Bulatov_A/rmt/runs/gsm8k/gpt2/SEGM_1x1024_1024_LR1e-03-cot/checkpoint-23500/pytorch_model.bin\",\n",
    "    \"/workspace-SR006.nfs2/Bulatov_A/rmt/runs/gsm8k/gpt2/SEGM_1x1024_1024_LR1e-05-cot/checkpoint-5000/pytorch_model.bin\",\n",
    "    \"/workspace-SR006.nfs2/Bulatov_A/rmt/runs/gsm8k/gpt2/SEGM_1x1024_1024_LR3e-04-cot/checkpoint-17500/pytorch_model.bin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cpt_path = \"/workspace-SR006.nfs2/Bulatov_A/rmt/runs/test/4_by_4_mult/Llama-3.2-1B-Instruct/smol:qa1-5-1:9/SEGM_1x1024_1024_64_LR3e-04-lora-mnc-distill__short/checkpoint-5000/pytorch_model.bin\"\n",
    "# cpt_path = \"/workspace-SR006.nfs2/Bulatov_A/rmt/runs/test/4_by_4_mult/gpt2/SEGM_1x1024_1024_LR3e-04/checkpoint-25000/pytorch_model.bin\"\n",
    "# cpt_path = \"/workspace-SR006.nfs2/Bulatov_A/rmt/runs/gsm8k/gpt2/SEGM_1x1024_1024_LR3e-04/checkpoint-16500/pytorch_model.bin\"\n",
    "cpt_path = \"/workspace-SR006.nfs2/Bulatov_A/rmt/runs/gsm8k/gpt2/SEGM_1x1024_1024_LR1e-03-cot/checkpoint-23500/pytorch_model.bin\"\n",
    "model.load_state_dict(torch.load(cpt_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|| 384620/384620 [00:00<00:00, 935478.06 examples/s] \n",
      "Generating valid split: 100%|| 500/500 [00:00<00:00, 110028.96 examples/s]\n",
      "Generating test split: 100%|| 1319/1319 [00:00<00:00, 242176.81 examples/s]\n",
      "Generating train_no_aug split: 100%|| 6973/6973 [00:00<00:00, 436918.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# dataset_dir = \"/workspace-SR006.nfs2/Bulatov_A/rmt/data/implicit_chain_of_thought/4_by_4_mult\"\n",
    "\n",
    "# train_path = os.path.join(dataset_dir, \"train\")\n",
    "# valid_path = os.path.join(dataset_dir, \"valid\")\n",
    "# train_dataset = datasets.load_from_disk(train_path)\n",
    "# valid_dataset = datasets.load_from_disk(valid_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = Holder()\n",
    "# args.use_cot = False\n",
    "args.use_cot = True\n",
    "args.num_mem_tokens = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "think = ans = tokenizer.bos_token_id\n",
    "eos = tokenizer.eos_token_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, input_ids_generate, labels, labels_mask, attention_mask = [], [], [], [], []\n",
    "    for sample in batch:\n",
    "        task, lab, cot = sample['task'], sample['labels'], sample['cot']\n",
    "        task_tokens = tokenizer.encode(task, add_special_tokens=False)\n",
    "        labels_tokens = tokenizer.encode(lab, add_special_tokens=False)\n",
    "        cot_tokens = tokenizer.encode(cot, add_special_tokens=False)\n",
    "\n",
    "        if args.use_cot:\n",
    "            full_input = task_tokens + [think] + cot_tokens + [ans] + labels_tokens + [eos]\n",
    "            gen_input = task_tokens + [think]\n",
    "        else:\n",
    "            full_input = task_tokens + [ans] + labels_tokens + [eos]\n",
    "            gen_input = task_tokens + [ans]\n",
    "        \n",
    "        inp_ids = torch.tensor(full_input)\n",
    "        input_ids.append(inp_ids)\n",
    "        input_ids_generate.append(torch.tensor(gen_input))\n",
    "\n",
    "\n",
    "        lab = torch.tensor(full_input)\n",
    "        lab[:len(task_tokens)] = -100\n",
    "        labels.append(lab)\n",
    "\n",
    "        lab_mask = torch.ones_like(inp_ids)\n",
    "        lab_mask[:len(task_tokens)] = 0\n",
    "        labels_mask.append(lab_mask)\n",
    "        attention_mask.append(torch.ones_like(inp_ids))\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "    # input_ids_generate = pad_sequence(input_ids_generate, padding_value=id_pad_value, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "    labels = pad_sequence(labels, padding_value=id_pad_value, batch_first=True)\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'input_ids_generate': input_ids_generate,\n",
    "                'labels': labels,\n",
    "                'attention_mask': attention_mask,\n",
    "                }\n",
    "    if args.num_mem_tokens is not None:\n",
    "        # add labels mask only for RMT, ARMT\n",
    "        collated['labels_mask'] = labels_mask.bool()\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "think_text = tokenizer.decode(think)\n",
    "ans_text = tokenizer.decode(ans)\n",
    "\n",
    "def extract_cot(text):\n",
    "        try:\n",
    "                return text[:text.index(ans_text)]\n",
    "        except ValueError:\n",
    "                return ''\n",
    "\n",
    "def extract_answer(text):\n",
    "        try:\n",
    "                return text.split(ans_text)[1]\n",
    "        except IndexError:\n",
    "                return ''\n",
    "                \n",
    "def compute_accuracy(eval_pred):\n",
    "        preds = eval_pred.predictions.argmax(axis=-1)[:, 1:-1]\n",
    "        labels = eval_pred.label_ids[:, 2:]\n",
    "\n",
    "        labels_masks = labels > 0\n",
    "        preds_full = [p[m] for p, m in zip(preds, labels_masks)]\n",
    "        labels_full = [lab[m] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "        print(len(preds_full), len(labels_full))\n",
    "\n",
    "        preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "        labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "        preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "        preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "        labels_cot = [extract_cot(lab) for lab in labels_full_text]\n",
    "        labels_ans = [extract_answer(lab) for lab in labels_full_text]\n",
    "\n",
    "        acc_cot = np.mean([c == p for c, p in zip(preds_cot, labels_cot)])\n",
    "        acc_ans = np.mean([c == lab for c, lab in zip(preds_ans, labels_ans)])\n",
    "\n",
    "        return {'accuracy_cot': acc_cot, 'accuracy_ans': acc_ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "batch = [valid_dataset[i] for i in range(10)]\n",
    "collated = collate_fn(batch)\n",
    "\n",
    "out = model(**collated)\n",
    "print(out.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### no teacher forcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "gen_outputs = [model.generate(inp.reshape(1, -1), \n",
    "                              attention_mask=torch.ones_like(inp.reshape(1, -1)),\n",
    "                              max_new_tokens=50)[0] for inp in collated['input_ids_generate']]\n",
    "\n",
    "gen_outputs_m2 = [model.generate(inp.reshape(1, -1), attention_mask=torch.ones_like(inp.reshape(1, -1)))[0] for inp in gen_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_outputs_m2 = [out[len(inp):] for inp, out in zip(collated['input_ids_generate'], gen_outputs_m2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>><|endoftext|>300<|endoftext|>',\n",
       " '<<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>><|endoftext|>10<|endoftext|>',\n",
       " '<<2000*30/100=600>> <<2000-600=1400>><|endoftext|>1400<|endoftext|>',\n",
       " '<<21/7=3>> <<5*3=15>><|endoftext|>15<|endoftext|>',\n",
       " '<<200*3=600>> <<600*0.4=240>><|endoftext|>240<|endoftext|>',\n",
       " '<<1/2*100=50>> <<3/5*50=30>> <<50+30=80>> <<100-80=20>><|endoftext|>20<|endoftext|>',\n",
       " '<<40*2=80>> <<80*2=160>><|endoftext|>160<|endoftext|>',\n",
       " '<<12*2=24>> <<4*1=4>> <<4*3=12>> <<24+4+12=40>> <<40/4=10>><|endoftext|>10<|endoftext|>',\n",
       " '<<2*2.25=4.50>> <<4.50+3.50+4+3.50=15.00>> <<2*2.50=5.00>> <<15+5+3.50=23.50>><|endoftext|>',\n",
       " '<<28/4=7>> <<3*7+1=22>><|endoftext|>22<|endoftext|>']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(gen_outputs_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "Accuracy COT: 0.3\n",
      "Accuracy Answer: 0.6\n"
     ]
    }
   ],
   "source": [
    "preds = gen_outputs_m2\n",
    "labels_masks = labels > 0\n",
    "\n",
    "preds_full = gen_outputs_m2 #[out[len(inp):] for inp, out in zip(collated['input_ids_generate'], gen_outputs_m2)]\n",
    "labels_full = [lab[m][1:] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "print(len(preds_full), len(labels_full))\n",
    "\n",
    "preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "labels_cot = [extract_cot(lab) for lab in labels_full_text]\n",
    "labels_ans = [extract_answer(lab) for lab in labels_full_text]\n",
    "\n",
    "acc_cot = np.mean([c == p for c, p in zip(preds_cot, labels_cot)])\n",
    "acc_ans = np.mean([c == lab for c, lab in zip(preds_ans, labels_ans)])\n",
    "\n",
    "print(f\"Accuracy COT: {acc_cot}\")\n",
    "print(f\"Accuracy Answer: {acc_ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>><|endoftext|>300<|endoftext|>',\n",
       " '<<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>><|endoftext|>10<|endoftext|>',\n",
       " '<<2000*30/100=600>> <<2000-600=1400>><|endoftext|>1400<|endoftext|>',\n",
       " '<<21/7=3>> <<5*3=15>><|endoftext|>15<|endoftext|>',\n",
       " '<<200*3=600>> <<600*0.4=240>><|endoftext|>240<|endoftext|>',\n",
       " '<<1/2*100=50>> <<3/5*50=30>> <<50+30=80>> <<100-80=20>><|endoftext|>20<|endoftext|>',\n",
       " '<<40*2=80>> <<80*2=160>><|endoftext|>160<|endoftext|>',\n",
       " '<<12*2=24>> <<4*1=4>> <<4*3=12>> <<24+4+12=40>> <<40/4=10>><|endoftext|>10<|endoftext|>',\n",
       " '<<2*2.25=4.50>> <<4.50+3.50+4+3.50=15.00>> <<2*2.50=5.00>> <<15+5+3.50=23.50>><|endoftext|>',\n",
       " '<<28/4=7>> <<3*7+1=22>><|endoftext|>22<|endoftext|>']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>><|endoftext|>300<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>><|endoftext|>10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<30/100*2000=600>> <<2000-600=1400>><|endoftext|>1400<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<21/7=3>> <<5*3=15>><|endoftext|>15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<200*3=600>> <<600*.4=240>><|endoftext|>240<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<1/2*100=50>> <<3/5*50=30>> <<50-30=20>><|endoftext|>20<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<40/2=20>> <<20/2=10>><|endoftext|>10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<12*2=24>> <<4*1=4>> <<3*4=12>> <<24+4+12=40>> <<12+4+4=20>> <<40/20=2>><|endoftext|>2<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<2*2.25=4.50>> <<2*4=8.00>> <<2*2.50=5.00>> <<4.50+8.00+.50+5.00+3.50+3.50=25.00>><|endoftext|>25<|endoftext|>',\n",
       " '<<32=32>> <<8=8>><|endoftext|>25<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>>',\n",
       " '<<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>>',\n",
       " '<<2000*30/100=600>> <<2000-600=1400>>',\n",
       " '<<21/7=3>> <<5*3=15>>',\n",
       " '<<200*3=600>> <<600*0.4=240>>',\n",
       " '<<1/2*100=50>> <<3/5*50=30>> <<50+30=80>> <<100-80=20>>',\n",
       " '<<40*2=80>> <<80*2=160>>',\n",
       " '<<12*2=24>> <<4*1=4>> <<4*3=12>> <<24+4+12=40>> <<40/4=10>>',\n",
       " '<<2*2.25=4.50>> <<4.50+3.50+4+3.50=15.00>> <<2*2.50=5.00>> <<15+5+3.50=23.50>>',\n",
       " '<<28/4=7>> <<3*7+1=22>>']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>>',\n",
       " '<<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>>',\n",
       " '<<30/100*2000=600>> <<2000-600=1400>>',\n",
       " '<<21/7=3>> <<5*3=15>>',\n",
       " '<<200*3=600>> <<600*.4=240>>',\n",
       " '<<1/2*100=50>> <<3/5*50=30>> <<50-30=20>>',\n",
       " '<<40/2=20>> <<20/2=10>>',\n",
       " '<<12*2=24>> <<4*1=4>> <<3*4=12>> <<24+4+12=40>> <<12+4+4=20>> <<40/20=2>>',\n",
       " '<<2*2.25=4.50>> <<2*4=8.00>> <<2*2.50=5.00>> <<4.50+8.00+.50+5.00+3.50+3.50=25.00>>',\n",
       " '<<32=32>> <<8=8>>']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300', '10', '1400', '15', '240', '20', '160', '10', '', '22']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300', '10', '1400', '15', '240', '20', '10', '2', '25', '25']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### teacher forcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 172]), (10, 173))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_masks.shape, preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "Accuracy COT: 0.3\n",
      "Accuracy Answer: 0.9\n"
     ]
    }
   ],
   "source": [
    "# preds = eval_pred.predictions.argmax(axis=-1)[:, :-1]\n",
    "# labels = eval_pred.label_ids[:, 1:]\n",
    "\n",
    "preds = out.logits.argmax(dim=-1).cpu().numpy()[:, :-1]\n",
    "labels = collated['labels'][:, 1:]\n",
    "\n",
    "labels_masks = labels > 0\n",
    "# labels_masks[:, :1] = False\n",
    "preds_full = [p[m][1:] for p, m in zip(preds, labels_masks)]\n",
    "labels_full = [lab[m][1:] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "print(len(preds_full), len(labels_full))\n",
    "\n",
    "preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "labels_cot = [extract_cot(lab) for lab in labels_full_text]\n",
    "labels_ans = [extract_answer(lab) for lab in labels_full_text]\n",
    "\n",
    "acc_cot = np.mean([c == p for c, p in zip(preds_cot, labels_cot)])\n",
    "acc_ans = np.mean([c == lab for c, lab in zip(preds_ans, labels_ans)])\n",
    "\n",
    "print(f\"Accuracy COT: {acc_cot}\")\n",
    "print(f\"Accuracy Answer: {acc_ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>><|endoftext|>300<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>><|endoftext|>10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<2000/100*2000=600>> <<2000-600=1400>><|endoftext|>1400<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<21/7=3>> <<5*3=15>><|endoftext|>15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<200*3=600>> <<600*4=240>><|endoftext|>240<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<1/2*100=50>> <<3/5*50=30>> <<50+30=20>><|endoftext|>20<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<40*2=20>> <<20/2=10>><|endoftext|>10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<12*2=24>> <<4*1=4>> <<4*4=12>> <<24+4+12=40>> <<40+4+12=20>> <<40/20=2>><|endoftext|>2<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<2*2.25=4.50>> <<4*4=8.00>> <<2*2.50=5.00>> <<4.50+8+00+50+5+00+3.50=2.50=28.00>><|endoftext|>25<|endoftext|>',\n",
       " '<<28/32>> <<(.8>> <<8<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>><|endoftext|>300<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>><|endoftext|>10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<30/100*2000=600>> <<2000-600=1400>><|endoftext|>1400<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<21/7=3>> <<5*3=15>><|endoftext|>15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<200*3=600>> <<600*.4=240>><|endoftext|>240<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<1/2*100=50>> <<3/5*50=30>> <<50-30=20>><|endoftext|>20<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<40/2=20>> <<20/2=10>><|endoftext|>10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<12*2=24>> <<4*1=4>> <<3*4=12>> <<24+4+12=40>> <<12+4+4=20>> <<40/20=2>><|endoftext|>2<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<<2*2.25=4.50>> <<2*4=8.00>> <<2*2.50=5.00>> <<4.50+8.00+.50+5.00+3.50+3.50=25.00>><|endoftext|>25<|endoftext|>',\n",
       " '<<32=32>> <<8=8>><|endoftext|>25<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>>',\n",
       " '<<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>>',\n",
       " '<<2000/100*2000=600>> <<2000-600=1400>>',\n",
       " '<<21/7=3>> <<5*3=15>>',\n",
       " '<<200*3=600>> <<600*4=240>>',\n",
       " '<<1/2*100=50>> <<3/5*50=30>> <<50+30=20>>',\n",
       " '<<40*2=20>> <<20/2=10>>',\n",
       " '<<12*2=24>> <<4*1=4>> <<4*4=12>> <<24+4+12=40>> <<40+4+12=20>> <<40/20=2>>',\n",
       " '<<2*2.25=4.50>> <<4*4=8.00>> <<2*2.50=5.00>> <<4.50+8+00+50+5+00+3.50=2.50=28.00>>',\n",
       " '<<28/32>> <<(.8>> <<8']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>>',\n",
       " '<<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>>',\n",
       " '<<2000/100*2000=600>> <<2000-600=1400>>',\n",
       " '<<21/7=3>> <<5*3=15>>',\n",
       " '<<200*3=600>> <<600*4=240>>',\n",
       " '<<1/2*100=50>> <<3/5*50=30>> <<50+30=20>>',\n",
       " '<<40*2=20>> <<20/2=10>>',\n",
       " '<<12*2=24>> <<4*1=4>> <<4*4=12>> <<24+4+12=40>> <<40+4+12=20>> <<40/20=2>>',\n",
       " '<<2*2.25=4.50>> <<4*4=8.00>> <<2*2.50=5.00>> <<4.50+8+00+50+5+00+3.50=2.50=28.00>>',\n",
       " '<<28/32>> <<(.8>> <<8']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300', '10', '1400', '15', '240', '20', '10', '2', '25', '']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300', '10', '1400', '15', '240', '20', '10', '2', '25', '25']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Older pretokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "# if args.use_cot in (False, None):\n",
    "#     inputs_key = 'examples_nocot'\n",
    "#     labels_key = 'labels_nocot'\n",
    "# else:\n",
    "#     inputs_key = 'examples_all'\n",
    "#     labels_key = 'labels_all'\n",
    "    \n",
    "# def collate_fn(batch):\n",
    "#     input_ids = [torch.tensor(b[inputs_key]) for b in batch]\n",
    "#     labels = [torch.tensor(b[labels_key]) for b in batch]\n",
    "#     attention_mask = [torch.ones_like(b, dtype=int) for b in input_ids]\n",
    "#     # labels_mask defines which input_ids participate in loss calculation\n",
    "#     labels_mask = [torch.sign(torch.tensor(b[labels_key])) for b in batch]\n",
    "\n",
    "\n",
    "#     input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "#     labels = pad_sequence(labels, padding_value=id_pad_value, batch_first=True)\n",
    "#     attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "#     labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "#     collated = {'input_ids': input_ids,\n",
    "#                 'labels': labels, \n",
    "#                 'attention_mask': attention_mask,\n",
    "#                 }\n",
    "#     if args.num_mem_tokens is not None:\n",
    "#         # add labels mask only for RMT, ARMT\n",
    "#         collated['labels_mask'] = labels_mask.bool()\n",
    "#     return collated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_cot(text):\n",
    "#     if '<|endoftext|>' not in text:\n",
    "#         return ''\n",
    "#     else:\n",
    "#         return text.split('<|endoftext|>')[0].strip()\n",
    "\n",
    "# def extract_answer(text):\n",
    "#     if '####' not in text:\n",
    "#         return ''\n",
    "#     else:\n",
    "#         ans = text.split('####')[-1]\n",
    "#         ans = ans.split('<|endoftext|>')[0]\n",
    "#         return ans.strip()\n",
    "        \n",
    "# def compute_accuracy(eval_pred):\n",
    "#     preds = eval_pred.predictions[:, :-1]\n",
    "#     labels = eval_pred.label_ids[:, 1:]\n",
    "#     # inputs = eval_pred.inputs\n",
    "#     # losses = eval_pred.losses\n",
    "\n",
    "#     # labels = collated['labels'][:, 1:]\n",
    "\n",
    "#     labels_masks = labels > 0\n",
    "#     preds_full = [p[m] for p, m in zip(preds, labels_masks)]\n",
    "#     labels_full = [l[m] for l, m in zip(labels, labels_masks)]\n",
    "\n",
    "#     preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "#     labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "#     preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "#     preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "#     labels_cot = [extract_cot(l) for l in labels_full_text]\n",
    "#     labels_ans = [extract_answer(l) for l in labels_full_text]\n",
    "    \n",
    "#     # Calculate accuracy only on the unignored tokens\n",
    "#     acc_cot = np.mean([c == l for c, l in zip(preds_cot, labels_cot)])\n",
    "#     acc_ans = np.mean([c == l for c, l in zip(preds_ans, labels_ans)])\n",
    "\n",
    "#     return {'accuracy_cot': acc_cot, 'accuracy_ans': acc_ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = eval_pred.predictions\n",
    "# label_ids = eval_pred.label_ids\n",
    "# inputs = eval_pred.inputs\n",
    "# losses = eval_pred.losses\n",
    "# # elements = (self.predictions, self.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "batch = [valid_dataset[i] for i in range(10)]\n",
    "collated = collate_fn(batch)\n",
    "\n",
    "out = model(**collated)\n",
    "print(out.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0672, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 175)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = out.logits.argmax(dim=-1).cpu().numpy()\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_text = tokenizer.batch_decode(preds, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_text[0].split('<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''.split('<|endoftext|>')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = collated['labels'][:, 1:]\n",
    "\n",
    "labels_masks = labels > 0\n",
    "preds_full = [p[m] for p, m in zip(preds[:, :-1], labels_masks)]\n",
    "labels_full = [l[m] for l, m in zip(labels, labels_masks)]\n",
    "\n",
    "preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "labels_cot = [extract_cot(l) for l in labels_full_text]\n",
    "labels_ans = [extract_answer(l) for l in labels_full_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = collated['labels'][:, 1:]\n",
    "\n",
    "# labels_masks = labels > 0\n",
    "# preds_full = [p[m] for p, m in zip(preds[:, :-1], labels_masks)]\n",
    "# labels_full = [l[m] for l, m in zip(labels, labels_masks)]\n",
    "\n",
    "# preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "# labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "# preds_cot = [p.split('<|endoftext|>')[0].strip() for p in preds_full_text]\n",
    "# labels_cot = [l.split('<|endoftext|>')[0].strip() for l in labels_full_text]\n",
    "\n",
    "# # preds_ans = [p.split('<|endoftext|>')[1].strip()[4:] for p in preds_full_text]\n",
    "# # labels_ans = [l.split('<|endoftext|>')[1].strip()[4:] for l in labels_full_text]\n",
    "\n",
    "# preds_ans = [p.split('####')[1].strip()[4:] for p in preds_full_text]\n",
    "# labels_ans = [l.split('####')[1].split('astrip()[4:] for l in labels_full_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year?<|endoftext|><<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>><|endoftext|>300<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'Hannah has three dogs. The first dog eats 1.5 cups of dog food a day. The second dog eats twice as much while the third dog eats 2.5 cups more than the second dog. How many cups of dog food should Hannah prepare in a day for her three dogs?<|endoftext|><<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>><|endoftext|>10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'Travis wants to fly to Australia. The regular tickets cost about $2000. As Travis is a student, he will get a 30% discount on this price. How much does he need to pay for his ticket?<|endoftext|><<30/100*2000=600>> <<2000-600=1400>><|endoftext|>1400<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'A set of 7 spoons costs $21. If each spoon would be sold separately, how much would 5 spoons cost?<|endoftext|><<21/7=3>> <<5*3=15>><|endoftext|>15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'Tom bought his games for $200.  They tripled in value and he then sold 40% of them.  How much did he sell the games for?<|endoftext|><<200*3=600>> <<600*.4=240>><|endoftext|>240<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"Maggie went to Lou's aquarium and saw 100 goldfish in the aquarium. She asked if she could take some home to care for, and she was allowed to catch half of them. While using a catching net, she caught 3/5 of the total number of goldfish she was allowed to take home. How many goldfish does Maggie remain with to catch to get the total number she was allowed to take home?<|endoftext|><<1/2*100=50>> <<3/5*50=30>> <<50-30=20>><|endoftext|>20<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " 'Kenny played basketball last week. He ran for twice as long as he played basketball, and he practiced on the trumpet for twice as long as he ran. If he practiced on the trumpet for 40 hours, how many hours did Kenny play basketball last week?<|endoftext|><<40/2=20>> <<20/2=10>><|endoftext|>10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'Marcia wants to buy some fruit. Apples cost $2, bananas cost $1, and oranges cost $3. If Marcia buys 12 apples, 4 bananas and 4 oranges, what is the average cost of each piece of fruit in dollars?<|endoftext|><<12*2=24>> <<4*1=4>> <<3*4=12>> <<24+4+12=40>> <<12+4+4=20>> <<40/20=2>><|endoftext|>2<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"Its Meghans turn to pick up her team's coffee order.  She needs 2 drip coffees that are $2.25 each and one double shot espresso thats $3.50.  She needs 2 lattes that are $4.00 and needs to add vanilla syrup to one of those for an additional $0.50.  She also needs 2 cold brew coffees that are $2.50 each and 1 cappuccino for $3.50.  How much is the coffee order?<|endoftext|><<2*2.25=4.50>> <<2*4=8.00>> <<2*2.50=5.00>> <<4.50+8.00+.50+5.00+3.50+3.50=25.00>><|endoftext|>25<|endoftext|>\",\n",
       " 'Roman and Remy took separate showers. Remy used 1 more gallon than 3 times the number of gallons that Roman used for his shower. Together the boys used 33 gallons of water.  How many gallons did Remy use?<|endoftext|><<32=32>> <<8=8>><|endoftext|>25<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(collated['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>><|endoftext|>300<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<|endoftext|><<1.5*2=3>> <<3+2.5=5.5>> <<1.5+3+5.5=10>><|endoftext|>10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<|endoftext|><<30/100*2000=600>> <<2000-600=1400>><|endoftext|>1400<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<|endoftext|><<21/7=3>> <<5*3=15>><|endoftext|>15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<|endoftext|><<200*3=600>> <<600*.4=240>><|endoftext|>240<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<|endoftext|><<1/2*100=50>> <<3/5*50=30>> <<50-30=20>><|endoftext|>20<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<|endoftext|><<40/2=20>> <<20/2=10>><|endoftext|>10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<|endoftext|><<12*2=24>> <<4*1=4>> <<3*4=12>> <<24+4+12=40>> <<12+4+4=20>> <<40/20=2>><|endoftext|>2<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '<|endoftext|><<2*2.25=4.50>> <<2*4=8.00>> <<2*2.50=5.00>> <<4.50+8.00+.50+5.00+3.50+3.50=25.00>><|endoftext|>25<|endoftext|>',\n",
       " '<|endoftext|><<32=32>> <<8=8>><|endoftext|>25<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlabels_text\u001b[49m\n\u001b[32m      2\u001b[39m [\u001b[32m0\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m<|endoftext|>\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'labels_text' is not defined"
     ]
    }
   ],
   "source": [
    "labels_text\n",
    "[0].split('<|endoftext|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'labels_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p, l, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(preds, collated[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m], \u001b[43mcollated\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(p[m], l[m])\n",
      "\u001b[31mKeyError\u001b[39m: 'labels_mask'"
     ]
    }
   ],
   "source": [
    "for p, l, m in zip(preds, collated['labels'], collated['labels_mask']):\n",
    "    print(p[m], l[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   11,   860,   860,   604,  1635,   657,   657,   767,   657,\n",
       "         642,   642,   642,   642,   718,   352,  1343,   657,   657,\n",
       "         718,   604,   860,   657,   357,   642,   642,   352,   352,\n",
       "         352,   352,  1267,  1343,   657,   657,   642,   860,   657,\n",
       "         767,   657,   357,   642,   642,   718,   657,   362,   807,\n",
       "         657,  1267,  1343,   657,   657,   657,   657,   718,   604,\n",
       "         860,   657,   220, 50256,  1303, 21017,   642,   642,   718,\n",
       "         657,   807,   362,   657,   352,   220, 50256,  1303])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[', 9 9 4 * 0 0 7 0 5 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|> #### 5 5 6 0 8 2 0 1 <|endoftext|> #', ' the 0 1 6 0 1 0 8 3 6 6 7 1 1 3 + 0 4 8 7 0 2 ( 6 1 0 9 3 2 ) + 0 0 4 8 7 0 2 ( 6 1 4 7 1 3 2 ) + 0 0 0 2 7 3 6 3 <|endoftext|> #### 6 1 4 9 8 6 8 3 <|endoftext|> #', ' the 0 8 4 + 2 8 3 0 8 8 0 0 5 7 + 0 4 8 3 4 8 ( 8 4 8 8 1 9 ) + 0 0 6 7 3 9 0 ( 8 4 4 6 5 8 1 ) + 0 0 0 2 3 6 5 6 <|endoftext|> #### 8 4 4 8 8 4 7 6 <|endoftext|> #', ', 2 3 2\\n 0 0 0 1 9 9 0 9 2 1 + 0 2 1 2 7 1 ( 9 2 0 5 8 1 ) + 0 0 8 1 8 5 2 ( 9 2 8 6 6 7 2 ) + 0 0 0 5 1 5 1 2 <|endoftext|> #### 9 2 8 1 8 2 4 2 <|endoftext|> #', ' the 1 - 0 0 0 1 3 2 0 0 4 6 8 5 + 0 0 5 6 6 3 ( 0 4 1 5 2 4 ) + 0 0 0 8 9 3 4 ( 0 4 1 3 2 8 4 ) + 0 0 0 0 5 6 6 3 <|endoftext|> #### 0 4 1 3 7 4 1 4 <|endoftext|> #', ', 4 5 1 0 8 2 8 1 2 2 5 2 4 2 + 0 9 8 1 8 1 ( 2 4 1 6 0 2 ) + 0 0 4 0 5 8 4 ( 2 4 5 6 5 0 5 ) + 0 0 0 1 4 4 2 4 <|endoftext|> #### 2 4 5 7 9 4 7 4 <|endoftext|> #', ' the 7 0 9\\n 0 8 3 0 0 8 4 7 9 0 + 0 6 6 8 3 4 ( 8 0 4 8 4 4 ) + 0 0 4 7 8 4 0 ( 8 0 8 5 3 9 0 ) + 0 0 0 4 4 2 9 2 <|endoftext|> #### 8 0 8 9 7 1 0 3 <|endoftext|> #', ', 3 9 2 0 0 0 3 0 0 0 0 0 0 0 + 0 0 6 4 8 0 ( 0 0 6 4 8 0 ) + 0 0 2 9 6 1 0 ( 0 0 8 3 5 2 0 ) + 0 0 0 8 2 2 5 1 <|endoftext|> #### 0 0 8 1 8 4 5 1 <|endoftext|> #', ', 7 7 3average 1 0 0 2 5 5 5 5 2 2 + 0 6 6 0 7 2 ( 5 1 2 3 9 2 ) + 0 0 0 0 0 0 0 ( 5 1 2 3 9 2 0 ) + 0 0 0 5 5 5 2 2 <|endoftext|> #### 5 1 2 8 4 8 2 2 <|endoftext|> #', ', 5 9 1ACH 0 3 7 0 0 0 0 0 0 0 + 0 7 3 9 5 3 ( 0 7 3 9 5 3 ) + 0 0 9 7 9 1 1 ( 0 7 2 7 5 5 1 ) + 0 0 0 9 7 9 1 1 <|endoftext|> #### 0 7 2 6 3 5 3 1 <|endoftext|> #']\n"
     ]
    }
   ],
   "source": [
    "pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=False)\n",
    "print(pred_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 5 6 3 2 * 7 4 3 4 <|endoftext|> 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|> #### 5 5 6 0 8 2 0 1 <|endoftext|>', ' 6 9 1 5 * 6 4 4 7 <|endoftext|> 6 7 1 1 3 + 0 4 8 7 0 2 ( 6 1 0 9 3 2 ) + 0 0 4 8 7 0 2 ( 6 1 4 7 1 3 2 ) + 0 0 0 2 7 3 6 3 <|endoftext|> #### 6 1 4 9 8 6 8 3 <|endoftext|>', ' 6 7 3 9 * 8 9 1 7 <|endoftext|> 8 0 0 5 7 + 0 4 8 3 4 8 ( 8 4 8 8 1 9 ) + 0 0 6 7 3 9 0 ( 8 4 4 6 5 8 1 ) + 0 0 0 2 3 6 5 6 <|endoftext|> #### 8 4 4 8 8 4 7 6 <|endoftext|>', ' 3 0 3 4 * 3 4 6 5 <|endoftext|> 9 0 9 2 1 + 0 2 1 2 7 1 ( 9 2 0 5 8 1 ) + 0 0 8 1 8 5 2 ( 9 2 8 6 6 7 2 ) + 0 0 0 5 1 5 1 2 <|endoftext|> #### 9 2 8 1 8 2 4 2 <|endoftext|>', ' 0 3 3 7 * 8 5 6 5 <|endoftext|> 0 4 6 8 5 + 0 0 5 6 6 3 ( 0 4 1 5 2 4 ) + 0 0 0 8 9 3 4 ( 0 4 1 3 2 8 4 ) + 0 0 0 0 5 6 6 3 <|endoftext|> #### 0 4 1 3 7 4 1 4 <|endoftext|>', ' 3 6 0 6 * 4 3 8 7 <|endoftext|> 2 5 2 4 2 + 0 9 8 1 8 1 ( 2 4 1 6 0 2 ) + 0 0 4 0 5 8 4 ( 2 4 5 6 5 0 5 ) + 0 0 0 1 4 4 2 4 <|endoftext|> #### 2 4 5 7 9 4 7 4 <|endoftext|>', ' 4 7 8 4 * 2 9 1 6 <|endoftext|> 8 4 7 9 0 + 0 6 6 8 3 4 ( 8 0 4 8 4 4 ) + 0 0 4 7 8 4 0 ( 8 0 8 5 3 9 0 ) + 0 0 0 4 4 2 9 2 <|endoftext|> #### 8 0 8 9 7 1 0 3 <|endoftext|>', ' 2 9 6 1 * 0 5 1 9 <|endoftext|> 0 0 0 0 0 + 0 0 6 4 8 0 ( 0 0 6 4 8 0 ) + 0 0 2 9 6 1 0 ( 0 0 8 3 5 2 0 ) + 0 0 0 8 2 2 5 1 <|endoftext|> #### 0 0 8 1 8 4 5 1 <|endoftext|>', ' 1 1 5 4 * 5 6 0 5 <|endoftext|> 5 5 5 2 2 + 0 6 6 0 7 2 ( 5 1 2 3 9 2 ) + 0 0 0 0 0 0 0 ( 5 1 2 3 9 2 0 ) + 0 0 0 5 5 5 2 2 <|endoftext|> #### 5 1 2 8 4 8 2 2 <|endoftext|>', ' 3 9 9 3 * 0 9 3 3 <|endoftext|> 0 0 0 0 0 + 0 7 3 9 5 3 ( 0 7 3 9 5 3 ) + 0 0 9 7 9 1 1 ( 0 7 2 7 5 5 1 ) + 0 0 0 9 7 9 1 1 <|endoftext|> #### 0 7 2 6 3 5 3 1 <|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "labels_texts = tokenizer.batch_decode(collated['input_ids'], skip_special_tokens=False)\n",
    "print(labels_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 71])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', 9 9 4 * 0 0 7 0 5 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|> #### 5 5 6 0 8 2 0 1 <|endoftext|> #'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   11,   860,   860,   604,  1635,   657,   657,   767,   657,\n",
       "         642,   642,   642,   642,   718,   352,  1343,   657,   657,\n",
       "         718,   604,   860,   657,   357,   642,   642,   352,   352,\n",
       "         352,   352,  1267,  1343,   657,   657,   642,   860,   657,\n",
       "         767,   657,   357,   642,   642,   718,   657,   362,   807,\n",
       "         657,  1267,  1343,   657,   657,   657,   657,   718,   604,\n",
       "         860,   657,   220, 50256,  1303, 21017,   642,   642,   718,\n",
       "         657,   807,   362,   657,   352,   220, 50256,  1303])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([71])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",  6\n",
      " 9  3\n",
      " 9  2\n",
      " 4  *\n",
      " *  7\n",
      " 0  4\n",
      " 0  3\n",
      " 7  4\n",
      " 0  \n",
      " 5 <|endoftext|>\n",
      " 5  5\n",
      " 5  5\n",
      " 5  5\n",
      " 6  6\n",
      " 1  1\n",
      " +  +\n",
      " 0  0\n",
      " 0  0\n",
      " 6  6\n",
      " 4  4\n",
      " 9  9\n",
      " 0  0\n",
      " (  (\n",
      " 5  5\n",
      " 5  5\n",
      " 1  1\n",
      " 1  1\n",
      " 1  1\n",
      " 1  1\n",
      " )  )\n",
      " +  +\n",
      " 0  0\n",
      " 0  0\n",
      " 5  5\n",
      " 9  9\n",
      " 0  0\n",
      " 7  7\n",
      " 0  0\n",
      " (  (\n",
      " 5  5\n",
      " 5  5\n",
      " 6  6\n",
      " 0  0\n",
      " 2  2\n",
      " 8  8\n",
      " 0  0\n",
      " )  )\n",
      " +  +\n",
      " 0  0\n",
      " 0  0\n",
      " 0  0\n",
      " 0  0\n",
      " 6  6\n",
      " 4  4\n",
      " 9  9\n",
      " 0  0\n",
      "   \n",
      "<|endoftext|> <|endoftext|>\n",
      " #  #\n",
      "### ###\n",
      " 5  5\n",
      " 5  5\n",
      " 6  6\n",
      " 0  0\n",
      " 8  8\n",
      " 2  2\n",
      " 0  0\n",
      " 1  1\n",
      "   \n",
      "<|endoftext|> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for p, t in zip(preds[0], collated['input_ids'][0][1:]):\n",
    "    # print(p, tokenizer.decode([p]), t, tokenizer.decode([t]))\n",
    "    print(tokenizer.decode([p]), tokenizer.decode([t]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
