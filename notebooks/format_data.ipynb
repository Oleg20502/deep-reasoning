{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mult & gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../tools/implicit_chain_of_thought/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace-SR006.nfs2/Bulatov_A/env_main/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import argparse\n",
    "import os\n",
    "import tqdm\n",
    "import inspect\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "\n",
    "from models.teacher import Teacher\n",
    "from models.configuration_teacher import TeacherConfig\n",
    "from data import CoTDataset, CoTDataCollator, extract_answer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Holder()\n",
    "args.train_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/4_by_4_mult/train.txt'\n",
    "args.val_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/4_by_4_mult/valid.txt'\n",
    "args.batch_size = 8\n",
    "args.learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at /workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/4_by_4_mult/train.txt\n",
      "tgt_avg:  49.0\n",
      "src_avg:  10.0\n",
      "ratios:  0.20408163265306123\n",
      "tgt_avg:  13.0\n",
      "src_avg:  10.0\n",
      "ratios:  0.7692307692307693\n",
      " 1 3 3 8 * 5 1 0 5 <|endoftext|> 5 5 6 1 4 + 0 1 3 3 8 0 ( 5 6 9 4 2 1 ) + 0 0 0 0 0 0 0 ( 5 6 9 4 2 1 0 ) + 0 0 0 5 5 6 1 4 <|endoftext|> #### 5 6 9 9 7 7 1 4 <|endoftext|>\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 642, 642, 718, 352, 604, 1343, 657, 352, 513, 513, 807, 657, 357, 642, 718, 860, 604, 362, 352, 1267, 1343, 657, 657, 657, 657, 657, 657, 657, 357, 642, 718, 860, 604, 362, 352, 657, 1267, 1343, 657, 657, 657, 642, 642, 718, 352, 604, 220, 50256]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1303, 21017, 642, 718, 860, 860, 767, 767, 352, 604, 220, 50256]\n",
      "[352, 513, 513, 807, 1635, 642, 352, 657, 642, 220, 50256, 1303, 21017, 642, 718, 860, 860, 767, 767, 352, 604, 220, 50256]\n",
      " 1 3 3 8 * 5 1 0 5 <|endoftext|> #### 5 6 9 9 7 7 1 4 <|endoftext|>\n",
      "[352, 513, 513, 807, 1635, 642, 352, 657, 642, 220]\n",
      "[50256, 1303, 21017, 642, 718, 860, 860, 767, 767, 352, 604, 220, 50256]\n",
      "Creating features from dataset file at /workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/4_by_4_mult/valid.txt\n",
      "tgt_avg:  49.0\n",
      "src_avg:  10.0\n",
      "ratios:  0.20408163265306123\n",
      "tgt_avg:  13.0\n",
      "src_avg:  10.0\n",
      "ratios:  0.7692307692307693\n",
      " 5 6 3 2 * 7 4 3 4 <|endoftext|> 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|> #### 5 5 6 0 8 2 0 1 <|endoftext|>\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 642, 642, 642, 718, 352, 1343, 657, 657, 718, 604, 860, 657, 357, 642, 642, 352, 352, 352, 352, 1267, 1343, 657, 657, 642, 860, 657, 767, 657, 357, 642, 642, 718, 657, 362, 807, 657, 1267, 1343, 657, 657, 657, 657, 718, 604, 860, 657, 220, 50256]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1303, 21017, 642, 642, 718, 657, 807, 362, 657, 352, 220, 50256]\n",
      "[642, 718, 513, 362, 1635, 767, 604, 513, 604, 220, 50256, 1303, 21017, 642, 642, 718, 657, 807, 362, 657, 352, 220, 50256]\n",
      " 5 6 3 2 * 7 4 3 4 <|endoftext|> #### 5 5 6 0 8 2 0 1 <|endoftext|>\n",
      "[642, 718, 513, 362, 1635, 767, 604, 513, 604, 220]\n",
      "[50256, 1303, 21017, 642, 642, 718, 657, 807, 362, 657, 352, 220, 50256]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = teacher.tokenizer\n",
    "collate_fn = CoTDataCollator(tokenizer)\n",
    "train_dataset = CoTDataset(tokenizer, args.train_path, 1024)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataset = CoTDataset(tokenizer, args.val_path, 1024)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = val_dataset[0]\n",
    "# len(x), [i.shape for i in x if isinstance(i, torch.Tensor)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     44\u001b[39m         data_dict[\u001b[33m\"\u001b[39m\u001b[33mlabels_all\u001b[39m\u001b[33m\"\u001b[39m].append(labels_all.numpy().tolist())\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data_dict\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m train_hf_data_dict = convert_to_hf_format(\u001b[43mtrain_dataset\u001b[49m)\n\u001b[32m     50\u001b[39m hf_train_dataset = datasets.Dataset.from_dict(train_hf_data_dict)\n\u001b[32m     52\u001b[39m valid_hf_data_dict = convert_to_hf_format(val_dataset)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def convert_to_hf_format(dataset):\n",
    "    data_dict = {\n",
    "        \"examples_cot\": [],\n",
    "        \"examples_nocot\": [],\n",
    "        \"labels_cot\": [],\n",
    "        \"labels_cot_shift\": [],\n",
    "        \"labels_nocot\": [],\n",
    "        \"src_sent_cot\": [],\n",
    "        \"src_sent_nocot\": [],\n",
    "        \"tgt_sent_cot\": [],\n",
    "        \"tgt_sent_nocot\": [],\n",
    "        \"examples_only\": [],\n",
    "        \"examples_all\": [],\n",
    "        \"labels_all\": []\n",
    "    }\n",
    "    \n",
    "    for item in dataset:\n",
    "        (\n",
    "            examples_cot,\n",
    "            examples_nocot,\n",
    "            labels_cot,\n",
    "            labels_cot_shift,\n",
    "            labels_nocot,\n",
    "            src_sent_cot,\n",
    "            src_sent_nocot,\n",
    "            tgt_sent_cot,\n",
    "            tgt_sent_nocot,\n",
    "            examples_only,\n",
    "            examples_all,\n",
    "            labels_all\n",
    "        ) = item  # Unpack the tuple returned by __getitem__\n",
    "        \n",
    "        data_dict[\"examples_cot\"].append(examples_cot.numpy().tolist())\n",
    "        data_dict[\"examples_nocot\"].append(examples_nocot.numpy().tolist())\n",
    "        data_dict[\"labels_cot\"].append(labels_cot.numpy().tolist())\n",
    "        data_dict[\"labels_cot_shift\"].append(labels_cot_shift.numpy().tolist())\n",
    "        data_dict[\"labels_nocot\"].append(labels_nocot.numpy().tolist())\n",
    "        data_dict[\"src_sent_cot\"].append(src_sent_cot.numpy().tolist())\n",
    "        data_dict[\"src_sent_nocot\"].append(src_sent_nocot.numpy().tolist())\n",
    "        data_dict[\"tgt_sent_cot\"].append(tgt_sent_cot.numpy().tolist())\n",
    "        data_dict[\"tgt_sent_nocot\"].append(tgt_sent_nocot.numpy().tolist())\n",
    "        data_dict[\"examples_only\"].append(examples_only.numpy().tolist())\n",
    "        data_dict[\"examples_all\"].append(examples_all.numpy().tolist())\n",
    "        data_dict[\"labels_all\"].append(labels_all.numpy().tolist())\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "train_hf_data_dict = convert_to_hf_format(train_dataset)\n",
    "hf_train_dataset = datasets.Dataset.from_dict(train_hf_data_dict)\n",
    "\n",
    "valid_hf_data_dict = convert_to_hf_format(val_dataset)\n",
    "hf_valid_dataset = datasets.Dataset.from_dict(valid_hf_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9G\t/workspace-SR006.nfs2/Bulatov_A/rmt/data/implicit_chain_of_thought/4_by_4_mult\n"
     ]
    }
   ],
   "source": [
    "! du -hs /workspace-SR006.nfs2/Bulatov_A/rmt/data/implicit_chain_of_thought/4_by_4_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/7 shards):   0%|          | 0/808000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (7/7 shards): 100%|██████████| 808000/808000 [00:04<00:00, 182417.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 60091.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/workspace-SR006.nfs2/Bulatov_A/rmt/data/implicit_chain_of_thought/4_by_4_mult\"\n",
    "hf_train_dataset.save_to_disk(os.path.join(save_path, \"train\"))\n",
    "hf_valid_dataset.save_to_disk(os.path.join(save_path, \"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_cot  5 6 3 2 * 7 4 3 4 <|endoftext|> 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|>\n",
      "\n",
      "examples_nocot  5 6 3 2 * 7 4 3 4 <|endoftext|> #### 5 5 6 0 8 2 0 1 <|endoftext|>\n",
      "\n",
      "labels_cot !!!!!!!!!!! 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|>\n",
      "\n",
      "labels_cot_shift !!!!!!!!!!<|endoftext|> 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|>\n",
      "\n",
      "labels_nocot !!!!!!!!!!! #### 5 5 6 0 8 2 0 1 <|endoftext|>\n",
      "\n",
      "src_sent_cot  5 6 3 2 * 7 4 3 4 \n",
      "\n",
      "src_sent_nocot  5 6 3 2 * 7 4 3 4 \n",
      "\n",
      "tgt_sent_cot <|endoftext|> 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|>\n",
      "\n",
      "tgt_sent_nocot <|endoftext|> #### 5 5 6 0 8 2 0 1 <|endoftext|>\n",
      "\n",
      "examples_only  5 6 3 2 * 7 4 3 4 <|endoftext|> \n",
      "\n",
      "examples_all  5 6 3 2 * 7 4 3 4 <|endoftext|> 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|> #### 5 5 6 0 8 2 0 1 <|endoftext|>\n",
      "\n",
      "labels_all !!!!!!!!!!! 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|> #### 5 5 6 0 8 2 0 1 <|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out first all features of sample\n",
    "\n",
    "for k, v in hf_valid_dataset[0].items():\n",
    "    tokens = [i if i > 0 else 0 for i in v]\n",
    "    print(k, tokenizer.decode(tokens))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mode = 'cot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.train_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/5_by_5_mult/train.txt'\n",
    "args.val_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/5_by_5_mult/valid.txt'\n",
    "\n",
    "# tokenizer = teacher.tokenizer\n",
    "collate_fn = CoTDataCollator(tokenizer)\n",
    "train_dataset = CoTDataset(tokenizer, args.train_path, 1024)\n",
    "val_dataset = CoTDataset(tokenizer, args.val_path, 1024)\n",
    "\n",
    "train_hf_data_dict = convert_to_hf_format(train_dataset)\n",
    "hf_dataset = datasets.Dataset.from_dict(train_hf_data_dict)\n",
    "\n",
    "valid_hf_data_dict = convert_to_hf_format(val_dataset)\n",
    "hf_valid_dataset = datasets.Dataset.from_dict(valid_hf_data_dict)\n",
    "\n",
    "save_path = \"/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/5_by_5_mult/\"\n",
    "hf_train_dataset.save_to_disk(os.path.join(save_path, \"train\"))\n",
    "hf_valid_dataset.save_to_disk(os.path.join(save_path, \"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hf_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.train_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/gsm8k/train.txt'\n",
    "args.val_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/gsm8k/valid.txt'\n",
    "args.train_no_aug_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/gsm8k/train_no_aug.txt'\n",
    "args.test_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/gsm8k/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at /workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/gsm8k/train_no_aug.txt\n",
      "tgt_avg:  68.31206080596587\n",
      "src_avg:  55.93819016205364\n",
      "ratios:  0.8188625771507747\n",
      "tgt_avg:  6.091352359099384\n",
      "src_avg:  55.93819016205364\n",
      "ratios:  9.18321365509123\n",
      " The town’s annual budget totals $32 million. If half of the budget goes towards policing and $12 million goes towards education. How much money is left for managing public spaces? <|endoftext|> The annual budget for policing is 32 / 2 = $16 million. The combined budget for education and policing is 16 + 12 = $28 million. There is 32 - 28 = $4 million to manage public spaces. <|endoftext|> #### 4 <|endoftext|>\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 383, 5079, 4466, 329, 21922, 318, 3933, 1220, 362, 796, 720, 1433, 1510, 13, 383, 5929, 4466, 329, 3707, 290, 21922, 318, 1467, 1343, 1105, 796, 720, 2078, 1510, 13, 1318, 318, 3933, 532, 2579, 796, 720, 19, 1510, 284, 6687, 1171, 9029, 13, 220, 50256]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1303, 21017, 604, 220, 50256]\n",
      "[383, 3240, 447, 247, 82, 5079, 4466, 26310, 720, 2624, 1510, 13, 1002, 2063, 286, 262, 4466, 2925, 3371, 21922, 290, 720, 1065, 1510, 2925, 3371, 3707, 13, 1374, 881, 1637, 318, 1364, 329, 11149, 1171, 9029, 30, 220, 50256, 1303, 21017, 604, 220, 50256]\n",
      " The town’s annual budget totals $32 million. If half of the budget goes towards policing and $12 million goes towards education. How much money is left for managing public spaces? <|endoftext|> #### 4 <|endoftext|>\n",
      "[383, 3240, 447, 247, 82, 5079, 4466, 26310, 720, 2624, 1510, 13, 1002, 2063, 286, 262, 4466, 2925, 3371, 21922, 290, 720, 1065, 1510, 2925, 3371, 3707, 13, 1374, 881, 1637, 318, 1364, 329, 11149, 1171, 9029, 30, 220]\n",
      "[50256, 1303, 21017, 604, 220, 50256]\n"
     ]
    }
   ],
   "source": [
    "collate_fn = CoTDataCollator(tokenizer)\n",
    "val_dataset = CoTDataset(tokenizer, args.val_path, 1024)\n",
    "test_dataset = CoTDataset(tokenizer, args.test_path, 1024)\n",
    "train_dataset = CoTDataset(tokenizer, args.train_path, 1024)\n",
    "train_no_aug_dataset = CoTDataset(tokenizer, args.train_no_aug_path, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_cot  Jason has to drive home which is 120 miles away. If he drives at 60 miles per hour for 30 minutes, what speed does he have to average for the remainder of the drive to get there in exactly 1 hour 30 minutes? <|endoftext|> <<60*0.5=30>> <<120-30=90>> <<1.5-0.5=1>> <|endoftext|>\n",
      "\n",
      "examples_nocot  Jason has to drive home which is 120 miles away. If he drives at 60 miles per hour for 30 minutes, what speed does he have to average for the remainder of the drive to get there in exactly 1 hour 30 minutes? <|endoftext|> #### 90 <|endoftext|>\n",
      "\n",
      "labels_cot !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! <<60*0.5=30>> <<120-30=90>> <<1.5-0.5=1>> <|endoftext|>\n",
      "\n",
      "labels_cot_shift !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<|endoftext|> <<60*0.5=30>> <<120-30=90>> <<1.5-0.5=1>> <|endoftext|>\n",
      "\n",
      "labels_nocot !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! #### 90 <|endoftext|>\n",
      "\n",
      "src_sent_cot  Jason has to drive home which is 120 miles away. If he drives at 60 miles per hour for 30 minutes, what speed does he have to average for the remainder of the drive to get there in exactly 1 hour 30 minutes? \n",
      "\n",
      "src_sent_nocot  Jason has to drive home which is 120 miles away. If he drives at 60 miles per hour for 30 minutes, what speed does he have to average for the remainder of the drive to get there in exactly 1 hour 30 minutes? \n",
      "\n",
      "tgt_sent_cot <|endoftext|> <<60*0.5=30>> <<120-30=90>> <<1.5-0.5=1>> <|endoftext|>\n",
      "\n",
      "tgt_sent_nocot <|endoftext|> #### 90 <|endoftext|>\n",
      "\n",
      "examples_only  Jason has to drive home which is 120 miles away. If he drives at 60 miles per hour for 30 minutes, what speed does he have to average for the remainder of the drive to get there in exactly 1 hour 30 minutes? <|endoftext|> \n",
      "\n",
      "examples_all  Jason has to drive home which is 120 miles away. If he drives at 60 miles per hour for 30 minutes, what speed does he have to average for the remainder of the drive to get there in exactly 1 hour 30 minutes? <|endoftext|> <<60*0.5=30>> <<120-30=90>> <<1.5-0.5=1>> <|endoftext|> #### 90 <|endoftext|>\n",
      "\n",
      "labels_all !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! <<60*0.5=30>> <<120-30=90>> <<1.5-0.5=1>> <|endoftext|> #### 90 <|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out first all features of sample\n",
    "i += 1\n",
    "for k, v in hf_valid_dataset[i].items():\n",
    "    tokens = [i if i > 0 else 0 for i in v]\n",
    "    print(k, tokenizer.decode(tokens))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_hf_data_dict = convert_to_hf_format(val_dataset)\n",
    "hf_valid_dataset = datasets.Dataset.from_dict(valid_hf_data_dict)\n",
    "\n",
    "test_hf_data_dict = convert_to_hf_format(test_dataset)\n",
    "hf_test_dataset = datasets.Dataset.from_dict(test_hf_data_dict)\n",
    "\n",
    "train_hf_data_dict = convert_to_hf_format(train_no_aug_dataset)\n",
    "hf_train_dataset = datasets.Dataset.from_dict(train_hf_data_dict)\n",
    "\n",
    "train_no_aug_hf_data_dict = convert_to_hf_format(train_no_aug_dataset)\n",
    "hf_train_no_aug_dataset = datasets.Dataset.from_dict(train_no_aug_hf_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 7518.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1319/1319 [00:00<00:00, 33810.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6973/6973 [00:00<00:00, 42020.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6973/6973 [00:00<00:00, 68710.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_path = \"/workspace-SR006.nfs2/Bulatov_A/rmt/data/implicit_chain_of_thought/gsm8k/\"\n",
    "\n",
    "hf_valid_dataset.save_to_disk(os.path.join(save_path, \"valid\"))\n",
    "hf_test_dataset.save_to_disk(os.path.join(save_path, \"test\"))\n",
    "hf_train_no_aug_dataset.save_to_disk(os.path.join(save_path, \"train_no_aug\"))\n",
    "hf_train_dataset.save_to_disk(os.path.join(save_path, \"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dir = \"/workspace-SR006.nfs2/Bulatov_A/rmt/data/implicit_chain_of_thought/4_by_4_mult\"\n",
    "\n",
    "# train_path = os.path.join(dataset_dir, \"train\")\n",
    "# valid_path = os.path.join(dataset_dir, \"valid\")\n",
    "\n",
    "# hf_train_dataset = datasets.load_from_disk(train_path)\n",
    "# hf_valid_dataset = datasets.load_from_disk(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_valid_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [hf_valid_dataset[i] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eot_id = 50256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor([e[\"examples_all\"] for e in batch])\n",
    "# attention_mask = torch.tensor([e[\"attention_mask\"] for e in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 116/116 [00:02<00:00, 44.46ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 116/116 [00:02<00:00, 43.12ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 116/116 [00:02<00:00, 47.66ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 116/116 [00:02<00:00, 49.97ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 116/116 [00:02<00:00, 41.50ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 116/116 [00:02<00:00, 39.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 116/116 [00:02<00:00, 43.68ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 7/7 [00:35<00:00,  5.08s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 79.90ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/booydar/4_by_4_mult/commit/e91a3ffd2668499d817d8c2f7fc3d45adbee3dc8', commit_message='Upload dataset', commit_description='', oid='e91a3ffd2668499d817d8c2f7fc3d45adbee3dc8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/booydar/4_by_4_mult', endpoint='https://huggingface.co', repo_type='dataset', repo_id='booydar/4_by_4_mult'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"/workspace-SR006.nfs2/Bulatov_A/rmt/data/implicit_chain_of_thought/4_by_4_mult\"\n",
    "train_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"train\"))\n",
    "valid_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"valid\"))\n",
    "if os.path.exists(os.path.join(dataset_path, \"test\")):\n",
    "    test_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"test\"))\n",
    "else:\n",
    "    test_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"valid\"))\n",
    "\n",
    "# unite train and valid datsets in a single one\n",
    "hf_ds = datasets.DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"valid\": valid_dataset\n",
    "})\n",
    "\n",
    "# add huggingface access token\n",
    "\n",
    "\n",
    "hf_ds.push_to_hub('booydar/4_by_4_mult', token='hf_1234567890abcdefg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00,  9.81ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 30.56ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 19.07ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 10.08ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/booydar/gsm8k/commit/4bce75ba7d3f7df96e5db3ce0446a69e0bb1055c', commit_message='Upload dataset', commit_description='', oid='4bce75ba7d3f7df96e5db3ce0446a69e0bb1055c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/booydar/gsm8k', endpoint='https://huggingface.co', repo_type='dataset', repo_id='booydar/gsm8k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"/workspace-SR006.nfs2/Bulatov_A/rmt/data/implicit_chain_of_thought/gsm8k\"\n",
    "train_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"train\"))\n",
    "valid_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"valid\"))\n",
    "test_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"test\"))\n",
    "train_no_aug_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"train_no_aug\"))\n",
    "\n",
    "\n",
    "# unite train and valid datsets in a single one\n",
    "hf_ds = datasets.DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"valid\": valid_dataset,\n",
    "    \"test\": test_dataset,\n",
    "    \"test_no_aug\": train_no_aug_dataset,\n",
    "})\n",
    "\n",
    "# add huggingface access token\n",
    "\n",
    "hf_ds.push_to_hub('booydar/gsm8k', token='hf_1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890!@#$%^&*()_+{}|:\"<>?`~АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя1234567890!@#$%^&*()_+{}|:\"<>?`~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add huggingface access token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_mem_tokens = None\n",
    "# args.training_mode = 'no_cot'\n",
    "args.use_cot = False\n",
    "if args.use_cot in (False, None):\n",
    "    inputs_key = 'examples_nocot'\n",
    "    labels_key = 'labels_nocot'\n",
    "else:\n",
    "    inputs_key = 'examples_all'\n",
    "    labels_key = 'labels_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "# id_pad_value = -100\n",
    "\n",
    "\n",
    "if args.use_cot in (False, None):\n",
    "    inputs_key = 'examples_nocot'\n",
    "    labels_key = 'labels_nocot'\n",
    "else:\n",
    "    inputs_key = 'examples_all'\n",
    "    labels_key = 'labels_all'\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(b[inputs_key]) for b in batch]\n",
    "    labels = [torch.tensor(b[labels_key]) for b in batch]\n",
    "    attention_mask = [torch.ones_like(b, dtype=int) for b in input_ids]\n",
    "    # labels_mask defines which input_ids participate in loss calculation\n",
    "    labels_mask = [torch.sign(torch.tensor(b[labels_key])) for b in batch]\n",
    "\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "    labels = pad_sequence(labels, padding_value=id_pad_value, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'attention_mask': attention_mask,\n",
    "                }\n",
    "    if args.num_mem_tokens is not None:\n",
    "        # add labels mask only for RMT, ARMT\n",
    "        collated['labels_mask'] = labels_mask.bool()\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [valid_dataset[i] for i in range(10)]\n",
    "collated = collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "out = model(**collated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'attention_mask'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.7507, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_cot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "# id_pad_value = -100\n",
    "\n",
    "\n",
    "if args.use_cot in (False, None):\n",
    "    inputs_key = 'examples_nocot'\n",
    "    labels_key = 'labels_nocot'\n",
    "else:\n",
    "    inputs_key = 'examples_all'\n",
    "    labels_key = 'labels_all'\n",
    "    \n",
    "split_cot_by = \">> <<\"\n",
    "# def collate_fn(batch):\n",
    "input_ids = [torch.tensor(b[inputs_key]) for b in batch]\n",
    "labels = [torch.tensor(b[labels_key]) for b in batch]\n",
    "attention_mask = [torch.ones_like(b, dtype=int) for b in input_ids]\n",
    "# labels_mask defines which input_ids participate in loss calculation\n",
    "labels_mask = [torch.sign(torch.tensor(b[labels_key])) for b in batch]\n",
    "\n",
    "\n",
    "input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "labels = pad_sequence(labels, padding_value=id_pad_value, batch_first=True)\n",
    "attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "collated = {'input_ids': input_ids,\n",
    "            'labels': labels, \n",
    "            'attention_mask': attention_mask,\n",
    "            }\n",
    "if args.num_mem_tokens is not None:\n",
    "    # add labels mask only for RMT, ARMT\n",
    "    collated['labels_mask'] = labels_mask.bool()\n",
    "# return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_cot  John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year? <|endoftext|> <<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>> <|endoftext|>\n",
      "\n",
      "examples_nocot  John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year? <|endoftext|> #### 300 <|endoftext|>\n",
      "\n",
      "labels_cot !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! <<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>> <|endoftext|>\n",
      "\n",
      "labels_cot_shift !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<|endoftext|> <<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>> <|endoftext|>\n",
      "\n",
      "labels_nocot !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! #### 300 <|endoftext|>\n",
      "\n",
      "src_sent_cot  John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year? \n",
      "\n",
      "src_sent_nocot  John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year? \n",
      "\n",
      "tgt_sent_cot <|endoftext|> <<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>> <|endoftext|>\n",
      "\n",
      "tgt_sent_nocot <|endoftext|> #### 300 <|endoftext|>\n",
      "\n",
      "examples_only  John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year? <|endoftext|> \n",
      "\n",
      "examples_all  John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year? <|endoftext|> <<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>> <|endoftext|> #### 300 <|endoftext|>\n",
      "\n",
      "labels_all !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! <<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>> <|endoftext|> #### 300 <|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out first all features of sample\n",
    "\n",
    "for k, v in valid_dataset[0].items():\n",
    "    tokens = [i if i > 0 else 0 for i in v]\n",
    "    print(k, tokenizer.decode(tokens))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split sample list by eot token\n",
    "sample = batch[0]['examples_all']\n",
    "parts = []\n",
    "current_part = []\n",
    "for i, token in enumerate(sample):\n",
    "    if token == eot_id:\n",
    "        parts.append(current_part)\n",
    "        current_part = []\n",
    "    else:\n",
    "        current_part.append(token)\n",
    "parts.append(current_part)  # Add the last part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [torch.tensor(b['input_ids']) for b in batch]\n",
    "    if not(\"labels\" in batch[0].keys()):\n",
    "        labels = [torch.tensor(b['input_ids'].copy()) for b in batch]\n",
    "    else:a\n",
    "        labels = [torch.tensor(b['labels']) for b in batch]\n",
    "    attention_mask = [torch.ones_like(b, dtype=int) for b in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(b['input_ids']) for b in batch]\n",
    "    if not(\"labels\" in batch[0].keys()):\n",
    "        labels = [torch.tensor(b['input_ids'].copy()) for b in batch]\n",
    "    else:a\n",
    "        labels = [torch.tensor(b['labels']) for b in batch]\n",
    "    attention_mask = [torch.ones_like(b, dtype=int) for b in input_ids]\n",
    "\n",
    "\n",
    "    labels_mask = [torch.ones_like(b, dtype=int) for b in input_ids]\n",
    "    \n",
    "    if getattr(args, 'loss_from_last_seg_only', False):\n",
    "        for m in labels_mask:\n",
    "            m[:-args.segment_size] = False\n",
    "\n",
    "    if getattr(args, 'no_loss_from_first_segment', False):\n",
    "        for m in labels_mask:\n",
    "            m[:args.segment_size] = False\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "    labels = pad_sequence(labels, padding_value=-100, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'attention_mask': attention_mask,\n",
    "                }\n",
    "    if args.num_mem_tokens is not None:\n",
    "        # add labels mask only for RMT, ARMT\n",
    "        collated['labels_mask'] = labels_mask.bool()\n",
    "    # TODO: add masking for response only\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5 6 3 2 * 7 4 3 4 <|endoftext|> 5 5 5 6 1 + 0 0 6 4 9 0 ( 5 5 1 1 1 1 ) + 0 0 5 9 0 7 0 ( 5 5 6 0 2 8 0 ) + 0 0 0 0 6 4 9 0 <|endoftext|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(hf_valid_dataset[0]['examples_cot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_valid_dataset[0]['labels_cot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  642,   718,   513,   362,  1635,   767,   604,   513,   604,   220,\n",
       "         50256,   642,   642,   642,   718,   352,  1343,   657,   657,   718,\n",
       "           604,   860,   657,   357,   642,   642,   352,   352,   352,   352,\n",
       "          1267,  1343,   657,   657,   642,   860,   657,   767,   657,   357,\n",
       "           642,   642,   718,   657,   362,   807,   657,  1267,  1343,   657,\n",
       "           657,   657,   657,   718,   604,   860,   657,   220, 50256]),\n",
       " tensor([  642,   718,   513,   362,  1635,   767,   604,   513,   604,   220,\n",
       "         50256,  1303, 21017,   642,   642,   718,   657,   807,   362,   657,\n",
       "           352,   220, 50256]),\n",
       " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,   642,   642,   642,   718,   352,  1343,   657,   657,   718,\n",
       "           604,   860,   657,   357,   642,   642,   352,   352,   352,   352,\n",
       "          1267,  1343,   657,   657,   642,   860,   657,   767,   657,   357,\n",
       "           642,   642,   718,   657,   362,   807,   657,  1267,  1343,   657,\n",
       "           657,   657,   657,   718,   604,   860,   657,   220, 50256]),\n",
       " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         50256,   642,   642,   642,   718,   352,  1343,   657,   657,   718,\n",
       "           604,   860,   657,   357,   642,   642,   352,   352,   352,   352,\n",
       "          1267,  1343,   657,   657,   642,   860,   657,   767,   657,   357,\n",
       "           642,   642,   718,   657,   362,   807,   657,  1267,  1343,   657,\n",
       "           657,   657,   657,   718,   604,   860,   657,   220, 50256]),\n",
       " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  1303, 21017,   642,   642,   718,   657,   807,   362,   657,\n",
       "           352,   220, 50256]),\n",
       " tensor([ 642,  718,  513,  362, 1635,  767,  604,  513,  604,  220]),\n",
       " tensor([ 642,  718,  513,  362, 1635,  767,  604,  513,  604,  220]),\n",
       " tensor([50256,   642,   642,   642,   718,   352,  1343,   657,   657,   718,\n",
       "           604,   860,   657,   357,   642,   642,   352,   352,   352,   352,\n",
       "          1267,  1343,   657,   657,   642,   860,   657,   767,   657,   357,\n",
       "           642,   642,   718,   657,   362,   807,   657,  1267,  1343,   657,\n",
       "           657,   657,   657,   718,   604,   860,   657,   220, 50256]),\n",
       " tensor([50256,  1303, 21017,   642,   642,   718,   657,   807,   362,   657,\n",
       "           352,   220, 50256]),\n",
       " tensor([  642,   718,   513,   362,  1635,   767,   604,   513,   604,   220,\n",
       "         50256,   220]),\n",
       " tensor([  642,   718,   513,   362,  1635,   767,   604,   513,   604,   220,\n",
       "         50256,   642,   642,   642,   718,   352,  1343,   657,   657,   718,\n",
       "           604,   860,   657,   357,   642,   642,   352,   352,   352,   352,\n",
       "          1267,  1343,   657,   657,   642,   860,   657,   767,   657,   357,\n",
       "           642,   642,   718,   657,   362,   807,   657,  1267,  1343,   657,\n",
       "           657,   657,   657,   718,   604,   860,   657,   220, 50256,  1303,\n",
       "         21017,   642,   642,   718,   657,   807,   362,   657,   352,   220,\n",
       "         50256]),\n",
       " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,   642,   642,   642,   718,   352,  1343,   657,   657,   718,\n",
       "           604,   860,   657,   357,   642,   642,   352,   352,   352,   352,\n",
       "          1267,  1343,   657,   657,   642,   860,   657,   767,   657,   357,\n",
       "           642,   642,   718,   657,   362,   807,   657,  1267,  1343,   657,\n",
       "           657,   657,   657,   718,   604,   860,   657,   220, 50256,  1303,\n",
       "         21017,   642,   642,   718,   657,   807,   362,   657,   352,   220,\n",
       "         50256]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m gen = \u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m)\n\u001b[32m      2\u001b[39m batch = \u001b[38;5;28mnext\u001b[39m(gen)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
