{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user33/.conda/envs/rmt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.reasoning import make_segment, split_cot\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_rmt.language_modeling import MemoryCell\n",
    "from modeling_rmt.experimental import RecurrentWrapperNoSegmentationGenerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "checkpoint_path = \"/home/user33/kashurin/RMT_SmolLM2-135M/cot/checkpoint-2200/pytorch_model.bin\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pad = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "bos = [tokenizer.bos_token_id]\n",
    "eos = [tokenizer.eos_token_id]\n",
    "think = tokenizer.encode(\"<issue_start>\")\n",
    "ans = tokenizer.encode(\"<issue_closed>\")\n",
    "\n",
    "delim = \">> <<\"\n",
    "\n",
    "\n",
    "memory_cell = MemoryCell(\n",
    "    model,\n",
    "    num_mem_tokens=16\n",
    ")\n",
    "\n",
    "model = RecurrentWrapperNoSegmentationGenerate(memory_cell, \n",
    "                                             max_n_segments=10, \n",
    "                                             think_token_id=think[0],\n",
    "                                             answer_token_id=ans[0],\n",
    "                                             bos_token_id=bos[0],\n",
    "                                             eos_token_id=eos[0]\n",
    "                                             )\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "model.to(device)\n",
    "print(':)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The future of AI is\",\n",
    "    \"In a galaxy far far away\",\n",
    "    \"Hello\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   504,  1774,   282,  5646,   314],\n",
       "        [  788,   253, 13247,  1869,  1869,  2025],\n",
       "        [    0,     0,     0,     0,     0, 19556]]), 'attention_mask': tensor([[0, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side='left')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = Holder()\n",
    "args.num_mem_tokens = 16\n",
    "args.task_name = 'gsm8k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # first, we segment each sample into task, cot steps and labels\n",
    "    segments_batch = []\n",
    "    for sample in batch:\n",
    "        task, lab, cot = sample['task'], sample['labels'], sample['cot']\n",
    "        task_tokens = tokenizer.encode(task, add_special_tokens=False)\n",
    "        labels_tokens = tokenizer.encode(lab, add_special_tokens=False)\n",
    "        cot_segments = split_cot(cot, by=delim)\n",
    "        cot_segment_tokens = tokenizer.batch_encode_plus(cot_segments, add_special_tokens=False)['input_ids']\n",
    "\n",
    "        segments = []\n",
    "        segments.append(make_segment(bos + task_tokens + think, loss=False))\n",
    "        for segment in cot_segment_tokens[:-1]:\n",
    "            segments.append(make_segment(bos + segment + think, loss=True))\n",
    "        segments.append(make_segment(bos + cot_segment_tokens[-1] + ans, loss=True))\n",
    "\n",
    "        segments.append(make_segment(bos + labels_tokens + eos, loss=True))\n",
    "        segments_batch.append(segments)\n",
    "\n",
    "    # if some samples have less segments than others, we pad them with empty segments\n",
    "    num_segments = max(len(segments) for segments in segments_batch)\n",
    "    for segments in segments_batch:\n",
    "        if len(segments) < num_segments:\n",
    "            segments.extend([make_segment(eos, loss=False)] * (num_segments - len(segments)))\n",
    "\n",
    "    # prepare segments for the whole batch\n",
    "    batch_segments = []\n",
    "    for i in range(num_segments):\n",
    "        padding_side = 'right'\n",
    "        if i == 0:\n",
    "            padding_side = 'left'\n",
    "\n",
    "        input_ids = [s[i]['input_ids'] for s in segments_batch]\n",
    "        attention_mask = [s[i]['attention_mask'] for s in segments_batch]\n",
    "        labels = [s[i]['labels'] for s in segments_batch]\n",
    "        labels_mask = [s[i]['labels_mask'] for s in segments_batch]\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad, padding_side=padding_side)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0, padding_side=padding_side)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100, padding_side=padding_side)\n",
    "        labels_mask = pad_sequence(labels_mask, batch_first=True, padding_value=False, padding_side=padding_side)\n",
    "\n",
    "        batch_segment = {'input_ids': input_ids,\n",
    "                            'attention_mask': attention_mask,\n",
    "                            'labels_mask': labels_mask,\n",
    "                            'labels': labels\n",
    "                            }\n",
    "        \n",
    "        batch_segments.append(batch_segment)\n",
    "    full_labels = torch.cat([s['labels'] for s in batch_segments], dim=1)\n",
    "    return {\"segments\": batch_segments, 'labels': full_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'booydar/gsm8k'\n",
    "train_dataset = datasets.load_dataset(dataset, split='train')\n",
    "valid_dataset = datasets.load_dataset(dataset, split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = Holder()\n",
    "args.max_new_tokens = 100\n",
    "args.task_name = 'gsm8k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels = [], []\n",
    "all_preds_cot, all_labels_cot = [], []\n",
    "all_preds_ans, all_labels_ans = [], []\n",
    "\n",
    "batch = valid_dataset.select(range(4))\n",
    "collated = collate_fn(batch)\n",
    "task = collated['segments'][0]\n",
    "task = {k:v.to(device) for k,v in task.items()}\n",
    "\n",
    "task_length = task['input_ids'].shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen_out = model.generate(\n",
    "        [task],\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        pad_token_id=eos[0]\n",
    "    )\n",
    "\n",
    "preds_full = torch.cat(gen_out, dim=1)\n",
    "labels = collated['labels']\n",
    "for i, (lab_tokens, pred_tokens) in enumerate(zip(labels, preds_full)):\n",
    "    labels_mask = lab_tokens != -100\n",
    "    lab_tokens = lab_tokens[labels_mask].tolist()\n",
    "\n",
    "    pred_tokens = pred_tokens[task_length:].tolist()\n",
    "    \n",
    "    ans_start_index_l = max(i for i, x in enumerate(lab_tokens) if x == ans[0])\n",
    "    ans_end_index_l = min(i for i, x in enumerate(lab_tokens) if x == eos[0])\n",
    "\n",
    "    if ans[0] in pred_tokens:\n",
    "        ans_start_index_p = max(i for i, x in enumerate(pred_tokens) if x == ans[0])\n",
    "    else:\n",
    "        ans_start_index_p = ans_start_index_l\n",
    "\n",
    "    if eos[0] in pred_tokens:\n",
    "        ans_end_index_p = min(i for i, x in enumerate(pred_tokens) if x == eos[0])\n",
    "    else:\n",
    "        ans_end_index_p = ans_end_index_l\n",
    "\n",
    "    pred_cot_tokens = pred_tokens[:ans_start_index_p]\n",
    "    lab_cot_tokens = lab_tokens[:ans_start_index_l]\n",
    "\n",
    "    all_preds_cot.append(pred_cot_tokens)\n",
    "    all_labels_cot.append(lab_cot_tokens)\n",
    "\n",
    "    pred_and_tokens = pred_tokens[ans_start_index_p+1:ans_end_index_p]\n",
    "    lab_ans_tokens = lab_tokens[ans_start_index_l+1:ans_end_index_l]\n",
    "\n",
    "    all_preds_ans.append(pred_and_tokens)\n",
    "    all_labels_ans.append(lab_ans_tokens)\n",
    "\n",
    "    all_preds.append(pred_tokens)\n",
    "    all_labels.append(lab_tokens)\n",
    "\n",
    "cot_correct = [p == l for p, l in zip(all_preds_cot, all_labels_cot)]\n",
    "ans_correct = [p == l for p, l in zip(all_preds_ans, all_labels_ans)]\n",
    "\n",
    "res = {'accuracy_cot': np.mean(cot_correct), 'accuracy_ans': np.mean(ans_correct)}\n",
    "data = {\"all_preds_cot\": all_preds_cot,\n",
    "        \"all_labels_cot\": all_labels_cot,\n",
    "        \"all_preds_ans\": all_preds_ans,\n",
    "        \"all_labels_ans\": all_labels_ans,\n",
    "        \"all_preds\": all_preds,\n",
    "        \"all_labels\": all_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5*2=3<issue_start><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>3+2.5=5.5<issue_start><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>5.5+1.5=7<issue_closed><|endoftext|><|endoftext|><|endoftext|>7<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>7<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(preds_full[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<21/7=3>> <<3*5=15>><issue_closed>15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy_cot': np.float64(0.25), 'accuracy_ans': np.float64(0.75)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"all_labels_cot\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"all_labels_ans\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.5'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"all_preds_ans\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred <<21/7=3>> <<3*5=15>><issue_closed>15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Lab <<21/7=3>> <<5*3=15>><issue_closed>15<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred\", tokenizer.decode(data[\"all_preds\"][3]))\n",
    "print(\"Lab\", tokenizer.decode(data[\"all_labels\"][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [33, 32, 30, 37], [], []]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[35, 32, 32]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_correct = [p == l for p, l in zip(all_preds_ans, all_labels_ans)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
