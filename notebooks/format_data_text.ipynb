{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace-SR006.nfs2/Bulatov_A/env_main/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    split_pattern = '####'\n",
    "    if split_pattern not in text:\n",
    "        return text.strip().replace(',', '')\n",
    "    else:\n",
    "        _, ans = text.strip().split('####', 1)\n",
    "        ans = '####' + ans\n",
    "        ans = ans.strip().replace(',', '')\n",
    "        return ans\n",
    "\n",
    "def extract_cot(text):\n",
    "    split_pattern = '####'\n",
    "    if split_pattern not in text:\n",
    "        return None\n",
    "    else:\n",
    "        cot, _ = text.strip().split('####', 1)\n",
    "        cot = cot.strip()\n",
    "        return cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def get_hf_dataset(file_path):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        lines = [line.split('||') for line in f.read().splitlines() if (len(line) > 0 and not line.isspace()\n",
    "                                                                            and len(line.split('||')) ==2 )]\n",
    "    src_lines, tgt_lines = list(zip(*lines))\n",
    "    src_lines = list(src_lines)\n",
    "    tgt_lines = list(tgt_lines)\n",
    "\n",
    "    srcs, tgts, cots = [], [], []\n",
    "    for src, tgt in zip(src_lines, tgt_lines):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        ans = extract_answer(tgt)[5:]\n",
    "        cot = extract_cot(tgt)\n",
    "        tgt = tgt.split('#### ')[1]\n",
    "\n",
    "        srcs.append(src)\n",
    "        tgts.append(tgt)\n",
    "        cots.append(cot)\n",
    "    ds = Dataset.from_dict(dict(task=srcs, labels=tgts, cot=cots))\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/4_by_4_mult/train.txt'\n",
    "val_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/4_by_4_mult/valid.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = get_hf_dataset(train_path)\n",
    "ds_val = get_hf_dataset(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 808/808 [00:01<00:00, 654.62ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.31s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 567.95ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/booydar/multiplication_4x4/commit/e224243bb9970a0937976173287d24816d683ff9', commit_message='Upload dataset', commit_description='', oid='e224243bb9970a0937976173287d24816d683ff9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/booydar/multiplication_4x4', endpoint='https://huggingface.co', repo_type='dataset', repo_id='booydar/multiplication_4x4'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset = datasets.DatasetDict(train=ds_train, valid=ds_val)\n",
    "hf_dataset.push_to_hub('booydar/multiplication_4x4', token=\"your_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 808/808 [00:01<00:00, 694.47ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:05<00:00,  5.30s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 591.08ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/booydar/multiplication_5x5/commit/0460ea5f5468419ce40dd642de25a3a7b896daaa', commit_message='Upload dataset', commit_description='', oid='0460ea5f5468419ce40dd642de25a3a7b896daaa', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/booydar/multiplication_5x5', endpoint='https://huggingface.co', repo_type='dataset', repo_id='booydar/multiplication_5x5'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/5_by_5_mult/train.txt'\n",
    "val_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/5_by_5_mult/valid.txt'\n",
    "\n",
    "ds_train = get_hf_dataset(train_path)\n",
    "ds_val = get_hf_dataset(val_path)\n",
    "\n",
    "hf_dataset = datasets.DatasetDict(train=ds_train, valid=ds_val)\n",
    "hf_dataset.push_to_hub('booydar/multiplication_5x5', token=\"your_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 385/385 [00:00<00:00, 415.34ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.26s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 623.41ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 643.94ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 369.91ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/booydar/gsm8k/commit/71eb04c3261d8da50bae2dccdec5d70dace5d07d', commit_message='Upload dataset', commit_description='', oid='71eb04c3261d8da50bae2dccdec5d70dace5d07d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/booydar/gsm8k', endpoint='https://huggingface.co', repo_type='dataset', repo_id='booydar/gsm8k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/gsm8k/train.txt'\n",
    "val_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/gsm8k/valid.txt'\n",
    "test_path = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/gsm8k/test.txt'\n",
    "train_no_aug = '/workspace-SR006.nfs2/Bulatov_A/rmt/tools/implicit_chain_of_thought/data/gsm8k/train_no_aug.txt'\n",
    "\n",
    "ds_train = get_hf_dataset(train_path)\n",
    "ds_val = get_hf_dataset(val_path)\n",
    "ds_test = get_hf_dataset(test_path)\n",
    "ds_train_no_aug = get_hf_dataset(train_no_aug)\n",
    "\n",
    "hf_dataset = datasets.DatasetDict(train=ds_train, valid=ds_val, test=ds_test, train_no_aug=ds_train_no_aug)\n",
    "hf_dataset.push_to_hub('booydar/gsm8k', token=\"your_token_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'Jen shared a pack of chocolates among her friends. She gave 20% to Lucy, 30% to Sarah and the remaining were shared equally among four others. If the pack contained 100 chocolates, how many chocolates were each of the four others getting?',\n",
       " 'labels': '12.5',\n",
       " 'cot': '<<20+30=50>> <<100-50=50>> <<100*50/100=50>> <<50/4=12.5>>'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = datasets.load_dataset(\"booydar/multiplication_4x4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Holder()\n",
    "args.use_cot = False\n",
    "args.num_mem_tokens = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "# id_pad_value = -100\n",
    "\n",
    "\n",
    "if args.use_cot in (False, None):\n",
    "    inputs_key = 'examples_nocot'\n",
    "    labels_key = 'labels_nocot'\n",
    "else:\n",
    "    inputs_key = 'examples_all'\n",
    "    labels_key = 'labels_all'\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(b[inputs_key]) for b in batch]\n",
    "    labels = [torch.tensor(b[labels_key]) for b in batch]\n",
    "    attention_mask = [torch.ones_like(b, dtype=int) for b in input_ids]\n",
    "    # labels_mask defines which input_ids participate in loss calculation\n",
    "    labels_mask = [torch.sign(torch.tensor(b[labels_key])) for b in batch]\n",
    "\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "    labels = pad_sequence(labels, padding_value=id_pad_value, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'attention_mask': attention_mask,\n",
    "                }\n",
    "    if args.num_mem_tokens is not None:\n",
    "        # add labels mask only for RMT, ARMT\n",
    "        collated['labels_mask'] = labels_mask.bool()\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = hf_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [train_dataset[i] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': '1 3 3 8 * 5 1 0 5',\n",
       " 'labels': '5 6 9 9 7 7 1 4',\n",
       " 'cot': '5 5 6 1 4 + 0 1 3 3 8 0 ( 5 6 9 4 2 1 ) + 0 0 0 0 0 0 0 ( 5 6 9 4 2 1 0 ) + 0 0 0 5 5 6 1 4'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "think = tokenizer.bos_token_id\n",
    "ans = tokenizer.bos_token_id\n",
    "eos = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_mem_tokens = 10\n",
    "args.use_cot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "think = tokenizer.bos_token_id\n",
    "ans = tokenizer.bos_token_id\n",
    "eos = tokenizer.eos_token_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, labels, labels_mask, attention_mask = [], [], [], []\n",
    "    for sample in batch:\n",
    "        task, lab, cot = sample['task'], sample['labels'], sample['cot']\n",
    "        task_tokens = tokenizer.encode(task, add_special_tokens=False)\n",
    "        labels_tokens = tokenizer.encode(lab, add_special_tokens=False)\n",
    "        cot_tokens = tokenizer.encode(cot, add_special_tokens=False)\n",
    "\n",
    "        if args.use_cot:\n",
    "            full_input = task_tokens + [think] + cot_tokens + [ans] + labels_tokens + [eos]\n",
    "        else:\n",
    "            full_input = task_tokens + [ans] + labels_tokens + [eos]\n",
    "        inp_ids = torch.tensor(full_input)\n",
    "        input_ids.append(inp_ids)\n",
    "\n",
    "        lab = torch.tensor(full_input)\n",
    "        lab[:len(task_tokens)] = -100\n",
    "        labels.append(lab)\n",
    "\n",
    "        lab_mask = torch.ones_like(inp_ids)\n",
    "        lab_mask[:len(task_tokens)] = 0\n",
    "        labels_mask.append(lab_mask)\n",
    "        attention_mask.append(torch.ones_like(inp_ids))\n",
    "        \n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "    labels = pad_sequence(labels, padding_value=id_pad_value, batch_first=True)\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'attention_mask': attention_mask,\n",
    "                }\n",
    "    if args.num_mem_tokens is not None:\n",
    "        # add labels mask only for RMT, ARMT\n",
    "        collated['labels_mask'] = labels_mask.bool()\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "collated = collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 3 3 8 * 5 1 0 5<|endoftext|>5 5 6 1 4 + 0 1 3 3 8 0 ( 5 6 9 4 2 1 ) + 0 0 0 0 0 0 0 ( 5 6 9 4 2 1 0 ) + 0 0 0 5 5 6 1 4<|endoftext|>5 6 9 9 7 7 1 4<|endoftext|>'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(collated['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>5 5 6 1 4 + 0 1 3 3 8 0 ( 5 6 9 4 2 1 ) + 0 0 0 0 0 0 0 ( 5 6 9 4 2 1 0 ) + 0 0 0 5 5 6 1 4<|endoftext|>5 6 9 9 7 7 1 4<|endoftext|>'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(collated['input_ids'][0][collated['labels_mask'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>5 5 6 1 4 + 0 1 3 3 8 0 ( 5 6 9 4 2 1 ) + 0 0 0 0 0 0 0 ( 5 6 9 4 2 1 0 ) + 0 0 0 5 5 6 1 4<|endoftext|>5 6 9 9 7 7 1 4<|endoftext|>'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(collated['labels'][0][collated['labels_mask'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "think_text = tokenizer.decode(think)\n",
    "ans_text = tokenizer.decode(ans)\n",
    "\n",
    "def extract_cot(text):\n",
    "    try:\n",
    "        start_index = text.index(think_text)\n",
    "        end_index = text.index(ans_text, start_index + len(think_text))\n",
    "        return text[start_index + len(think_text):end_index]\n",
    "    except ValueError as e:\n",
    "        return ''\n",
    "\n",
    "def extract_answer(text):\n",
    "    return text.split(ans_text)[-2]\n",
    "        \n",
    "def compute_accuracy(eval_pred):\n",
    "    preds = eval_pred.predictions.argmax(axis=-1)[:, :-1]\n",
    "    labels = eval_pred.label_ids[:, 1:]\n",
    "    print(\"preds.shape, labels.shape\")\n",
    "    print(preds.shape, labels.shape)\n",
    "\n",
    "    labels_masks = labels > 0\n",
    "    preds_full = [p[m] for p, m in zip(preds, labels_masks)]\n",
    "    labels_full = [l[m] for l, m in zip(labels, labels_masks)]\n",
    "\n",
    "    print(len(preds_full), len(labels_full))\n",
    "    # print(preds_full, labels_full)\n",
    "\n",
    "    preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "    labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "    preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "    preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "    labels_cot = [extract_cot(l) for l in labels_full_text]\n",
    "    labels_ans = [extract_answer(l) for l in labels_full_text]\n",
    "    \n",
    "    acc_cot = np.mean([c == l for c, l in zip(preds_cot, labels_cot)])\n",
    "    acc_ans = np.mean([c == l for c, l in zip(preds_ans, labels_ans)])\n",
    "\n",
    "    return {'accuracy_cot': acc_cot, 'accuracy_ans': acc_ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = collated['labels']\n",
    "preds = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds.shape, labels.shape\n",
      "torch.Size([10, 66]) torch.Size([10, 66])\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "# preds = eval_pred.predictions.argmax(axis=-1)[:, :-1]\n",
    "# labels = eval_pred.label_ids[:, 1:]\n",
    "print(\"preds.shape, labels.shape\")\n",
    "print(preds.shape, labels.shape)\n",
    "\n",
    "labels_masks = labels > 0\n",
    "preds_full = [p[m] for p, m in zip(preds, labels_masks)]\n",
    "labels_full = [l[m] for l, m in zip(labels, labels_masks)]\n",
    "\n",
    "print(len(preds_full), len(labels_full))\n",
    "# print(preds_full, labels_full)\n",
    "\n",
    "preds_full_text = tokenizer.batch_decode(preds_full, add_special_tokens=True)\n",
    "labels_full_text = tokenizer.batch_decode(labels_full, add_special_tokens=True)\n",
    "\n",
    "preds_cot = [extract_cot(p) for p in preds_full_text]\n",
    "preds_ans = [extract_answer(p) for p in preds_full_text]\n",
    "\n",
    "labels_cot = [extract_cot(l) for l in labels_full_text]\n",
    "labels_ans = [extract_answer(l) for l in labels_full_text]\n",
    "\n",
    "acc_cot = np.mean([c == l for c, l in zip(preds_cot, labels_cot)])\n",
    "acc_ans = np.mean([c == l for c, l in zip(preds_ans, labels_ans)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5 6 9 9 7 7 1 4',\n",
       " '0 4 1 8 0 0 7 7',\n",
       " '6 7 4 8 1 3 4 5',\n",
       " '2 1 7 5 3 3 8 0',\n",
       " '2 1 9 4 0 4 0 1',\n",
       " '0 1 3 0 3 7 5 1',\n",
       " '8 6 7 8 0 7 9 0',\n",
       " '0 2 5 4 2 2 5 1',\n",
       " '0 0 6 2 1 6 6 0',\n",
       " '6 1 3 9 0 3 2 3']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|endoftext|>5 5 6 1 4 + 0 1 3 3 8 0 ( 5 6 9 4 2 1 ) + 0 0 0 0 0 0 0 ( 5 6 9 4 2 1 0 ) + 0 0 0 5 5 6 1 4',\n",
       " '|endoftext|>0 0 0 0 0 + 0 4 4 2 5 3 ( 0 4 4 2 5 3 ) + 0 0 7 7 6 1 6 ( 0 4 1 0 2 5 6 ) + 0 0 0 8 8 4 0 7',\n",
       " '|endoftext|>6 9 9 3 1 + 0 8 8 9 1 4 ( 6 7 8 3 3 4 ) + 0 0 6 8 9 8 4 ( 6 7 4 2 3 3 5 ) + 0 0 0 6 8 9 8 4',\n",
       " '|endoftext|>2 3 4 3 3 + 0 8 4 1 0 5 ( 2 1 9 4 3 5 ) + 0 0 8 8 2 2 2 ( 2 1 7 3 6 7 2 ) + 0 0 0 2 7 5 5 0',\n",
       " '|endoftext|>2 1 9 4 2 + 0 0 0 0 0 0 ( 2 1 9 4 2 0 ) + 0 0 0 6 7 0 2 ( 2 1 9 0 0 1 2 ) + 0 0 0 4 0 3 8 0',\n",
       " '|endoftext|>0 6 3 0 4 + 0 5 4 0 5 0 ( 0 1 8 0 9 0 ) + 0 0 5 4 0 5 0 ( 0 1 3 5 9 5 0 ) + 0 0 0 5 3 1 5 1',\n",
       " '|endoftext|>8 9 8 1 1 + 0 7 4 8 7 1 ( 8 6 3 0 9 1 ) + 0 0 4 6 8 5 1 ( 8 6 7 6 7 7 1 ) + 0 0 0 2 3 9 7 0',\n",
       " '|endoftext|>0 0 0 0 0 + 0 2 7 8 3 1 ( 0 2 7 8 3 1 ) + 0 0 8 3 1 2 1 ( 0 2 5 2 5 3 1 ) + 0 0 0 2 7 8 3 1',\n",
       " '|endoftext|>0 4 8 2 1 + 0 6 7 9 7 1 ( 0 0 6 2 9 1 ) + 0 0 0 4 8 2 1 ( 0 0 6 6 7 4 1 ) + 0 0 0 6 3 1 5 0',\n",
       " '|endoftext|>6 8 0 0 4 + 0 3 4 0 0 2 ( 6 1 5 0 4 2 ) + 0 0 8 4 4 3 5 ( 6 1 3 5 8 5 5 ) + 0 0 0 4 2 7 6 2']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_cot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "think = tokenizer.bos_token_id\n",
    "ans = tokenizer.bos_token_id\n",
    "eos = tokenizer.eos_token_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, labels, labels_mask, attention_mask = [], [], [], []\n",
    "    for sample in batch:\n",
    "        task, lab, cot = sample['task'], sample['labels'], sample['cot']\n",
    "        task_tokens = tokenizer.encode(task, add_special_tokens=False)\n",
    "        labels_tokens = tokenizer.encode(lab, add_special_tokens=False)\n",
    "        cot_tokens = tokenizer.encode(cot, add_special_tokens=False)\n",
    "\n",
    "        if args.use_cot:\n",
    "            full_input = task_tokens + [think] + cot_tokens + [ans] + labels_tokens + [eos]\n",
    "        else:\n",
    "            full_input = task_tokens + [ans] + labels_tokens + [eos]\n",
    "        inp_ids = torch.tensor(full_input)\n",
    "        input_ids.append(inp_ids)\n",
    "\n",
    "        lab = torch.tensor(full_input)\n",
    "        lab[:len(task_tokens)] = -100\n",
    "        labels.append(lab)\n",
    "\n",
    "        lab_mask = torch.ones_like(inp_ids)\n",
    "        lab_mask[:len(task_tokens)] = 0\n",
    "        labels_mask.append(lab_mask)\n",
    "        attention_mask.append(torch.ones_like(inp_ids))\n",
    "        \n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "    labels = pad_sequence(labels, padding_value=id_pad_value, batch_first=True)\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'attention_mask': attention_mask,\n",
    "                }\n",
    "    if args.num_mem_tokens is not None:\n",
    "        # add labels mask only for RMT, ARMT\n",
    "        collated['labels_mask'] = labels_mask.bool()\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
