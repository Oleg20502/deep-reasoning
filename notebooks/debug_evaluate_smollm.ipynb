{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user33/.conda/envs/rmt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.reasoning import make_segment, split_cot\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "checkpoint_path = \"/home/user33/kashurin/TR_SmolLM2-135M/cot/checkpoint-1500/pytorch_model.bin\"\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "\n",
    "model.to(device)\n",
    "print(':)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "bos = [tokenizer.bos_token_id]\n",
    "eos = [tokenizer.eos_token_id]\n",
    "think = tokenizer.encode(\"<issue_start>\")\n",
    "ans = tokenizer.encode(\"<issue_closed>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids, labels, labels_mask, attention_mask = [], [], [], []\n",
    "    for sample in batch:\n",
    "        task, lab, cot = sample['task'], sample['labels'], sample['cot']\n",
    "        task_tokens = tokenizer.encode(task, add_special_tokens=False)\n",
    "        labels_tokens = tokenizer.encode(lab, add_special_tokens=False)\n",
    "        cot_tokens = tokenizer.encode(cot, add_special_tokens=False)\n",
    "\n",
    "        inp_ids = torch.tensor(task_tokens + think)\n",
    "        input_ids.append(inp_ids)\n",
    "\n",
    "        full_input = task_tokens + think + cot_tokens + ans + labels_tokens + eos\n",
    "        lab = torch.tensor(full_input)\n",
    "        lab[:inp_ids.shape[0]] = -100\n",
    "        labels.append(lab)\n",
    "\n",
    "        lab_mask = torch.ones_like(lab)\n",
    "        lab_mask[:inp_ids.shape[0]] = 0\n",
    "        labels_mask.append(lab_mask)\n",
    "        attention_mask.append(torch.ones_like(inp_ids))\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=pad, batch_first=True, padding_side='left')\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True, padding_side='left')\n",
    "    labels = pad_sequence(labels, padding_value=-100, batch_first=True, padding_side='left')\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True, padding_side='left')\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels,\n",
    "                'attention_mask': attention_mask,\n",
    "                }\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'booydar/gsm8k'\n",
    "train_dataset = datasets.load_dataset(dataset, split='train')\n",
    "valid_dataset = datasets.load_dataset(dataset, split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = Holder()\n",
    "# args.use_cot = False\n",
    "args.max_new_tokens = 200\n",
    "args.task_name = 'gsm8k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The future of AI is\",\n",
    "    \"In a galaxy far far away\",\n",
    "    \"To solve this problem we need\",\n",
    "    \"Hello\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[504, 1774, 282, 5646, 314], [788, 253, 13247, 1869, 1869, 2025], [2068, 5482, 451, 1732, 392, 737], [19556]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1]]}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(prompts, padding_side='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  504,  1774,   282,  5646,   314,     0],\n",
       "        [  788,   253, 13247,  1869,  1869,  2025],\n",
       "        [ 2068,  5482,   451,  1732,   392,   737],\n",
       "        [19556,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|><|endoftext|>The future of AI is'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "all_preds, all_labels = [], []\n",
    "all_preds_cot, all_labels_cot = [], []\n",
    "all_preds_ans, all_labels_ans = [], []\n",
    "\n",
    "batch = valid_dataset.select(range(4))\n",
    "collated = collate_fn(batch)\n",
    "task = {k:v.to(device) for k,v in collated.items()}\n",
    "\n",
    "task_length = collated['input_ids'].shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_full = model.generate(**task, max_new_tokens=args.max_new_tokens)\n",
    "\n",
    "labels = collated['labels']\n",
    "for i, (lab_tokens, pred_tokens) in enumerate(zip(labels, preds_full)):\n",
    "    labels_mask = lab_tokens != -100\n",
    "    lab_tokens = lab_tokens[labels_mask].tolist()\n",
    "\n",
    "    pred_tokens = pred_tokens[task_length:].tolist()\n",
    "    \n",
    "    ans_start_index_l = max(i for i, x in enumerate(lab_tokens) if x == ans[0])\n",
    "    ans_end_index_l = min(i for i, x in enumerate(lab_tokens) if x == eos[0])\n",
    "\n",
    "    if ans[0] in pred_tokens:\n",
    "        ans_start_index_p = max(i for i, x in enumerate(pred_tokens) if x == ans[0])\n",
    "    else:\n",
    "        ans_start_index_p = ans_start_index_l\n",
    "\n",
    "    if eos[0] in pred_tokens:\n",
    "        ans_end_index_p = min(i for i, x in enumerate(pred_tokens) if x == eos[0])\n",
    "    else:\n",
    "        ans_end_index_p = ans_end_index_l\n",
    "\n",
    "    pred_cot_tokens = pred_tokens[:ans_start_index_p]\n",
    "    lab_cot_tokens = lab_tokens[:ans_start_index_l]\n",
    "\n",
    "    all_preds_cot.append(pred_cot_tokens)\n",
    "    all_labels_cot.append(lab_cot_tokens)\n",
    "\n",
    "    pred_and_tokens = pred_tokens[ans_start_index_p+1:ans_end_index_p]\n",
    "    lab_ans_tokens = lab_tokens[ans_start_index_l+1:ans_end_index_l]\n",
    "\n",
    "    all_preds_ans.append(pred_and_tokens)\n",
    "    all_labels_ans.append(lab_ans_tokens)\n",
    "\n",
    "    all_preds.append(pred_tokens)\n",
    "    all_labels.append(lab_tokens)\n",
    "\n",
    "cot_correct = [p == l for p, l in zip(all_preds_cot, all_labels_cot)]\n",
    "ans_correct = [p == l for p, l in zip(all_preds_ans, all_labels_ans)]\n",
    "\n",
    "res = {'accuracy_cot': np.mean(cot_correct), 'accuracy_ans': np.mean(ans_correct)}\n",
    "data = {\"all_preds_cot\": all_preds_cot,\n",
    "        \"all_labels_cot\": all_labels_cot,\n",
    "        \"all_preds_ans\": all_preds_ans,\n",
    "        \"all_labels_ans\": all_labels_ans,\n",
    "        \"all_preds\": all_preds,\n",
    "        \"all_labels\": all_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hannah has three dogs. The first dog eats 1.5 cups of dog food a day. The second dog eats twice as much while the third dog eats 2.5 cups more than the second dog. How many cups of dog food should Hannah prepare in a day for her three dogs?<issue_start><<1.5*2=3>> <<3+3=6>> <<1.5+3+6=10.5>><issue_closed>10.5<|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(preds_full[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<21/7=3>> <<3*5=15>><issue_closed>15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy_cot': np.float64(0.25), 'accuracy_ans': np.float64(0.75)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<4-2=2>> <<2/.5=4>> <<12/4=3>> <<100*3=300>>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"all_labels_cot\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"all_labels_ans\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.5'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"all_preds_ans\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred <<21/7=3>> <<3*5=15>><issue_closed>15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Lab <<21/7=3>> <<5*3=15>><issue_closed>15<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred\", tokenizer.decode(data[\"all_preds\"][3]))\n",
    "print(\"Lab\", tokenizer.decode(data[\"all_labels\"][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [33, 32, 30, 37], [], []]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[35, 32, 32]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_correct = [p == l for p, l in zip(all_preds_ans, all_labels_ans)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
