{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fa097-c74b-457c-9b9a-6dcb7dc691e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import EarlyStoppingCallback, set_seed\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import AdamW\n",
    "# from lm_experiments_tools.dataset_preprocessing import load_and_preprocess_task, combine_datasets\n",
    "# from lm_experiments_tools.instruction_utils import mask_non_completion, mask_non_completion_multi\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import accelerate\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# import transformers  # noqa: E402\n",
    "from transformers import AutoConfig, AutoTokenizer, HfArgumentParser  # noqa: E402\n",
    "\n",
    "# from lm_experiments_tools.utils import get_cls_by_name, get_optimizer, prepare_run  # noqa: E402\n",
    "from lm_experiments_tools.utils import get_cls_by_name\n",
    "\n",
    "from utils.reasoning import make_segment, split_cot\n",
    "\n",
    "logger_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(format=logger_fmt, level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "\n",
    "# if CUDA_VISIBLE_DEVICES is not set make all gpus visible\n",
    "if os.environ.get('CUDA_VISIBLE_DEVICES', None) is None:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(i) for i in range(torch.cuda.device_count())])\n",
    "\n",
    "logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "# first call to torch.cuda.device_count() sets visible gpus, following calls will not change the result\n",
    "logger.info(f\"CUDA DEVICE COUNT: {torch.cuda.device_count()}\")\n",
    "\n",
    "\n",
    "    # set current working dir\n",
    "    args.working_dir = str(Path(args.working_dir).expanduser().absolute())\n",
    "    os.chdir(args.working_dir)\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # workaround with setting bigger tiomeout for NCCL (useful for big dataset, to avoid timeout at tokenization)\n",
    "    timeout = timedelta(seconds=20 * 1800)\n",
    "    accelerator = accelerate.Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "                                         kwargs_handlers=[accelerate.InitProcessGroupKwargs(timeout=timeout)])\n",
    "    from accelerate.logging import get_logger\n",
    "    logger = get_logger('')\n",
    "\n",
    "    logger.info(f'num processes: {accelerator.num_processes}')\n",
    "    logger.info(f'mixed precision: {accelerator.mixed_precision}')\n",
    "\n",
    "    if args.output_dir is None:\n",
    "        logger.warning('output_dir is not set: config, logs and checkpoints will not be saved.')\n",
    "\n",
    "    # ============================\n",
    "    # === Prepare tokenizer and datasets\n",
    "    # ============================\n",
    "    if not args.from_pretrained:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.from_pretrained)\n",
    "    if args.tokenizer_for_chat_template is not None:\n",
    "        it_tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_for_chat_template, trust_remote_code=True)\n",
    "        tokenizer.chat_template = it_tokenizer.chat_template\n",
    "    if args.padding_side is not None:\n",
    "        tokenizer.padding_side = args.padding_side\n",
    "    # Prepare datasets\n",
    "    logger.info(f'preparing dataset for {args.task_name}')\n",
    "\n",
    "    if args.dataset_name is not None:\n",
    "        hf_dataset = datasets.load_dataset(args.dataset_name)\n",
    "        train_dataset = hf_dataset[\"train\"]\n",
    "        valid_dataset = hf_dataset[\"valid\"]\n",
    "        if \"test\" in hf_dataset:\n",
    "            test_dataset = hf_dataset[\"test\"]\n",
    "        else:\n",
    "            test_dataset = None\n",
    "    else:\n",
    "        dataset_path = os.path.join(args.dataset_dir, args.task_name)\n",
    "\n",
    "        train_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"train\"))\n",
    "        valid_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"valid\"))\n",
    "        if os.path.exists(os.path.join(dataset_path, \"test\")):\n",
    "            test_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"test\"))\n",
    "        else:\n",
    "            test_dataset = datasets.load_from_disk(os.path.join(dataset_path, \"valid\"))\n",
    "\n",
    "    if args.max_cot_steps is not None:\n",
    "        train_dataset = train_dataset.filter(lambda x: x['cot_len'] <= args.max_cot_steps)\n",
    "        valid_dataset = valid_dataset.filter(lambda x: x['cot_len'] <= args.max_cot_steps)\n",
    "        test_dataset = test_dataset.filter(lambda x: x['cot_len'] <= args.max_cot_steps)\n",
    "        logger.info(f\"Filtered ds sizes: {len(train_dataset), len(valid_dataset), len(test_dataset)}\")\n",
    "    if 'gsm8k' in args.task_name:\n",
    "        delim = \">> <<\"\n",
    "    elif 'multiplication' in args.task_name:\n",
    "        delim = ' + '\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown task name {args.task_name}\")\n",
    "\n",
    "    id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "    think = tokenizer.encode('????')\n",
    "    bos = tokenizer.encode('////')\n",
    "    ans = tokenizer.encode('!!!!')\n",
    "    eos = [tokenizer.eos_token_id]\n",
    "    if 'gsm8k' in args.task_name:\n",
    "        delim = \">> <<\"\n",
    "    elif 'multiplication' in args.task_name:\n",
    "        delim = ' + '\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown task name {args.task_name}\")\n",
    "\n",
    "\n",
    "    # ============================\n",
    "    # === Prepare data collator ===\n",
    "    # ============================\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        # first, we segment each sample into task, cot steps and labels\n",
    "        segments_batch = []\n",
    "        for sample in batch:\n",
    "            task, lab, cot = sample['task'], sample['labels'], sample['cot']\n",
    "            task_tokens = tokenizer.encode(task, add_special_tokens=False)\n",
    "            labels_tokens = tokenizer.encode(lab, add_special_tokens=False)\n",
    "            if getattr(args, 'use_cot', False):\n",
    "                cot_segments = split_cot(cot, by=delim)\n",
    "            else:\n",
    "                cot_segments = [cot]\n",
    "            cot_segment_tokens = tokenizer.batch_encode_plus(cot_segments, add_special_tokens=False)['input_ids']\n",
    "\n",
    "            segments = []\n",
    "            segments.append(make_segment(bos + task_tokens + think, loss=False))\n",
    "            for segment in cot_segment_tokens[:-1]:\n",
    "                segments.append(make_segment(bos + segment + think, loss=True))\n",
    "            segments.append(make_segment(bos + cot_segment_tokens[-1] + ans, loss=True))\n",
    "\n",
    "            segments.append(make_segment(bos + labels_tokens + eos, loss=True))\n",
    "            segments_batch.append(segments)\n",
    "\n",
    "        # if some samples have less segments than others, we pad them with empty segments\n",
    "        num_segments = max(len(segments) for segments in segments_batch)\n",
    "        for segments in segments_batch:\n",
    "            if len(segments) < num_segments:\n",
    "                segments.extend([make_segment(eos, loss=False)] * (num_segments - len(segments)))\n",
    "\n",
    "        # prepare segments for the whole batch\n",
    "        batch_segments = []\n",
    "        for i in range(num_segments):\n",
    "            input_ids = [s[i]['input_ids'] for s in segments_batch]\n",
    "            attention_mask = [s[i]['attention_mask'] for s in segments_batch]\n",
    "            labels = [s[i]['labels'] for s in segments_batch]\n",
    "            labels_mask = [s[i]['labels_mask'] for s in segments_batch]\n",
    "\n",
    "            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=id_pad_value)\n",
    "            attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "            labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "            labels_mask = pad_sequence(labels_mask, batch_first=True, padding_value=False)\n",
    "\n",
    "            batch_segment = {'input_ids': input_ids,\n",
    "                             'attention_mask': attention_mask,\n",
    "                             'labels_mask': labels_mask,\n",
    "                             'labels': labels\n",
    "                             }\n",
    "            batch_segments.append(batch_segment)\n",
    "        full_labels = torch.cat([s['labels'] for s in batch_segments], dim=1)\n",
    "        return {\"segments\": batch_segments, 'labels': full_labels}\n",
    "\n",
    "\n",
    "    # ============================\n",
    "    # === Define model ==========\n",
    "    # ============================\n",
    "    # TODO: move model building to separate function\n",
    "    model_cls = get_cls_by_name(args.model_cls)\n",
    "    logger.info(f'Using model class: {model_cls}')\n",
    "\n",
    "    if args.use_adapter:\n",
    "        model_cfg = AutoConfig.from_pretrained(args.from_pretrained)\n",
    "\n",
    "        model_cfg.use_parallel_adapter = args.use_adapter\n",
    "        model_cfg.parallel_adapter_mode = 'ffn'\n",
    "        model_cfg.adapter_bottleneck_dim = args.adapter_bottleneck_dim\n",
    "        model_cfg.adapter_dropout = args.adapter_dropout\n",
    "        model_cfg.adapter_scale = args.adapter_scale\n",
    "\n",
    "        model = model_cls(config=model_cfg)\n",
    "\n",
    "        logger.info(f'Loading pretrained model: {args.from_pretrained}')\n",
    "        base_model = model_cls.from_pretrained(args.from_pretrained, use_safetensors=False)\n",
    "\n",
    "        model.load_state_dict(base_model.state_dict(), strict=False)\n",
    "        del base_model\n",
    "        logger.info('Added adapters')\n",
    "    else:\n",
    "        # TODO: fix if for Qwen and Llama\n",
    "        if not args.from_pretrained:\n",
    "            model_cfg = AutoConfig.from_pretrained(args.model_cfg)\n",
    "            model = model_cls.from_config(model_cfg)\n",
    "        else:\n",
    "            logger.info(f'Loading pretrained model: {args.from_pretrained}')\n",
    "            if \"Qwen\" in args.from_pretrained or \"Llama\" in args.from_pretrained:\n",
    "                model = model_cls.from_pretrained(args.from_pretrained,\n",
    "                                                  attn_implementation=\"flash_attention_2\",\n",
    "                                                  torch_dtype=torch.bfloat16,\n",
    "                                                  trust_remote_code=True)\n",
    "            else:\n",
    "                model = model_cls.from_pretrained(args.from_pretrained)\n",
    "\n",
    "    # add LoRA adapters\n",
    "    if args.use_lora:\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=args.lora_attn_dim,\n",
    "            lora_alpha=args.lora_attn_alpha,\n",
    "            lora_dropout=args.lora_dropout\n",
    "            )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        logger.info('Added LoRA, trainable parameters with LoRA only:')\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "    # load cpt of backbone model\n",
    "    if args.backbone_cpt:\n",
    "        if 'bin' in args.backbone_cpt:\n",
    "            backbone_cpt = args.backbone_cpt\n",
    "        else:\n",
    "            backbone_cpt = os.path.join(args.backbone_cpt, \"model_best.pth\")\n",
    "        cpt = torch.load(backbone_cpt, map_location='cpu')\n",
    "        model.load_state_dict(cpt['model_state_dict'], strict=True)\n",
    "        logger.info(f'Loaded baseline state dict from: {args.backbone_cpt}')\n",
    "    \n",
    "    # Pass memory settings to pretrained model\n",
    "    if args.num_mem_tokens is not None:\n",
    "        memory_cell_cls = get_cls_by_name(args.memory_cell_cls)\n",
    "        recurrent_wrapper_cls = get_cls_by_name(args.recurrent_wrapper_cls)\n",
    "        logger.info(f'Wrapping in: {memory_cell_cls} and {recurrent_wrapper_cls}')\n",
    "        mem_cell_args = dict(\n",
    "            base_model=model,\n",
    "            num_mem_tokens=args.num_mem_tokens,\n",
    "        )\n",
    "        # additional parameters for ARMT model\n",
    "        if args.d_mem is not None:\n",
    "            mem_cell_args['d_mem'] = args.d_mem\n",
    "            mem_cell_args['wrap_pos'] = args.wrap_pos\n",
    "            mem_cell_args['correction'] = not args.no_correction\n",
    "            # mem_cell_args['use_lora'] = args.use_lora\n",
    "        if args.layers_attr is not None:\n",
    "            mem_cell_args['layers_attr'] = args.layers_attr\n",
    "        if args.attend_to_previous_input:\n",
    "            mem_cell_args['attend_to_previous_input'] = args.attend_to_previous_input\n",
    "        \n",
    "        cell = memory_cell_cls(**mem_cell_args)\n",
    "        model = recurrent_wrapper_cls(\n",
    "            cell,\n",
    "            segment_size=args.segment_size,\n",
    "            max_n_segments=args.max_n_segments,\n",
    "            vary_n_segments=args.vary_n_segments,\n",
    "            k2=args.k2,\n",
    "            attend_to_previous_input=args.attend_to_previous_input,\n",
    "            return_all_logits=False,\n",
    "            answer_loss_weight=args.answer_loss_weight\n",
    "        )\n",
    "        \n",
    "        # load cpt of rmt\n",
    "        if args.model_cpt:\n",
    "            if \"safetensors\" in args.model_cpt:\n",
    "                print(model)\n",
    "                from safetensors.torch import load_model\n",
    "                load_model(model, args.model_cpt, device=\"cuda:0\")\n",
    "            else:\n",
    "                if \".bin\" in args.model_cpt:\n",
    "                    model_cpt = args.model_cpt\n",
    "                elif \"model_best\" in os.listdir(args.model_cpt):\n",
    "                    model_cpt = os.path.join(args.model_cpt, \"model_best\", \"pytorch_model.bin\")\n",
    "                else:\n",
    "                    dir_files = os.listdir(args.model_cpt)\n",
    "                    checkpoint_dir = [el for el in dir_files if \"checkpoint-\" in el][0]\n",
    "                    model_cpt = os.path.join(args.model_cpt, checkpoint_dir, \"pytorch_model.bin\")\n",
    "                cpt = torch.load(model_cpt, map_location='cpu')\n",
    "                model.load_state_dict(cpt, strict=False)\n",
    "            logger.info(f'Loaded RMT state dict from: {args.model_cpt}')\n",
    "            logger.info(f'Trainable parameters: {[n for n, p in model.named_parameters() if p.requires_grad]}')\n",
    "    \n",
    "    if args.add_lora_to_armt:\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=args.lora_attn_dim,\n",
    "            lora_alpha=args.lora_attn_alpha,\n",
    "            lora_dropout=args.lora_dropout\n",
    "            )\n",
    "        # add LoRA only to the inner model\n",
    "        model.memory_cell.model = get_peft_model(model.memory_cell.model, peft_config)\n",
    "        logger.info('Added LoRA, trainable parameters with LoRA only:')\n",
    "        model.memory_cell.model.print_trainable_parameters()\n",
    "        # print(model)\n",
    "    \n",
    "    if args.freeze_model_weights:\n",
    "        for n, p in model.named_parameters():\n",
    "            if 'memory' not in n and 'lora' not in n and 'adapter' not in n:\n",
    "                p.requires_grad = False\n",
    "            else:\n",
    "                p.requires_grad = True\n",
    "        logger.info('Frozen model weights')\n",
    "        logger.info(f'Remaining parameters: {[n for n, p in model.named_parameters() if p.requires_grad]}')\n",
    "    \n",
    "    if args.tune_only_memory:\n",
    "        for n, p in model.named_parameters():\n",
    "            if 'memory_cell.memory' not in n:\n",
    "                p.requires_grad = False\n",
    "            else:\n",
    "                p.requires_grad = True\n",
    "        logger.info('Frozen model weights')\n",
    "        logger.info(f'Remaining parameters: {[n for n, p in model.named_parameters() if p.requires_grad]}')\n",
    "    \n",
    "    if args.tune_only_armt:\n",
    "        for n, p in model.named_parameters():\n",
    "            if 'memory_cell.memory' not in n and 'W_mq' not in n \\\n",
    "                    and 'W_mk' not in n and 'W_mv' not in n and 'W_mb' not in n:\n",
    "                p.requires_grad = False\n",
    "            else:\n",
    "                p.requires_grad = True\n",
    "        logger.info('Frozen model weights')\n",
    "        logger.info(f'Remaining parameters: {[n for n, p in model.named_parameters() if p.requires_grad]}')\n",
    "\n",
    "    # fix the not-contiguous error\n",
    "    def make_contiguous(module):\n",
    "        with torch.no_grad():\n",
    "            for param in module.parameters():\n",
    "                param.set_(param.contiguous())\n",
    "    make_contiguous(model)\n",
    "    \n",
    "    # ============================\n",
    "    # === Preparing HF trainer ===\n",
    "    # ============================\n",
    "    training_args_dict = {key: value for key, value in vars(args).items() if hasattr(SFTConfig('.'), key)}\n",
    "\n",
    "    training_args_dict['remove_unused_columns'] = False\n",
    "    training_args_dict['save_safetensors'] = False\n",
    "    training_args_dict['label_names'] = ['labels']\n",
    "    training_args_dict['eval_strategy'] = 'steps'\n",
    "    per_device_eval_batch_size = training_args_dict.get('per_device_train_batch_size') // 8\n",
    "    training_args_dict['per_device_eval_batch_size'] = max(per_device_eval_batch_size, 1)\n",
    "    training_args_dict['eval_accumulation_steps'] = 16\n",
    "    if args.d_mem is None:\n",
    "        # for now, gradient checkpointing doesn't supported for ARMT\n",
    "        training_args_dict['gradient_checkpointing'] = True\n",
    "        training_args_dict['gradient_checkpointing_kwargs'] = {'use_reentrant': False}\n",
    "    training_args_dict['log_level'] = 'debug'\n",
    "    training_args_dict['load_best_model_at_end'] = args.early_stopping_patience != -1\n",
    "\n",
    "    training_args_dict['dataset_kwargs'] = {\"skip_prepare_dataset\": True}\n",
    "\n",
    "    if args.num_mem_tokens is not None:\n",
    "        # fix max_seq_length warning\n",
    "        training_args_dict[\"max_seq_length\"] = args.segment_size\n",
    "    training_args = SFTConfig(**training_args_dict)\n",
    "\n",
    "    def compute_accuracy(eval_pred):\n",
    "        preds = eval_pred.predictions.argmax(axis=-1)[:, :-1]\n",
    "        labels = eval_pred.label_ids[:, 1:]\n",
    "\n",
    "        labels_masks = labels > 0\n",
    "        preds_full = [p[m] for p, m in zip(preds, labels_masks)]\n",
    "        labels_full = [lab[m] for lab, m in zip(labels, labels_masks)]\n",
    "\n",
    "        special_tokens = {ans[0], bos[0]}\n",
    "        acc_cot, acc_ans = [], []\n",
    "        for lab_tokens, pred_tokens in zip(labels_full, preds_full):\n",
    "            ans_start_index = max(i for i, x in enumerate(lab_tokens) if x == ans[0])\n",
    "\n",
    "            pred_cot_tokens = pred_tokens[:ans_start_index].tolist()\n",
    "            lab_cot_tokens = lab_tokens[:ans_start_index].tolist()\n",
    "\n",
    "            cot_correct = [p == l for p, l in zip(pred_cot_tokens, lab_cot_tokens) if l not in special_tokens]\n",
    "            acc_cot.append(all(cot_correct))\n",
    "\n",
    "            pred_ans_tokens = pred_tokens[ans_start_index:].tolist()\n",
    "            lab_ans_tokens = lab_tokens[ans_start_index:].tolist()\n",
    "\n",
    "            ans_correct = [p == l for p, l in zip(pred_ans_tokens, lab_ans_tokens) if l not in special_tokens]\n",
    "            acc_ans.append(all(ans_correct))\n",
    "\n",
    "        return {'accuracy_cot': np.mean(acc_cot), 'accuracy_ans': np.mean(acc_ans)}\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < training_args.warmup_steps:\n",
    "            return current_step / training_args.warmup_steps\n",
    "        if args.lr_scheduler_type == \"linear\":\n",
    "            decay_factor = (training_args.max_steps - current_step) / (training_args.max_steps - training_args.warmup_steps)\n",
    "            return max(args.min_lr / training_args.learning_rate, decay_factor)\n",
    "        elif args.lr_scheduler_type == \"constant\":\n",
    "            return 1.0\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported lr_scheduler_type\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_accuracy,\n",
    "        optimizers=(optimizer, scheduler)\n",
    "    )\n",
    "    logger.info(f\"Trainer Gradient Checkpointing Enabled: {trainer.args.gradient_checkpointing}\")\n",
    "    if args.early_stopping_patience != -1:\n",
    "        early_stopping = EarlyStoppingCallback(\n",
    "            early_stopping_patience=args.early_stopping_patience\n",
    "        )\n",
    "        trainer.add_callback(early_stopping)\n",
    "    start_metrics = trainer.evaluate()\n",
    "    logger.info(f\"Metrics of initial model: {start_metrics}\")\n",
    "    if not args.validate_only:\n",
    "        trainer.train(resume_from_checkpoint=args.checkpoint)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
